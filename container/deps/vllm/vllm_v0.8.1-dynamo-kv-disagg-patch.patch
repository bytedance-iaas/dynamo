diff --git a/README.md b/README.md
index b439c132..20906f4f 100644
--- a/README.md
+++ b/README.md
@@ -150,4 +150,4 @@ If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs
 
 ## Media Kit
 
-- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
\ No newline at end of file
+- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
diff --git a/vllm/config.py b/vllm/config.py
index a3ca20f2..c248122d 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -2841,9 +2841,6 @@ class KVTransferConfig(BaseModel):
     # The KV connector for vLLM to transmit KV caches between vLLM instances.
     kv_connector: Optional[str] = None
 
-    # Whether to use NIXL prepped xfer for KV cache transfer.
-    use_prepped_xfer: bool = False
-
     # The device used by kv connector to buffer the KV cache.
     # Currently only support 'cuda'.
     kv_buffer_device: Optional[str] = "cuda"
@@ -2853,7 +2850,7 @@ class KVTransferConfig(BaseModel):
     kv_buffer_size: float = 1e9
 
     # Whether this vLLM instance produces, consumes KV cache, or both. Choices
-    # are 'kv_producer', 'kv_consumer', and 'kv_both'.
+    # are 'kv_producer', 'kv_consumer', and 'both'.
     kv_role: Optional[str] = None
 
     # The rank of this vLLM instance in the KV cache transfer. Typical value:
@@ -2874,13 +2871,6 @@ class KVTransferConfig(BaseModel):
     # any extra config that the connector may need
     kv_connector_extra_config: dict[str, Any] = {}
 
-    # This does not need to be set by the user. It is set by the connector.
-    kv_producers_parallel_size: Optional[int] = None
-    kv_producers_tensor_parallel_size: Optional[int] = None
-    kv_producers_pipeline_parallel_size: Optional[int] = None
-    kv_consumers_tensor_parallel_size: Optional[int] = None
-    kv_consumers_pipeline_parallel_size: Optional[int] = None
-
     def compute_hash(self) -> str:
         """
         WARNING: Whenever a new field is added to this config,
@@ -2914,12 +2904,11 @@ class KVTransferConfig(BaseModel):
                 f"Supported roles are `kv_producer`, `kv_consumer`, "
                 f"and `kv_both`")
 
-        if self.kv_connector is not None and self.kv_connector != "DynamoNixlConnector" and self.kv_role is None:
+        if self.kv_connector is not None and self.kv_role is None:
             raise ValueError("Please specify kv_disagg_role when kv_connector "
                              "is set, supported roles are `kv_producer`, "
                              "`kv_consumer`, and `kv_both`")
 
-
     @property
     def is_kv_transfer_instance(self) -> bool:
         return self.kv_connector is not None and \
@@ -2929,10 +2918,6 @@ class KVTransferConfig(BaseModel):
     def need_kv_parallel_group(self) -> bool:
         # for those database-based connector, vLLM does not need to create
         # parallel group, and in that case the kv parallel size will be 1.
-        if self.kv_connector == "DynamoNixlConnector":
-            return False
-        
-        print(f"self.kv_connector is {self.kv_connector}, self.kv_parallel_size is {self.kv_parallel_size}")
         return self.kv_connector is not None and self.kv_parallel_size > 1
 
     @property
@@ -2948,18 +2933,6 @@ class KVTransferConfig(BaseModel):
     def get_from_extra_config(self, key, default) -> Any:
         return self.kv_connector_extra_config.get(key, default)
 
-    @property
-    def tensor_parallel_multiplier(self) -> int:
-        return self.kv_consumers_tensor_parallel_size // self.kv_producers_tensor_parallel_size
-
-    @property
-    def kv_consumers_parallel_size(self) -> int:
-        return self.kv_parallel_size - self.kv_producers_parallel_size
-
-    @property
-    def kv_world_size(self) -> int:
-        return self.kv_producers_parallel_size + self.kv_consumers_parallel_size * self.tensor_parallel_multiplier
-
 
 class CompilationLevel:
     # constants for the levels of the compilation process
diff --git a/vllm/core/block/cpu_gpu_block_allocator.py b/vllm/core/block/cpu_gpu_block_allocator.py
index d52ee050..359b5b26 100644
--- a/vllm/core/block/cpu_gpu_block_allocator.py
+++ b/vllm/core/block/cpu_gpu_block_allocator.py
@@ -6,7 +6,6 @@ from vllm.core.block.interfaces import (Block, BlockAllocator, BlockId,
                                         DeviceAwareBlockAllocator)
 from vllm.core.block.naive_block import NaiveBlock, NaiveBlockAllocator
 from vllm.core.block.prefix_caching_block import PrefixCachingBlockAllocator
-from vllm.core.event_manager import KVCacheEventManager
 from vllm.platforms import current_platform
 from vllm.utils import Device
 
@@ -29,7 +28,6 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
         num_gpu_blocks: int,
         num_cpu_blocks: int,
         block_size: int,
-        event_manager: Optional[KVCacheEventManager] = None,
     ) -> DeviceAwareBlockAllocator:
         """Creates a CpuGpuBlockAllocator instance with the specified
         configuration.
@@ -66,7 +64,6 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
         cpu_block_ids = block_ids[num_gpu_blocks:]
 
         if allocator_type == "naive":
-            assert event_manager is None, "Event API not supported with naive allocator."
             gpu_allocator: BlockAllocator = NaiveBlockAllocator(
                 create_block=NaiveBlock,  # type: ignore
                 num_blocks=num_gpu_blocks,
@@ -85,14 +82,12 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
                 num_blocks=num_gpu_blocks,
                 block_size=block_size,
                 block_ids=gpu_block_ids,
-                event_manager=event_manager,
             )
 
             cpu_allocator = PrefixCachingBlockAllocator(
                 num_blocks=num_cpu_blocks,
                 block_size=block_size,
                 block_ids=cpu_block_ids,
-                event_manager=event_manager,
             )
         else:
             raise ValueError(f"Unknown allocator type {allocator_type=}")
@@ -100,12 +95,10 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
         return CpuGpuBlockAllocator(
             cpu_block_allocator=cpu_allocator,
             gpu_block_allocator=gpu_allocator,
-            event_manager=event_manager,
         )
 
     def __init__(self, cpu_block_allocator: BlockAllocator,
-                 gpu_block_allocator: BlockAllocator,
-                 event_manager: Optional[KVCacheEventManager] = None,):
+                 gpu_block_allocator: BlockAllocator):
         assert not (
             cpu_block_allocator.all_block_ids
             & gpu_block_allocator.all_block_ids
@@ -115,7 +108,6 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
             Device.CPU: cpu_block_allocator,
             Device.GPU: gpu_block_allocator,
         }
-        self.event_manager = event_manager
 
         self._swap_mapping: Dict[int, int] = {}
         self._null_block: Optional[Block] = None
diff --git a/vllm/core/block/naive_block.py b/vllm/core/block/naive_block.py
index 31ed7aa4..c388366b 100644
--- a/vllm/core/block/naive_block.py
+++ b/vllm/core/block/naive_block.py
@@ -2,7 +2,7 @@
 
 from collections import deque
 from typing import Deque, FrozenSet, Iterable, List, Optional, Tuple, Union
-import heapq
+
 from vllm.core.block.common import (BlockPool, CopyOnWriteTracker, RefCounter,
                                     get_all_blocks_recursively)
 from vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device
@@ -38,7 +38,7 @@ class NaiveBlockAllocator(BlockAllocator):
         if block_ids is None:
             block_ids = range(num_blocks)
 
-        self._free_block_indices: List[BlockId] = list(block_ids)
+        self._free_block_indices: Deque[BlockId] = deque(block_ids)
         self._all_block_indices = frozenset(block_ids)
         assert len(self._all_block_indices) == num_blocks
 
@@ -134,8 +134,7 @@ class NaiveBlockAllocator(BlockAllocator):
         if not self._free_block_indices:
             raise BlockAllocator.NoFreeBlocksError()
 
-        block_id = heapq.heappop(self._free_block_indices)
-        # TODO: figure out why sometime block_id is None
+        block_id = self._free_block_indices.popleft()
         self._refcounter.incr(block_id)
         return block_id
 
@@ -149,7 +148,7 @@ class NaiveBlockAllocator(BlockAllocator):
 
         refcount = self._refcounter.decr(block_id)
         if refcount == 0:
-            heapq.heappush(self._free_block_indices, block_id)
+            self._free_block_indices.appendleft(block_id)
 
     def free(self, block: Block, keep_block_object: bool = False) -> None:
         # Release the physical block id
diff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py
index b1591c0c..1ca9e49d 100644
--- a/vllm/core/block/prefix_caching_block.py
+++ b/vllm/core/block/prefix_caching_block.py
@@ -4,7 +4,7 @@ import sys
 from bisect import bisect_left
 from os.path import commonprefix
 from typing import (Callable, Dict, FrozenSet, Iterable, List, Optional, Set,
-                    Tuple, TYPE_CHECKING)
+                    Tuple)
 
 from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,
                                     get_all_blocks_recursively)
@@ -23,9 +23,6 @@ PrefixHash = int
 # then we know this block hasn't been accessed yet.
 _DEFAULT_LAST_ACCESSED_TIME = -1
 
-if TYPE_CHECKING:
-    from vllm.core.event_manager import KVCacheEventManager
-
 logger = init_logger(__name__)
 
 
@@ -83,7 +80,6 @@ class PrefixCachingBlockAllocator(BlockAllocator):
         block_size: int,
         block_ids: Optional[Iterable[int]] = None,
         eviction_policy: EvictionPolicy = EvictionPolicy.LRU,
-        event_manager: Optional["KVCacheEventManager"] = None,
     ):
         if block_ids is None:
             block_ids = range(num_blocks)
@@ -135,9 +131,6 @@ class PrefixCachingBlockAllocator(BlockAllocator):
 
         self.metric_data = CacheMetricData()
 
-        self.event_manager = event_manager
-
-    # Implements Block.Factory.
     def _create_block(
         self,
         prev_block: Optional[Block],
@@ -344,9 +337,6 @@ class PrefixCachingBlockAllocator(BlockAllocator):
         assert self._refcounter.get(_block_id) == 0
         assert _block_id == block_id
 
-        if self.event_manager:
-            self.event_manager.enqueue_removed_event(content_hash_to_evict)
-
         self._cached_blocks.pop(content_hash_to_evict)
 
         self._refcounter.incr(block_id)
@@ -523,10 +513,6 @@ class PrefixCachingBlockAllocator(BlockAllocator):
             # Mark this block as touched so that it can be marked as
             # computed after the entire batch of sequences are scheduled.
             self._touched_blocks.add(block.block_id)
-
-            if self.event_manager:
-                self.event_manager.enqueue_stored_event(block.prev_block, block)
-
             return block.block_id
 
         # Reuse the cached content hash
diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py
index 12cd4dc9..c5b3b04f 100644
--- a/vllm/core/block_manager.py
+++ b/vllm/core/block_manager.py
@@ -10,10 +10,7 @@ from vllm.core.block.interfaces import Block
 from vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,
                                                   LastAccessBlocksTracker)
 from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec
-from vllm.core.event_manager import KVCacheEventManager
 from vllm.core.interfaces import AllocStatus, BlockSpaceManager
-from vllm.envs import (VLLM_KV_CAPI_PATH, VLLM_KV_COMPONENT, VLLM_KV_NAMESPACE,
-                       VLLM_WORKER_ID)
 from vllm.sequence import Sequence, SequenceGroup, SequenceStatus
 from vllm.utils import Device
 
@@ -63,7 +60,6 @@ class SelfAttnBlockSpaceManager(BlockSpaceManager):
 
     def __init__(
         self,
-        model_name: str,
         block_size: int,
         num_gpu_blocks: int,
         num_cpu_blocks: int,
@@ -95,29 +91,11 @@ class SelfAttnBlockSpaceManager(BlockSpaceManager):
 
         self.watermark_blocks = int(watermark * num_gpu_blocks)
 
-        kv_event_manager_params = [
-            VLLM_WORKER_ID, VLLM_KV_CAPI_PATH, VLLM_KV_NAMESPACE,
-            VLLM_KV_COMPONENT
-        ]
-        set_kv_event_manager_params = len(
-            [param for param in kv_event_manager_params if param is not None])
-
-        if set_kv_event_manager_params == len(kv_event_manager_params):
-            self.event_manager = KVCacheEventManager(
-                namespace=VLLM_KV_NAMESPACE,
-                component=VLLM_KV_COMPONENT,
-                worker_id=VLLM_WORKER_ID,
-                lib_path=VLLM_KV_CAPI_PATH,
-                kv_block_size=block_size)
-        else:
-            self.event_manager = None
-
         self.block_allocator = CpuGpuBlockAllocator.create(
             allocator_type="prefix_caching" if enable_caching else "naive",
             num_gpu_blocks=num_gpu_blocks,
             num_cpu_blocks=num_cpu_blocks,
             block_size=block_size,
-            event_manager=self.event_manager,
         )
 
         self.block_tables: Dict[SeqId, BlockTable] = {}
diff --git a/vllm/core/event_manager.py b/vllm/core/event_manager.py
deleted file mode 100644
index a27af580..00000000
--- a/vllm/core/event_manager.py
+++ /dev/null
@@ -1,108 +0,0 @@
-# SPDX-License-Identifier: Apache-2.0
-import ctypes
-import logging
-import uuid
-from ctypes import c_char_p, c_size_t, c_uint32, c_void_p, c_int64
-from typing import Optional
-
-from vllm.core.block.prefix_caching_block import PrefixCachingBlock, PrefixHash
-
-logger = logging.getLogger(__name__)
-
-
-class DynamoResult:
-    OK = 0
-    ERR = 1
-
-
-class KVCacheEventManager:
-
-    def __init__(self, namespace: str, component: str, worker_id: int,
-                 lib_path: str, kv_block_size: int):
-        self.lib = None
-
-        try:
-            self.lib = ctypes.CDLL(lib_path)
-            self.lib.dynamo_llm_init.argtypes = [
-                c_char_p,
-                c_char_p,
-                c_int64,
-                c_uint32,
-            ]
-            self.lib.dynamo_llm_init.restype = c_uint32
-
-            result = self.lib.dynamo_llm_init(
-                namespace.encode(), component.encode(), worker_id, kv_block_size
-            )
-            if result == DynamoResult.OK:
-                logger.info(
-                    "KVCacheEventManager initialized successfully. Ready to publish KV Cache Events"
-                )
-            else:
-                logger.info("KVCacheEventManager initialization failed!")
-
-        except Exception as e:
-            print(f"Failed to load {lib_path}")
-            raise e
-
-        self.lib.dynamo_kv_event_publish_stored.argtypes = [
-            ctypes.c_uint64,  # event_id
-            ctypes.POINTER(ctypes.c_uint32),  # token_ids
-            ctypes.POINTER(ctypes.c_size_t),  # num_block_tokens
-            ctypes.POINTER(ctypes.c_uint64),  # block_ids
-            ctypes.c_size_t,  # num_blocks
-            ctypes.POINTER(ctypes.c_uint64),  # parent_hash
-            ctypes.c_uint64,  # lora_id
-        ]
-        self.lib.dynamo_kv_event_publish_stored.restype = ctypes.c_uint32  # dynamo_llm_result_t
-
-        self.lib.dynamo_kv_event_publish_removed.argtypes = [
-            ctypes.c_uint64,  # event_id
-            ctypes.POINTER(ctypes.c_uint64),  # block_ids
-            ctypes.c_size_t,  # num_blocks
-        ]
-        self.lib.dynamo_kv_event_publish_removed.restype = ctypes.c_uint32  # dynamo_llm_result_t
-
-        self.event_id_counter = 0
-
-    def enqueue_stored_event(self, parent: Optional[PrefixCachingBlock],
-                             block: PrefixCachingBlock):
-        token_ids_arr = (ctypes.c_uint32 *
-                         len(block.token_ids))(*block.token_ids)
-        num_block_tokens = (ctypes.c_size_t * 1)(len(block.token_ids))
-        block_hash = (ctypes.c_uint64 * 1)(block.content_hash)
-        parent_hash = ((ctypes.c_uint64 * 1)(parent.content_hash)
-                       if parent is not None else None)
-
-        # Publish the event
-        result = self.lib.dynamo_kv_event_publish_stored(
-            self.event_id_counter,  # uint64_t event_id
-            token_ids_arr,  # const uint32_t *token_ids
-            num_block_tokens,  # const uintptr_t *num_block_tokens
-            block_hash,  # const uint64_t *block_ids
-            1,  # uintptr_t num_blocks
-            parent_hash,  # const uint64_t *parent_hash
-            0,  # uint64_t lora_id
-        )
-
-        if result == DynamoResult.OK:
-            logger.debug(f"Store - Published KV Event: {block.content_hash}")
-        else:
-            logger.debug(
-                f"Store - Failed to Publish KV Event: {block.content_hash}")
-
-        self.event_id_counter += 1
-
-    def enqueue_removed_event(self, block_hash: PrefixHash):
-        result = self.lib.dynamo_kv_event_publish_removed(
-            self.event_id_counter,
-            (ctypes.c_uint64 * 1)(block_hash),
-            1,
-        )
-
-        if result == DynamoResult.OK:
-            logger.debug(f"Remove - Published KV Event: {block_hash}")
-        else:
-            logger.debug(f"Remove - Failed to Publish KV Event: {block_hash}")
-
-        self.event_id_counter += 1
diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py
index 7264e69d..e93143c8 100644
--- a/vllm/core/scheduler.py
+++ b/vllm/core/scheduler.py
@@ -4,14 +4,13 @@ import enum
 import os
 import random
 import time
-import copy
 from collections import deque
 from dataclasses import dataclass, field
 from typing import Callable, Deque, Dict, Iterable, List, Optional
 from typing import Sequence as GenericSequence
-from typing import Set, Tuple, Union, Any
+from typing import Set, Tuple, Union
 
-from vllm.config import ModelConfig, CacheConfig, LoRAConfig, SchedulerConfig
+from vllm.config import CacheConfig, LoRAConfig, SchedulerConfig
 from vllm.core.interfaces import AllocStatus, BlockSpaceManager
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
@@ -21,6 +20,7 @@ from vllm.sequence import (Sequence, SequenceData, SequenceGroup,
                            SequenceGroupMetadataDelta, SequenceStage,
                            SequenceStatus)
 from vllm.utils import Device, PyObjectCache
+
 logger = init_logger(__name__)
 
 # Test-only. If configured, decode is preempted with
@@ -292,7 +292,6 @@ class SchedulerPrefillOutputs:
     # Ignored sequence groups.
     ignored_seq_groups: List[SequenceGroup]
     num_lookahead_slots: int
-    num_remote_prefill_groups: int
 
     @classmethod
     def create_empty(cls) -> "SchedulerPrefillOutputs":
@@ -300,7 +299,6 @@ class SchedulerPrefillOutputs:
             seq_groups=[],
             ignored_seq_groups=[],
             num_lookahead_slots=0,
-            num_remote_prefill_groups=0,
         )
 
 
@@ -428,14 +426,12 @@ class Scheduler:
 
     def __init__(
         self,
-        model_config: ModelConfig,
         scheduler_config: SchedulerConfig,
         cache_config: CacheConfig,
         lora_config: Optional[LoRAConfig],
         pipeline_parallel_size: int = 1,
         output_proc_callback: Optional[Callable] = None,
     ) -> None:
-        self.model_config = model_config
         self.scheduler_config = scheduler_config
         self.cache_config = cache_config
         # Note for LoRA scheduling: the current policy is extremely
@@ -461,7 +457,6 @@ class Scheduler:
 
         # Create the block space manager.
         self.block_manager = BlockSpaceManagerImpl(
-            model_name=self.model_config.served_model_name,
             block_size=self.cache_config.block_size,
             num_gpu_blocks=num_gpu_blocks,
             num_cpu_blocks=num_cpu_blocks,
@@ -478,16 +473,6 @@ class Scheduler:
         # Sequence groups in the SWAPPED state.
         # Contain decode requests that are swapped out.
         self.swapped: Deque[SequenceGroup] = deque()
-
-        # Sequence groups in the REMOTE_PREFILLING state.
-        # Contain requests that are being prefilled by a remote worker.
-        self.remote_prefilling: Deque[SequenceGroup] = deque()
-        # Contain requests that are being prefilled by a local worker.
-        self.prefill_sending: Deque[SequenceGroup] = deque()
-
-        self._remote_prefill_outputs: Dict[str, int] = {}
-
-
         # Sequence groups finished requests ids since last step iteration.
         # It lets the model know that any state associated with these requests
         # can and must be released after the current step.
@@ -643,8 +628,8 @@ class Scheduler:
             self.block_manager.free_cross(seq_group)
 
     def has_unfinished_seqs(self) -> bool:
-        return len(self.waiting) != 0 or len(self.running) != 0 or len(
-            self.swapped) != 0 or len(self.remote_prefilling) != 0 or len(self.prefill_sending) != 0
+        return (len(self.waiting) != 0 or len(self.running) != 0
+                or len(self.swapped) != 0)
 
     def get_prefix_cache_hit_rate(self, device: Device) -> float:
         return self.block_manager.get_prefix_cache_hit_rate(device)
@@ -667,8 +652,6 @@ class Scheduler:
         curr_loras: Optional[Set[int]],
         enable_chunking: bool = False,
         partial_prefill_metadata: Optional[PartialPrefillMetadata] = None,
-        finished_prefills: Optional[Set[str]] = None,
-        finished_transfers: Optional[Set[str]] = None
     ) -> SchedulerRunningOutputs:
         """Schedule sequence groups that are running.
 
@@ -686,9 +669,6 @@ class Scheduler:
             partial_prefill_metadata: information about the partial prefills
             that are currently running
 
-            finished_remote_prefill_request_ids: Set of request ids of remote
-                prefills that have finished.
-    
         Returns:
             SchedulerRunningOutputs.
         """
@@ -717,38 +697,6 @@ class Scheduler:
         preempted: List[SequenceGroup] = ret.preempted
         swapped_out: List[SequenceGroup] = ret.swapped_out
 
-        remote_prefilling_queue = self.remote_prefilling
-        leftover_remote_prefilling_sequences: Deque[SequenceGroup] = deque()
-        while remote_prefilling_queue:
-            seq_group = remote_prefilling_queue.popleft()
-            if seq_group.request_id not in finished_prefills:
-                leftover_remote_prefilling_sequences.append(seq_group)
-                continue
-                
-            else:
-                finished_prefills.remove(seq_group.request_id)
-                assert len(seq_group.seqs) == 1
-                seq = seq_group.seqs[0]
-                # we computed all but the last token in prefill, we need to decode the first token on decode
-                seq_group.update_num_computed_tokens(seq.get_len() - 1)
-                seq.status = SequenceStatus.RUNNING
-                seq.data._stage = SequenceStage.DECODE
-                self.running.appendleft(seq_group)
-        remote_prefilling_queue.extendleft(leftover_remote_prefilling_sequences)
-
-        remote_transfers_queue = self.prefill_sending
-        leftover_remote_transfers_sequences: Deque[SequenceGroup] = deque()
-        while remote_transfers_queue:
-            seq_group = remote_transfers_queue.popleft()
-            if seq_group.request_id not in finished_transfers:
-                leftover_remote_transfers_sequences.append(seq_group)
-            else:
-                finished_transfers.remove(seq_group.request_id)
-                assert len(seq_group.seqs) == 1
-                seq = seq_group.seqs[0]
-                self.free_seq(seq)
-        remote_transfers_queue.extendleft(leftover_remote_transfers_sequences)
-
         running_queue = self.running
         assert len(self._async_stopped) == 0
         while running_queue:
@@ -1125,7 +1073,6 @@ class Scheduler:
         seq_groups: List[ScheduledSequenceGroup] = []
 
         waiting_queue = self.waiting
-        num_remote_prefill_groups = 0
 
         leftover_waiting_sequences: Deque[SequenceGroup] = deque()
         while self._passed_delay(time.time()) and waiting_queue:
@@ -1223,18 +1170,7 @@ class Scheduler:
             if curr_loras is not None and lora_int_id > 0:
                 curr_loras.add(lora_int_id)
             waiting_queue.popleft()
-
-            seq_group_copy = copy.deepcopy(seq_group)
-            seq_group_copy.seqs[0].seq_id = seq_group.seqs[0].seq_id + 1
-
-            logger.debug("Allocating and setting running or remote prefill for seq_group %s", seq_group.request_id)
-            logger.debug("Seq id: %s", seq_group.seqs[0].seq_id)
-            is_remote_prefill = self._allocate_and_set_running_or_remote_prefill(seq_group)
-            num_remote_prefill_groups += is_remote_prefill
-            if seq_group.remote_prefill_params is not None and seq_group.remote_prefill_params.is_remote_decode:
-                logger.debug("Seq id: %s", seq_group_copy.seqs[0].seq_id)
-                self._allocate_and_set_running_or_remote_prefill(seq_group_copy)
-                self.prefill_sending.append(seq_group_copy)
+            self._allocate_and_set_running(seq_group)
 
             if partial_prefill_metadata is not None:
                 partial_prefill_metadata.maybe_increment_partial_prefills(
@@ -1278,10 +1214,9 @@ class Scheduler:
             ignored_seq_groups=ignored_seq_groups,
             num_lookahead_slots=self._get_num_lookahead_slots(
                 is_prefill=True, enable_chunking=enable_chunking),
-            num_remote_prefill_groups=num_remote_prefill_groups
         )
 
-    def _schedule_default(self, finished_prefills: Optional[Set[str]] = None, finished_transfers: Optional[Set[str]] = None) -> SchedulerOutputs:
+    def _schedule_default(self) -> SchedulerOutputs:
         """Schedule queued requests.
 
         The current policy is designed to optimize the throughput. First,
@@ -1299,9 +1234,6 @@ class Scheduler:
         for seq_group in self.running:
             budget.add_num_seqs(seq_group.request_id,
                                 seq_group.get_max_num_running_seqs())
-        for seq_group in self.prefill_sending:
-            budget.add_num_seqs(seq_group.request_id,
-                                seq_group.get_max_num_running_seqs())
         curr_loras = (set(
             seq_group.lora_int_id for seq_group in self.running
             if seq_group.lora_int_id > 0) if self.lora_enabled else None)
@@ -1323,12 +1255,10 @@ class Scheduler:
         # Don't schedule decodes if prefills are scheduled.
         # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running
         # only contains decode requests, not chunked prefills.
-        if len(prefills.seq_groups) == prefills.num_remote_prefill_groups:
+        if len(prefills.seq_groups) == 0:
             running_scheduled = self._schedule_running(budget,
                                                        curr_loras,
-                                                       enable_chunking=False,
-                                                       finished_prefills=finished_prefills,
-                                                       finished_transfers=finished_transfers)
+                                                       enable_chunking=False)
 
             # If any sequence group is preempted, do not swap in any sequence
             # group. because it means there's no slot for new running requests.
@@ -1345,12 +1275,7 @@ class Scheduler:
         self.waiting.extendleft(running_scheduled.preempted)
         # Update new running requests.
         if len(prefills.seq_groups) > 0:
-            for s in prefills.seq_groups:
-                seq_group = s.seq_group
-                if seq_group.remote_prefill_params is not None and seq_group.remote_prefill_params.is_remote_prefill:
-                    self.remote_prefilling.append(seq_group)
-                else:
-                    self.running.append(seq_group)
+            self.running.extend([s.seq_group for s in prefills.seq_groups])
 
         self.running.extend(running_scheduled.decode_seq_groups_list)
 
@@ -1527,14 +1452,12 @@ class Scheduler:
         ]
         return finishing + not_finishing
 
-    def _schedule(self, finished_prefills: Optional[Set[str]] = None, finished_transfers: Optional[Set[str]] = None) -> SchedulerOutputs:
+    def _schedule(self) -> SchedulerOutputs:
         """Schedule queued requests."""
         if self.scheduler_config.chunked_prefill_enabled:
-            if finished_prefills or finished_transfers:
-                raise ValueError("Chunked prefill does not support remote prefills")
             return self._schedule_chunked_prefill()
         else:
-            return self._schedule_default(finished_prefills, finished_transfers)
+            return self._schedule_default()
 
     def _can_append_slots(self, seq_group: SequenceGroup,
                           enable_chunking: bool) -> bool:
@@ -1568,16 +1491,14 @@ class Scheduler:
         return no_single_seq
 
     def schedule(
-            self,
-            finished_prefills: Optional[Set[str]] = None,
-            finished_transfers: Optional[Set[str]] = None
+            self
     ) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs, bool]:
         # Schedule sequence groups.
         # This function call changes the internal states of the scheduler
         # such as self.running, self.swapped, and self.waiting.
-
         scheduler_start_time = time.perf_counter()
-        scheduler_outputs: SchedulerOutputs = self._schedule(finished_prefills, finished_transfers)
+
+        scheduler_outputs: SchedulerOutputs = self._schedule()
         now = time.time()
 
         if not self.cache_config.enable_prefix_caching:
@@ -1616,8 +1537,7 @@ class Scheduler:
                 encoder_seq_data = None
                 cross_block_table = None
 
-            running_or_remote_prefilling_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING) + seq_group.get_seqs(status=SequenceStatus.REMOTE_PREFILLING)
-            for seq in running_or_remote_prefilling_seqs:
+            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
                 seq_id = seq.seq_id
                 seq_data[seq_id] = seq.data
                 block_tables[seq_id] = self.block_manager.get_block_table(seq)
@@ -1648,16 +1568,9 @@ class Scheduler:
                         < seqs[0].data.get_len()):
                     do_sample = False
 
-            is_remote_prefill = False
-            if is_first_prefill and seq_group.remote_prefill_params is not None and seq_group.remote_prefill_params.is_remote_prefill:
-                is_remote_prefill = True
-            if is_first_prefill and seq_group.remote_prefill_params is not None and seq_group.remote_prefill_params.is_remote_decode:
-                block_tables[seq_group.seqs[0].seq_id + 1] = self.block_manager.block_tables[seq.seq_id + 1].physical_block_ids
-
             # It assumes the scheduled_seq_groups is ordered by
             # prefill < decoding.
             if is_first_prefill or not self.scheduler_config.send_delta_data:
-                logger.debug("Assinged blocks: %s", block_tables)
                 seq_group_metadata = SequenceGroupMetadata(
                     request_id=seq_group.request_id,
                     is_prompt=is_prompt,
@@ -1685,7 +1598,6 @@ class Scheduler:
                         if scheduler_outputs.num_prefill_groups > 0 else None),
                     mm_processor_kwargs=seq_group.mm_processor_kwargs,
                     prompt_adapter_request=seq_group.prompt_adapter_request,
-                    do_remote_prefill=is_remote_prefill,
                 )
             else:
                 # When SPMD mode is enabled, we only send delta data except for
@@ -1784,21 +1696,17 @@ class Scheduler:
 
             self._async_stopped.clear()
 
-    def _allocate_and_set_running_or_remote_prefill(self, seq_group: SequenceGroup) -> bool:
+    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:
         self.block_manager.allocate(seq_group)
-        is_remote_prefill = False
         for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
-            if seq_group.remote_prefill_params is not None and seq_group.remote_prefill_params.is_remote_prefill:
-                seq.status = SequenceStatus.REMOTE_PREFILLING
-                is_remote_prefill = True
-            else:
-                seq.status = SequenceStatus.RUNNING
-        return is_remote_prefill
+            seq.status = SequenceStatus.RUNNING
 
-    def _append_slots(self,
-                      seq_group: SequenceGroup,
-                      blocks_to_copy: List[Tuple[int, int]],
-                      enable_chunking: bool = False) -> None:
+    def _append_slots(
+        self,
+        seq_group: SequenceGroup,
+        blocks_to_copy: List[Tuple[int, int]],
+        enable_chunking: bool = False,
+    ) -> None:
         """Appends new slots to the sequences in the given sequence group.
 
         Args:
diff --git a/vllm/distributed/device_communicators/kv_rearrange.py b/vllm/distributed/device_communicators/kv_rearrange.py
deleted file mode 100644
index 9b938039..00000000
--- a/vllm/distributed/device_communicators/kv_rearrange.py
+++ /dev/null
@@ -1,61 +0,0 @@
-import torch
-import triton
-import triton.language as tl
-
-@triton.jit
-def rearrange_kernel(
-    t1_ptr,
-    t2_ptr,
-    N,
-    B,
-    H,
-    C,
-    d,
-    tensor_subset_size,
-    block_size,
-    token_size,
-    BLOCK_SIZE: tl.constexpr,
-):
-    pid = tl.program_id(0)
-    
-    block_start = pid * BLOCK_SIZE
-    offsets = block_start + tl.arange(0, BLOCK_SIZE)
-
-    curr_n = offsets // block_size
-    curr_b = offsets // token_size % B
-    curr_h = offsets // C % H 
-    curr_c = offsets % C
-
-    src_pos = offsets
-
-    tp_group = curr_h * d // H
-    dst_h = curr_h % (H // d)
-    tp_group_offset = curr_n * (block_size // d) + curr_b * (H // d) * C + dst_h * C + curr_c
-
-    dst_pos = tensor_subset_size * tp_group + tp_group_offset
-    
-    tl.store(t2_ptr + dst_pos, tl.load(t1_ptr + src_pos))
-
-def rearrange_tensors(t1: torch.Tensor, t2: torch.Tensor, d: int):
-    N, B, H, C = t1.shape
-    
-    assert t2.shape == (N, B, H, C), "Destination tensor must have same shape as source"
-    assert H % d == 0, "H must be divisible by d"
-
-    block_size = B * H * C
-    token_size = H * C
-    tensor_size = N * block_size
-    tensor_subset_size = tensor_size // d
-    
-    BLOCK_SIZE = 1024
-    grid = ((N * B * H * C + BLOCK_SIZE - 1) // BLOCK_SIZE,)
-    
-    rearrange_kernel[grid](
-        t1, t2,
-        N, B, H, C,
-        d,
-        tensor_subset_size,
-        block_size,
-        token_size,
-        BLOCK_SIZE=BLOCK_SIZE
-    )
\ No newline at end of file
diff --git a/vllm/distributed/device_communicators/nixl.py b/vllm/distributed/device_communicators/nixl.py
deleted file mode 100644
index 6c782507..00000000
--- a/vllm/distributed/device_communicators/nixl.py
+++ /dev/null
@@ -1,431 +0,0 @@
-import torch
-from typing import List, Tuple, Union
-from vllm.config import VllmConfig
-from vllm.logger import init_logger
-import msgspec
-import time
-import uuid
-from collections import defaultdict
-from .kv_rearrange import rearrange_tensors
-
-logger = init_logger(__name__)
-
-# Lazy import nixl_wrapper to avoid loading nixl_bindings if nixl is not used
-try:
-    from nixl._api import nixl_agent as NixlWrapper
-    logger.info("NIXL is available")
-except ImportError:
-    logger.warning("NIXL is not available")
-    NixlWrapper = None
-
-class NixlMetadata(
-        msgspec.Struct,
-        omit_defaults=True,  # type: ignore[call-arg]
-        # required for @cached_property.
-        dict=True):
-    engine_id: str
-    agent_metadata: List[bytes]
-    kv_caches_base_addr: List[List[List[int]]] # base address for each rank for each layer for keys and values
-    num_blocks: int
-
-
-class DynamoNixlConnector:
-    def __init__(self, vllm_config: VllmConfig, engine_id: str, rank: int):
-        self.vllm_config = vllm_config
-        if NixlWrapper is None:
-            logger.error("NIXL is not available")
-            raise RuntimeError("NIXL is not available")
-        logger.info("Initializing NIXL wrapper")
-        self.nixl_wrapper = NixlWrapper(str(uuid.uuid4()), None)
-
-        self.use_prepped_xfer = vllm_config.kv_transfer_config.use_prepped_xfer
-
-        self.num_layers = None
-        self.num_blocks = None
-        self.num_heads = None
-        self.block_len = None
-        self.kv_caches = None
-        self.kv_caches_base_addr = {}
-        self.kv_cache_shape = {}
-
-        self._registered_descs = []
-        self._remote_agents = {}
-        self.engine_id = engine_id
-        self.rank = rank
-        self._tp_size = {}
-        self.src_xfer_side_handles = {}
-        self.dst_xfer_side_handles = defaultdict(dict)
-        self.dst_num_blocks = {}
-        self.use_mla = vllm_config.model_config.use_mla
-
-        self._transfers = defaultdict(list)
-
-
-        self._tp_size[engine_id] = vllm_config.parallel_config.tensor_parallel_size
-        
-
-    @property
-    def agent_name(self):
-        return self.nixl_wrapper.name
-
-    def register_kv_caches(self, kv_caches: List[torch.Tensor]):
-        if self.use_mla:
-            num_blocks, block_size, head_dim = kv_caches[0].shape
-            self.block_len = block_size * head_dim * kv_caches[0].element_size()            
-        else:
-            _, num_blocks, block_size, num_heads, head_dim = kv_caches[0].shape
-            self.block_len = block_size * num_heads * head_dim * kv_caches[0].element_size()
-            self.num_heads = num_heads
-        logger.debug("Per layer kv cache size: %s", kv_caches[0].shape)
-        self.num_layers = len(kv_caches)
-        self.num_blocks = num_blocks
-        self.kv_caches = kv_caches
-        kv_caches_base_addr = []
-        caches_data = []
-        if self.use_mla:
-            for key_cache in kv_caches:
-                base_addr = key_cache.data_ptr()
-                region_len = num_blocks * self.block_len
-                caches_data.append((base_addr, region_len, self.rank, ""))
-                kv_caches_base_addr.append([key_cache.data_ptr()])
-        else:
-            for key_cache, value_cache in kv_caches:
-                base_addr = key_cache.data_ptr()
-                region_len = 2 * num_blocks * self.block_len
-                caches_data.append((base_addr, region_len, self.rank, ""))
-                kv_caches_base_addr.append([key_cache.data_ptr(), value_cache.data_ptr()])
-
-        self.kv_caches_base_addr[self.engine_id] = kv_caches_base_addr
-
-        descs = self.nixl_wrapper.get_reg_descs(caches_data, "VRAM")
-        logger.debug("Registering descs: %s", caches_data)
-        self.nixl_wrapper.register_memory(descs)
-        self._registered_descs.append(descs)
-
-    def get_agent_metadata(self):
-        return self.nixl_wrapper.get_agent_metadata()
-    
-    def shutdown(self):
-        for descs_list in self._registered_descs:
-            self.nixl_wrapper.deregister_memory(descs_list)
-        for agent_names in self._remote_agents.values():
-            for agent_name in agent_names:
-                self.nixl_wrapper.remove_remote_agent(agent_name)
-        for src_xfer_side_handle in self.src_xfer_side_handles.values():
-            self.nixl_wrapper.release_dlist_handle(src_xfer_side_handle)
-        for dst_xfer_side_handles in self.dst_xfer_side_handles.values():
-            for dst_xfer_side_handle in dst_xfer_side_handles.values():
-                self.nixl_wrapper.release_dlist_handle(dst_xfer_side_handle)
-    
-    def get_descs_ids(self, layer_ids, block_ids):
-        if layer_ids == "all":
-            layer_ids = list(range(self.num_layers))
-        if block_ids == "all":
-            block_ids = list(range(self.num_blocks))
-        descs_ids = []
-        for layer_id in layer_ids:
-            for block_id in block_ids:
-                assert block_id < self.num_blocks, f"Block id {block_id} is greater than the number of blocks {self.num_blocks}"
-                descs_ids.append(2 * (self.num_blocks * layer_id + block_id))
-                descs_ids.append(2 * (self.num_blocks * layer_id + block_id) + 1)
-        return descs_ids
-
-    def _get_range_descs(self, ranges, layer_ids, kv_caches_base_addr, tp_multiplier=1, rank=None, i=0, staging_ranges=None):
-        if rank is None:
-            rank = self.rank
-        block_len = self.block_len // tp_multiplier
-        logger.debug("Getting range descs for layer ids: %s, ranges: %s, tp_multiplier: %s, rank: %s, i: %s", layer_ids, ranges, tp_multiplier, rank, i)
-        if layer_ids == "all":
-            layer_ids = list(range(self.num_layers))
-        blocks_data = []
-        for layer_id in layer_ids:
-            for range_idx, (range_start, range_end) in enumerate(ranges):
-                if self.use_mla:
-                    range_len = range_end - range_start + 1
-                    key_base_addr = kv_caches_base_addr[layer_id][0]
-                    if staging_ranges is not None:
-                        start_offset = staging_ranges[range_idx][0] * self.block_len + i * block_len * (staging_ranges[range_idx][1] - staging_ranges[range_idx][0] + 1) + (range_start - staging_ranges[range_idx][0]) * block_len
-                    else:
-                        start_offset = range_start * block_len
-                    blocks_data.append((key_base_addr + start_offset, range_len * block_len, rank))
-                else:    
-                    range_len = range_end - range_start + 1
-                    key_base_addr, value_base_addr = kv_caches_base_addr[layer_id]
-                    if staging_ranges is not None:
-                        start_offset = staging_ranges[range_idx][0] * self.block_len + i * block_len * (staging_ranges[range_idx][1] - staging_ranges[range_idx][0] + 1) + (range_start - staging_ranges[range_idx][0]) * block_len
-                    else:
-                        start_offset = range_start * block_len
-                    blocks_data.append((key_base_addr + start_offset, range_len * block_len, rank))
-                    blocks_data.append((value_base_addr + start_offset, range_len * block_len, rank))
-        return self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
-    
-    def _get_ranges(self, block_ids):
-        # This function should return a list of ranges of block ids that are contiguous
-        # For example, if block_ids is [0, 1, 2, 4, 5, 6], the function should return [[0, 2], [4, 6]]
-        # The ranges are sorted by the starting block id
-        # The function should also make sure that the block ids are contiguous
-        # If the block ids are not contiguous, the function should raise an error
-        ranges = []
-        for i in range(len(block_ids)):
-            if i == 0 or block_ids[i] != block_ids[i-1] + 1:
-                ranges.append([block_ids[i], block_ids[i]])
-            else:
-                ranges[-1][1] = block_ids[i]
-        return ranges
-
-    def _get_same_length_ranges(self, src_ranges, dst_ranges, return_original_src_ranges=False):
-        # This function should return a list of ranges for both src and dst so that corresponding ranges are the same length
-        # For example, if src_ranges is [[0, 2] [4, 8]] and dst_ranges is [[1, 3], [5, 7], [9, 10]]
-        # The function should return ([[0, 2], [4, 6], [7, 8]], [[1, 3], [5, 7], [9, 10]])
-        src_overlapping_ranges, dst_overlapping_ranges = [], []
-
-        original_src_ranges = []
-        org_src_range = tuple(src_ranges[0])
-        
-        src_idx, dst_idx = 0, 0
-        while src_idx < len(src_ranges) and dst_idx < len(dst_ranges):
-            src_range = src_ranges[src_idx]
-            dst_range = dst_ranges[dst_idx]
-            
-            # Calculate the length of each range
-            src_len = src_range[-1] - src_range[0] + 1
-            dst_len = dst_range[-1] - dst_range[0] + 1
-            
-            # If ranges have the same length, add them directly
-            if src_len == dst_len:
-                src_overlapping_ranges.append([src_range[0], src_range[-1]])
-                dst_overlapping_ranges.append([dst_range[0], dst_range[-1]])
-                original_src_ranges.append(org_src_range)
-                src_idx += 1
-                dst_idx += 1
-                if src_idx < len(src_ranges):
-                    org_src_range = tuple(src_ranges[src_idx])
-            # If source range is longer, split it
-            elif src_len > dst_len:
-                src_overlapping_ranges.append([src_range[0], src_range[0] + dst_len - 1])
-                dst_overlapping_ranges.append([dst_range[0], dst_range[-1]])
-                original_src_ranges.append(org_src_range)
-                # Update source range for next iteration
-                src_ranges[src_idx] = [src_range[0] + dst_len, src_range[-1]]
-                dst_idx += 1
-            # If destination range is longer, split it
-            else:  # src_len < dst_len
-                src_overlapping_ranges.append([src_range[0], src_range[-1]])
-                dst_overlapping_ranges.append([dst_range[0], dst_range[0] + src_len - 1])
-                original_src_ranges.append(org_src_range)
-                # Update destination range for next iteration
-                dst_ranges[dst_idx] = [dst_range[0] + src_len, dst_range[-1]]
-                src_idx += 1
-                if src_idx < len(src_ranges):
-                    org_src_range = tuple(src_ranges[src_idx])
-        if return_original_src_ranges:
-            return src_overlapping_ranges, dst_overlapping_ranges, original_src_ranges
-        return src_overlapping_ranges, dst_overlapping_ranges
-    
-
-
-    def _get_block_descs_ids(self, engine_id, layer_ids, block_ids, i=None, tp_multiplier=1, staging_ranges=None):
-
-        if layer_ids == "all":
-            layer_ids = list(range(self.num_layers))
-        if block_ids == "all":
-            block_ids = list(range(self.num_blocks))
-
-        descs_ids = []
-
-
-        if i is not None:
-            num_blocks = self.num_blocks
-            for layer_id in layer_ids:
-                for is_value in [0, 1]:
-                    staging_range_idx = 0
-                    for block_id in block_ids:
-                        if block_id > staging_ranges[staging_range_idx][1] or block_id < staging_ranges[staging_range_idx][0]:
-                            staging_range_idx += 1
-                        start_offset = staging_ranges[staging_range_idx][0]
-                        i_offset = i * (staging_ranges[staging_range_idx][-1] - start_offset + 1)
-                        descs_ids.append(layer_id * 2 * num_blocks * tp_multiplier + is_value * num_blocks * tp_multiplier + start_offset * tp_multiplier + i_offset + (block_id - start_offset))
-        else:
-            num_blocks = self.dst_num_blocks[engine_id]
-            for layer_id in layer_ids:
-                for is_value in [0, 1]:
-                    for block_id in block_ids:
-                        descs_ids.append(layer_id * 2 * num_blocks + is_value * num_blocks + block_id)
-        return descs_ids
-                
-    
-    def transfer_mem(self, src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg):
-
-        if self.use_prepped_xfer:
-            self._transfer_mem_prepped_xfer(src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg)
-        else:
-            self._transfer_mem_create_xfer(src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg)
-
-    def _transfer_mem_prepped_xfer(self, src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg):
-        start_time = time.perf_counter()
-        logger.debug("Transferring memory from %s to %s with notify message %s", self.agent_name, dst_engine_id, notify_msg)
-
-        # hongkuanz: we send isl[:-1] tokens to the prefill where the kv for the last
-        # isl[-1] token is calculated in the first iteration in decode.
-        # If isl equals to a multiple of tokens_per_block + 1, prefill engine will have \
-        # one less block due to the missing last token.
-        dst_block_ids = dst_block_ids[:len(src_block_ids)]
-
-        assert len(staging_block_ids) == len(src_block_ids)
-        src_ranges = self._get_ranges(src_block_ids)
-        staging_ranges = self._get_ranges(staging_block_ids)
-
-        src_staging_overlapping_ranges, staging_src_overlapping_ranges = self._get_same_length_ranges(src_ranges, staging_ranges)
-        tp_multiplier = self._tp_size[dst_engine_id] // self._tp_size[self.engine_id]
-        
-        for src_range, staging_range in zip(src_staging_overlapping_ranges, staging_src_overlapping_ranges):
-            logger.debug("Rearranging tensors for cache: %s, src_range: %s, staging_range: %s", self.kv_caches[0].shape, src_range, staging_range)
-            for kv_cache in self.kv_caches:
-                if self.use_mla:
-                    cache_src = kv_cache[src_range[0]:src_range[1] + 1].unsqueeze(0)
-                    cache_staging = kv_cache[staging_range[0]:staging_range[1] + 1].unsqueeze(0)
-                    rearrange_tensors(cache_src, cache_staging, tp_multiplier)
-                else:    
-                    for cache in kv_cache:
-                        rearrange_tensors(cache[src_range[0]:src_range[1] + 1], cache[staging_range[0]:staging_range[1] + 1], tp_multiplier)
-
-        logger.debug("Time to rearrange tensors: %s ms", (time.perf_counter() - start_time) * 1000)
-
-        # getting block descs ids
-        dst_block_descs_ids = self._get_block_descs_ids(dst_engine_id, "all", dst_block_ids)
-        src_xfer_side_handle = self.src_xfer_side_handles[tp_multiplier]
-        
-        for i in range(tp_multiplier):
-            staging_block_descs_ids = self._get_block_descs_ids(self.engine_id, "all", staging_block_ids, i=i, tp_multiplier=tp_multiplier, staging_ranges=staging_src_overlapping_ranges)
-            assert len(staging_block_descs_ids) == len(dst_block_descs_ids)
-            dst_xfer_side_handle = self.dst_xfer_side_handles[dst_engine_id][i]
-
-
-            logger.debug("Time to get block descs ids: %s ms", (time.perf_counter() - start_time) * 1000)
-            handle = self.nixl_wrapper.make_prepped_xfer("WRITE", src_xfer_side_handle, staging_block_descs_ids,
-                                                        dst_xfer_side_handle, dst_block_descs_ids, 
-                                                        notify_msg)
-            self._transfers[notify_msg].append(handle)
-            logger.debug("Time to initialize xfer: %s ms", (time.perf_counter() - start_time) * 1000)
-            logger.debug("Transfer handle: %s", handle)
-            status = self.nixl_wrapper.transfer(handle)
-            logger.debug("Time to transfer: %s ms", (time.perf_counter() - start_time) * 1000)
-            logger.debug("Transfer status: %s", status)
-    
-    def _transfer_mem_create_xfer(self, src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg):
-        start_time = time.perf_counter()
-        logger.debug("Transferring memory from %s to %s with notify message %s", self.agent_name, dst_engine_id, notify_msg)
-
-        # hongkuanz: we send isl[:-1] tokens to the prefill where the kv for the last
-        # isl[-1] token is calculated in the first iteration in decode.
-        # If isl equals to a multiple of tokens_per_block + 1, prefill engine will have \
-        # one less block due to the missing last token.
-        dst_block_ids = dst_block_ids[:len(src_block_ids)]
-        assert len(staging_block_ids) == len(src_block_ids)
-        src_ranges = self._get_ranges(src_block_ids)
-        staging_ranges = self._get_ranges(staging_block_ids)
-        dst_ranges = self._get_ranges(dst_block_ids)
-
-        staging_src_overlapping_ranges, src_staging_overlapping_ranges = self._get_same_length_ranges(staging_ranges, src_ranges)
-        tp_multiplier = self._tp_size[dst_engine_id] // self._tp_size[self.engine_id]
-        
-        for src_range, staging_range in zip(src_staging_overlapping_ranges, staging_src_overlapping_ranges):
-            logger.debug("Rearranging tensors for cache: %s, src_range: %s, staging_range: %s", self.kv_caches[0].shape, src_range, staging_range)
-            for kv_cache in self.kv_caches:
-                if self.use_mla:
-                    cache_src = kv_cache[src_range[0]:src_range[1] + 1].unsqueeze(0)
-                    cache_staging = kv_cache[staging_range[0]:staging_range[1] + 1].unsqueeze(0)
-                    rearrange_tensors(cache_src, cache_staging, tp_multiplier)
-                else:    
-                    for cache in kv_cache:
-                        rearrange_tensors(cache[src_range[0]:src_range[1] + 1], cache[staging_range[0]:staging_range[1] + 1], tp_multiplier)
-
-        staging_overlapping_ranges, dst_overlapping_ranges, original_src_ranges = self._get_same_length_ranges(staging_src_overlapping_ranges, dst_ranges, return_original_src_ranges=True)
-        assert len(staging_overlapping_ranges) == len(dst_overlapping_ranges)
-
-        logger.debug("Time to get same length ranges: %s ms", (time.perf_counter() - start_time) * 1000)
-
-        for i in range(tp_multiplier):
-
-            src_descs = self._get_range_descs(staging_overlapping_ranges, "all", self.kv_caches_base_addr[self.engine_id], tp_multiplier, i=i, staging_ranges=original_src_ranges)
-            dst_descs = self._get_range_descs(dst_overlapping_ranges, "all", self.kv_caches_base_addr[dst_engine_id][self.rank * tp_multiplier + i], tp_multiplier, rank=self.rank * tp_multiplier + i)
-            logger.debug("Time to get descs: %s ms", (time.perf_counter() - start_time) * 1000)
-            
-            logger.debug("Transfering to agent %s", self._remote_agents[dst_engine_id][self.rank * tp_multiplier + i])
-            handle = self.nixl_wrapper.initialize_xfer("WRITE", src_descs, dst_descs,
-                                                        self._remote_agents[dst_engine_id][self.rank * tp_multiplier + i], 
-                                                        notify_msg)
-            self._transfers[notify_msg].append(handle)
-            logger.debug("Time to initialize xfer: %s ms", (time.perf_counter() - start_time) * 1000)
-            logger.debug("Transfer handle: %s", handle)
-            status = self.nixl_wrapper.transfer(handle)
-            logger.debug("Time to transfer: %s ms", (time.perf_counter() - start_time) * 1000)
-            logger.debug("Transfer status: %s", status)
-                
-    def get_notifs(self):
-        return self.nixl_wrapper.update_notifs()
-    
-    def get_new_notifs(self):
-        return self.nixl_wrapper.get_new_notifs()
-
-    def add_remote_agent(self, engine_id, agent_metadata, agent_tp, kv_caches_base_addr, num_blocks):
-        self._tp_size[engine_id] = agent_tp
-        agent_names = []
-        for agent_meta in agent_metadata:
-            agent_name = self.nixl_wrapper.add_remote_agent(agent_meta)
-            agent_names.append(agent_name)
-        self._remote_agents[engine_id] = agent_names
-        self.kv_caches_base_addr[engine_id] = kv_caches_base_addr
-
-        tp_multiplier = self._tp_size[engine_id] // self._tp_size[self.engine_id]
-        assert tp_multiplier > 0, f"Decode TP cannot be smaller than prefill TP, got {self._tp_size[engine_id]} and {self._tp_size[self.engine_id]}"
-
-        logger.debug("Creating src xfer side handles for engine %s, tp_multiplier: %s", engine_id, tp_multiplier)
-        dst_block_len = self.block_len // tp_multiplier
-        if tp_multiplier not in self.src_xfer_side_handles:
-            # create descs and xfer side handles
-            blocks_data = []
-            for layer_id in range(self.num_layers):
-                for base_addr in self.kv_caches_base_addr[self.engine_id][layer_id]:
-                    for block_id in range(self.num_blocks):
-                            block_offset = block_id * self.block_len
-                            for i in range(tp_multiplier):
-                                tp_multiplier_offset = i * dst_block_len
-                                blocks_data.append((base_addr + block_offset + tp_multiplier_offset, dst_block_len, self.rank))
-            logger.debug("Created %s blocks for src engine %s and rank %s", len(blocks_data), self.engine_id, self.rank * tp_multiplier + i)
-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
-            self.src_xfer_side_handles[tp_multiplier] = self.nixl_wrapper.prep_xfer_dlist("", descs)
-
-        # create dst xfer side handles
-        self.dst_num_blocks[engine_id] = num_blocks
-        for i in range(tp_multiplier):
-            blocks_data = []
-            for layer_id in range(self.num_layers):
-                for base_addr in self.kv_caches_base_addr[engine_id][self.rank * tp_multiplier + i][layer_id]:
-                    for block_id in range(num_blocks):
-                        block_offset = block_id * dst_block_len
-                        blocks_data.append((base_addr + block_offset, dst_block_len, self.rank * tp_multiplier + i))
-            logger.debug("Created %s blocks for dst engine %s and rank %s", len(blocks_data), engine_id, self.rank * tp_multiplier + i)
-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
-            self.dst_xfer_side_handles[engine_id][i] = self.nixl_wrapper.prep_xfer_dlist(self._remote_agents[engine_id][self.rank * tp_multiplier + i], descs)
-
-        return agent_names
-
-    def get_done_tranfers(self) -> List[str]:
-        done_req_ids = []
-        for req_id, handles in self._transfers.items():
-            running_reqs = []
-            for handle in handles:
-                xfer_state = self.nixl_wrapper.check_xfer_state(handle)
-                if xfer_state == "DONE":
-                    # self.nixl_wrapper.release_xfer_handle(handle) # TODO ptarasiewicz: why abort is throwing errors?
-                    continue
-                if xfer_state == "PROC":
-                    running_reqs.append(handle)
-                else:
-                    raise RuntimeError("Transfer failed with state %s", xfer_state)
-            if len(running_reqs) == 0:
-                done_req_ids.append(req_id)
-            else:
-                self._transfers[req_id] = running_reqs
-        return done_req_ids
diff --git a/vllm/distributed/kv_transfer/kv_connector/dynamo_connector.py b/vllm/distributed/kv_transfer/kv_connector/dynamo_connector.py
deleted file mode 100644
index 7b3344f8..00000000
--- a/vllm/distributed/kv_transfer/kv_connector/dynamo_connector.py
+++ /dev/null
@@ -1,350 +0,0 @@
-# SPDX-License-Identifier: Apache-2.0
-"""
-Simple KV Cache Connector for Distributed Machine Learning Inference
-
-The SimpleConnector transfers KV caches between prefill vLLM worker (KV cache 
-producer) and decode vLLM worker (KV cache consumer) using PyNcclPipe or
-MooncakePipe.
-
-But the logic can be extended to support other pipe and lookup buffer.
-"""
-import re
-from typing import TYPE_CHECKING, List, Optional, Tuple, Union
-
-import torch
-
-from vllm import _custom_ops as ops
-from vllm.config import VllmConfig, KVTransferConfig
-from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
-from vllm.distributed.utils import StatelessProcessGroup
-from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_buffer import (
-    SimpleBuffer)
-from vllm.logger import init_logger
-from vllm.sequence import IntermediateTensors
-
-if TYPE_CHECKING:
-    from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
-
-logger = init_logger(__name__)
-
-
-class DynamoConnector(KVConnectorBase):
-
-    def __init__(
-        self,
-        rank: int,
-        local_rank: int,
-        config: VllmConfig,
-        world_group,
-    ):
-
-        self.config = config.kv_transfer_config
-        self.tp_size = config.parallel_config.tensor_parallel_size
-        self.rank = rank
-
-        if self.config.kv_connector != "DynamoNcclConnector":
-            raise NotImplementedError("Only DynamoNcclConnector is supported by the DynamoConnector class")
-
-        from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import (
-            PyNcclPipe)
-        from vllm.distributed.kv_transfer.kv_pipe.dynamo_nccl_pipe import (
-            DynamoNcclDataPlane)
-        
-        logger.info(
-            "Initializing DynamoNcclConnector under kv_transfer_config %s",
-            self.config)
-
-        self.lookup_buffer_size = self.config.kv_buffer_size
-
-        self.producer_data_pipe: PyNcclPipe
-        self.consumer_data_pipe: PyNcclPipe
-        self.producer_signal_pipe: PyNcclPipe
-        self.consumer_signal_pipe: PyNcclPipe
-
-        self._broadcast_and_enhance_kv_config(rank, config, world_group)
-
-        self.kv_group_rank = self._get_kv_group_rank(self.config.kv_rank, rank, self.config)
-        self.tp_size = config.parallel_config.tensor_parallel_size
-
-        # 2 pipes for every rank in the world
-        if self.config.is_kv_producer:
-            port_offset_base = rank + 1
-        else:
-            port_offset_base = rank // self.config.tensor_parallel_multiplier + 1
-
-
-        self.local_kv_rank = rank % self.config.tensor_parallel_multiplier
-        self.global_kv_rank = self._get_global_kv_rank(self.config.kv_rank, rank, self.config)
-
-        self.data_pipe = PyNcclPipe(
-            kv_group_rank=self.kv_group_rank,
-            local_rank=local_rank,
-            config=self.config,
-            port_offset=port_offset_base,
-        )
-
-        self.data_plane = DynamoNcclDataPlane(
-            data_pipe=self.data_pipe,
-            port=self._get_data_plane_port(self.global_kv_rank),
-        )
-
-    def send_kv_caches_and_hidden_states(
-        self,
-        model_executable: torch.nn.Module,
-        model_input: "ModelInputForGPUWithSamplingMetadata",
-        kv_caches: List[torch.Tensor],
-        hidden_or_intermediate_states: Union[torch.Tensor,
-                                             IntermediateTensors],
-    ) -> None:
-
-        input_tokens_tensor = model_input.input_tokens
-        seq_lens = model_input.attn_metadata.seq_lens
-        slot_mapping_flat = model_input.attn_metadata.slot_mapping.flatten()
-        start_layer = model_executable.model.start_layer
-        end_layer = model_executable.model.end_layer
-        request_ids = list(model_input.request_ids_to_seq_ids.keys())
-
-        model_config = model_executable.model.config
-        is_deepseek = "deepseek" in model_config.architectures[0].lower()
-        if not is_deepseek:
-            num_heads = int(model_config.num_key_value_heads / self.tp_size)
-            hidden_size = model_config.hidden_size
-            num_attention_heads = model_config.num_attention_heads
-            head_size = int(hidden_size / num_attention_heads)
-        else:
-            num_heads = int(model_config.num_key_value_heads / self.tp_size)
-            hidden_size = model_config.hidden_size
-            num_attention_heads = model_config.num_attention_heads
-            head_size = int(4.5 * hidden_size / num_attention_heads)
-
-        # query_lens contains new KV caches that are added to vLLM.
-        # so we will send them to decode instance
-        # FIXME(Kuntai): This assume that all requests are prefill.
-        for idx, slen in enumerate(seq_lens):
-            start_pos = sum(seq_lens[:idx])
-            end_pos = start_pos + slen
-            current_tokens = input_tokens_tensor[start_pos:end_pos]
-            current_request_id = request_ids[idx]
-            decode_hostname, decode_kv_rank = self.parse_request_id(current_request_id)
-            decode_first_global_rank = self._get_global_kv_rank(decode_kv_rank, self.rank * self.config.tensor_parallel_multiplier, self.config)
-
-            for target_rank in range(self.config.tensor_parallel_multiplier):
-
-                keys, values = [], []
-
-                for layer_id in range(start_layer, end_layer):
-                    kv_cache = kv_caches[layer_id - start_layer]
-
-                    current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
-
-                    num_heads_per_rank = num_heads // self.config.tensor_parallel_multiplier
-                    head_start = target_rank * num_heads_per_rank
-                    head_end = head_start + num_heads_per_rank
-
-                    if not is_deepseek:
-                        key_cache = kv_cache[0].reshape(-1, num_heads, head_size)
-                        value_cache = kv_cache[1].reshape(-1, num_heads, head_size)
-                        keys.append(key_cache[current_slot_mapping, head_start:head_end].unsqueeze(0))
-                        values.append(value_cache[current_slot_mapping, head_start:head_end].unsqueeze(0))
-                    else:
-                        key_cache = kv_cache
-                        keys.append(key_cache[current_slot_mapping].unsqueeze(0))
-                        values.append(torch.empty(0))
-
-                keys = torch.cat(keys, dim=0)
-                values = torch.cat(values, dim=0)
-
-                decode_global_rank = decode_first_global_rank + target_rank
-                decode_port = self._get_data_plane_port(decode_global_rank)
-                partial_hidden_or_intermediate_states = hidden_or_intermediate_states[start_pos:end_pos]
-                self._send(decode_hostname, decode_port, current_request_id, keys, values,
-                            partial_hidden_or_intermediate_states)
-
-        logger.debug("[rank%d]: KV send DONE.", torch.distributed.get_rank())
-
-    def recv_kv_caches_and_hidden_states(
-        self, model_executable: torch.nn.Module,
-        model_input: "ModelInputForGPUWithSamplingMetadata",
-        kv_caches: List[torch.Tensor]
-    ) -> Tuple[Union[torch.Tensor, IntermediateTensors], bool,
-               "ModelInputForGPUWithSamplingMetadata"]:
-
-        # When bypass_model_exec is set to False, it means that at least for one
-        # request its corresponding KV cache or hidden state is missing.
-        # In this case we need to do prefilling to recompute missing KV cache
-        # and hidden states.
-        bypass_model_exec = True
-
-        input_tokens_tensor = model_input.input_tokens
-        seq_lens = model_input.attn_metadata.seq_lens
-        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
-        request_ids = list(model_input.request_ids_to_seq_ids.keys())
-
-        hidden_or_intermediate_states_for_one_req = []
-
-        input_tokens_list = []
-        start_pos_list = []
-
-        model_config = model_executable.model.config
-        is_deepseek = "deepseek" in model_config.architectures[0].lower()
-
-        # enumerate different requests
-        # FIXME(Kuntai): This impl assumes that all requests are prefill.
-        for idx, slen in enumerate(seq_lens):
-
-            start_pos = sum(seq_lens[:idx])
-            end_pos = start_pos + slen
-            current_tokens = input_tokens_tensor[start_pos:end_pos]
-            current_request_id = request_ids[idx]
-            num_tokens = slen
-
-            # collecting data for rebuilding the input
-            input_tokens_list.append(current_tokens)
-            start_pos_list.append(start_pos)
-
-            ret = self._recv(current_request_id)
-            keys: torch.Tensor = ret[0]
-            values: torch.Tensor = ret[1]
-            hidden: torch.Tensor = ret[2]
-
-            # put received KV caches into paged memory
-            for i in range(model_executable.model.start_layer,
-                           model_executable.model.end_layer):
-
-                kv_cache = kv_caches[i - model_executable.model.start_layer]
-                layer = model_executable.model.layers[i]
-
-                if not is_deepseek:
-                    key_cache, value_cache = kv_cache[0], kv_cache[1]
-                    ops.reshape_and_cache_flash(
-                        keys[i - model_executable.model.start_layer].to(
-                            key_cache.device),
-                        values[i - model_executable.model.start_layer].to(
-                            value_cache.device),
-                        key_cache,
-                        value_cache,
-                        slot_mapping[start_pos:end_pos],
-                        layer.self_attn.attn.kv_cache_dtype,
-                        layer.self_attn.attn._k_scale,
-                        layer.self_attn.attn._v_scale,
-                    )
-                else:
-                    key_cache = kv_cache
-                    copy_from =keys[i - model_executable.model.start_layer].to(
-                            key_cache.device)
-                    kv_cache[slot_mapping[start_pos:end_pos]] = copy_from
-
-            hidden_or_intermediate_states_for_one_req.append(hidden)
-
-        if not bypass_model_exec:
-            # Some of the KV cache is not retrieved
-            # Here we will fall back to normal model forwarding
-            # But optionally you can adjust model_input so that you only do
-            # prefilling on those tokens that are missing KV caches.
-            logger.debug(
-                "[rank%d]: Failed to receive all KVs and hidden "
-                "states, redo model forwarding.", torch.distributed.get_rank())
-            hidden_or_intermediate_states = None
-
-        else:
-            logger.debug(
-                "[rank%d]: Successfully received all KVs and hidden "
-                "states, skip model forwarding.", torch.distributed.get_rank())
-            hidden_or_intermediate_states = torch.cat(
-                hidden_or_intermediate_states_for_one_req, dim=0)
-
-        return hidden_or_intermediate_states, bypass_model_exec, model_input
-
-    def close(self):
-        self.data_pipe.close()
-        # self.data_plane.close()
-
-    @staticmethod
-    def parse_request_id(request_id: str) -> Tuple[str, int]:
-        # Regular expression to match the string hostname and integer decode_kv_rank
-        pattern = r"___decode_hostname_(.*)___decode_kv_rank_(\d+)"
-        
-        # Use re.search to find the pattern in the request_id
-        match = re.search(pattern, request_id)
-        if match:
-            # Extract the ranks
-            decode_hostname = match.group(1)
-            decode_rank = int(match.group(2))
-            
-            return decode_hostname, decode_rank
-        raise ValueError(f"Request id {request_id} does not contain hostname and decode_kv_rank")
-
-    def _send(self, hostname: str, port: int, request_id: str, keys: torch.Tensor, values: torch.Tensor, hidden: torch.Tensor):
-        remote_address = f"{hostname}:{port}"
-        self.data_plane.send_tensor(keys, f"{request_id}_keys", remote_address)
-        self.data_plane.send_tensor(values, f"{request_id}_values", remote_address)
-        self.data_plane.send_tensor(hidden, f"{request_id}_hidden", remote_address)
-
-    def _recv(self, request_id: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        keys = self.data_plane.recv_tensor(f"{request_id}_keys")
-        values = self.data_plane.recv_tensor(f"{request_id}_values")
-        hidden = self.data_plane.recv_tensor(f"{request_id}_hidden")
-        return keys, values, hidden
-
-    def _get_kv_group_rank(self, kv_rank: int, rank: int, config: KVTransferConfig) -> int:
-        if kv_rank < config.kv_producers_parallel_size:
-            return kv_rank
-        
-        kv_consumer_rank = kv_rank - config.kv_producers_parallel_size
-        return config.kv_producers_parallel_size + kv_consumer_rank * config.tensor_parallel_multiplier + rank % config.tensor_parallel_multiplier
-    
-
-    def _get_global_kv_rank(self, kv_rank: int, rank: int, config: KVTransferConfig) -> int:
-        if kv_rank <= config.kv_producers_parallel_size:
-            return kv_rank * config.kv_producers_tensor_parallel_size + rank
-        
-        kv_consumer_rank = kv_rank - config.kv_producers_parallel_size
-        return config.kv_producers_parallel_size * config.kv_producers_tensor_parallel_size + kv_consumer_rank * config.kv_consumers_tensor_parallel_size + rank
-
-
-    def _get_data_plane_port(self, global_kv_rank: int) -> int:
-        return self.config.kv_port + self.config.kv_producers_tensor_parallel_size + 1 + global_kv_rank
-
-    def _broadcast_and_enhance_kv_config(self, rank: int, config: VllmConfig, world_group):
-        if rank == 0:
-            config_group = StatelessProcessGroup.create(
-                host=self.config.kv_ip,
-                port=self.config.kv_port,
-                rank=self.config.kv_rank,
-                world_size=self.config.kv_parallel_size,
-            )
-            parallel_configs = config_group.all_gather_obj({
-                "kv_role": self.config.kv_role,
-                "tensor_parallel_size": config.parallel_config.tensor_parallel_size,
-                "pipeline_parallel_size": config.parallel_config.pipeline_parallel_size,
-            })
-            logger.debug("parallel_configs: %s", parallel_configs)
-            kv_config_enhanced = {
-                "kv_producers_tensor_parallel_size": None,
-                "kv_consumers_tensor_parallel_size": None,
-                "kv_producers_pipeline_parallel_size": None,
-                "kv_consumers_pipeline_parallel_size": None,
-                "kv_producers_parallel_size": 0,
-            }
-            for parallel_config in parallel_configs:
-                kv_role = parallel_config["kv_role"]
-                assert parallel_config["pipeline_parallel_size"] == 1, f"Only pipeline parallel size 1 is supported for kv transfer instances"
-                
-                if kv_role == "kv_producer":
-                    kv_config_enhanced["kv_producers_parallel_size"] += 1
-                if kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] is None:
-                    kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] = parallel_config["tensor_parallel_size"]
-                    kv_config_enhanced[f"{kv_role}s_pipeline_parallel_size"] = parallel_config["pipeline_parallel_size"]
-                else:
-                    assert kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] == parallel_config["tensor_parallel_size"], f"All kv {kv_role}s should have the same tensor parallel size"
-                    assert kv_config_enhanced[f"{kv_role}s_pipeline_parallel_size"] == parallel_config["pipeline_parallel_size"], f"All kv {kv_role}s should have the same pipeline parallel size"
-            world_group.broadcast_object(kv_config_enhanced)
-        else:
-            kv_config_enhanced = world_group.broadcast_object()
-        logger.info("kv_config_enhanced: %s", kv_config_enhanced)
-
-        self.config.kv_producers_tensor_parallel_size = kv_config_enhanced["kv_producers_tensor_parallel_size"]
-        self.config.kv_consumers_tensor_parallel_size = kv_config_enhanced["kv_consumers_tensor_parallel_size"]
-        self.config.kv_producers_pipeline_parallel_size = kv_config_enhanced["kv_producers_pipeline_parallel_size"]
-        self.config.kv_consumers_pipeline_parallel_size = kv_config_enhanced["kv_consumers_pipeline_parallel_size"]
-        self.config.kv_producers_parallel_size = kv_config_enhanced["kv_producers_parallel_size"]
diff --git a/vllm/distributed/kv_transfer/kv_connector/factory.py b/vllm/distributed/kv_transfer/kv_connector/factory.py
index 494e840f..7336c54e 100644
--- a/vllm/distributed/kv_transfer/kv_connector/factory.py
+++ b/vllm/distributed/kv_transfer/kv_connector/factory.py
@@ -27,13 +27,13 @@ class KVConnectorFactory:
 
     @classmethod
     def create_connector(cls, rank: int, local_rank: int,
-                         config: "VllmConfig", world_group) -> KVConnectorBase:
+                         config: "VllmConfig") -> KVConnectorBase:
         connector_name = config.kv_transfer_config.kv_connector
         if connector_name not in cls._registry:
             raise ValueError(f"Unsupported connector type: {connector_name}")
 
         connector_cls = cls._registry[connector_name]()
-        return connector_cls(rank, local_rank, config, world_group)
+        return connector_cls(rank, local_rank, config)
 
 
 # Register various connectors here.
@@ -53,8 +53,3 @@ KVConnectorFactory.register_connector(
     "LMCacheConnector",
     "vllm.distributed.kv_transfer.kv_connector.lmcache_connector",
     "LMCacheConnector")
-
-KVConnectorFactory.register_connector(
-    "DynamoNcclConnector",
-    "vllm.distributed.kv_transfer.kv_connector.dynamo_connector",
-    "DynamoConnector")
diff --git a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
index 8bbcf715..7315a6f4 100644
--- a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
@@ -8,15 +8,13 @@ MooncakePipe.
 
 But the logic can be extended to support other pipe and lookup buffer.
 """
-import re
 from typing import TYPE_CHECKING, List, Optional, Tuple, Union
 
 import torch
 
 from vllm import _custom_ops as ops
-from vllm.config import VllmConfig, KVTransferConfig
+from vllm.config import VllmConfig
 from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
-from vllm.distributed.utils import StatelessProcessGroup
 from vllm.distributed.kv_transfer.kv_lookup_buffer.simple_buffer import (
     SimpleBuffer)
 from vllm.logger import init_logger
@@ -35,7 +33,6 @@ class SimpleConnector(KVConnectorBase):
         rank: int,
         local_rank: int,
         config: VllmConfig,
-        world_group,
     ):
 
         self.config = config.kv_transfer_config
@@ -74,31 +71,20 @@ class SimpleConnector(KVConnectorBase):
         self.producer_signal_pipe: Union[PyNcclPipe, MooncakePipe]
         self.consumer_signal_pipe: Union[PyNcclPipe, MooncakePipe]
 
-        self._broadcast_and_enhance_kv_config(rank, config, world_group)
-
-        self.kv_group_rank = self._get_kv_group_rank(self.config.kv_rank, rank, self.config)
-        self.tp_size = config.parallel_config.tensor_parallel_size
-
         # 2 pipes for every rank in the world
-        if self.config.is_kv_producer:
-            port_offset_base = 2 * rank + 1
-        else:
-            port_offset_base = 2 * (rank // self.config.tensor_parallel_multiplier) + 1
+        port_offset_base = 2 * rank
 
-        self.local_kv_rank = rank % self.config.tensor_parallel_multiplier
         # In disaggregated prefill, the prefill vLLM only uses send pipe
         # and the decode vLLM only uses recv pipe
         if self.config.is_kv_producer:
 
             if self.config.kv_connector == "PyNcclConnector":
                 self.producer_data_pipe = PyNcclPipe(
-                    kv_group_rank=self.kv_group_rank,
                     local_rank=local_rank,
                     config=self.config,
                     port_offset=port_offset_base,
                 )
                 self.producer_signal_pipe = PyNcclPipe(
-                    kv_group_rank=self.kv_group_rank,
                     local_rank=local_rank,
                     config=self.config,
                     port_offset=port_offset_base + 1,
@@ -122,13 +108,11 @@ class SimpleConnector(KVConnectorBase):
             # its recv pipe to the send pipe of KV producder
             if self.config.kv_connector == "PyNcclConnector":
                 self.consumer_data_pipe = PyNcclPipe(
-                    kv_group_rank=self.kv_group_rank,
                     local_rank=local_rank,
                     config=self.config,
                     port_offset=port_offset_base,
                 )
                 self.consumer_signal_pipe = PyNcclPipe(
-                    kv_group_rank=self.kv_group_rank,
                     local_rank=local_rank,
                     config=self.config,
                     port_offset=port_offset_base + 1,
@@ -147,25 +131,21 @@ class SimpleConnector(KVConnectorBase):
                 self.config.kv_buffer_size,
             )
 
-    def select(self, source_rank: int, input_tokens: Optional[torch.Tensor],
+    def select(self, input_tokens: Optional[torch.Tensor],
                roi: Optional[torch.Tensor]) -> List[Optional[torch.Tensor]]:
 
-        logger.info("Selecting KV caches and hidden states for source rank %d", source_rank)
-
         assert self.consumer_buffer is not None, "Please initialize the "\
             "consumer buffer before calling select."
-        return self.consumer_buffer.drop_select(source_rank, self.local_kv_rank, input_tokens, roi)
+        return self.consumer_buffer.drop_select(input_tokens, roi)
 
-    def insert(self, kv_group_rank: int, target_rank: int, input_tokens: torch.Tensor, roi: torch.Tensor,
+    def insert(self, input_tokens: torch.Tensor, roi: torch.Tensor,
                key: torch.Tensor, value: torch.Tensor,
                hidden: torch.Tensor) -> None:
 
-        logger.info("Inserting KV caches and hidden states for kv_group_rank %d, target rank %d", kv_group_rank, target_rank)
-
         assert self.producer_buffer is not None, "Please initialize the "\
             "producer buffer before calling insert."
 
-        self.producer_buffer.insert(kv_group_rank, target_rank, input_tokens, roi, key, value, hidden)
+        self.producer_buffer.insert(input_tokens, roi, key, value, hidden)
 
     def send_kv_caches_and_hidden_states(
         self,
@@ -182,20 +162,13 @@ class SimpleConnector(KVConnectorBase):
         num_prefill_tokens = model_input.attn_metadata.num_prefill_tokens
         start_layer = model_executable.model.start_layer
         end_layer = model_executable.model.end_layer
-        request_ids = list(model_input.request_ids_to_seq_ids.keys())
 
         model_config = model_executable.model.config
-        is_deepseek = "deepseek" in model_config.architectures[0].lower()
-        if not is_deepseek:
-            num_heads = int(model_config.num_key_value_heads / self.tp_size)
-            hidden_size = model_config.hidden_size
-            num_attention_heads = model_config.num_attention_heads
-            head_size = int(hidden_size / num_attention_heads)
-        else:
-            num_heads = int(model_config.num_key_value_heads / self.tp_size)
-            hidden_size = model_config.hidden_size
-            num_attention_heads = model_config.num_attention_heads
-            head_size = int(4.5 * hidden_size / num_attention_heads)
+        num_heads = int(model_config.num_key_value_heads / self.tp_size)
+        hidden_size = model_config.hidden_size
+        num_attention_heads = model_config.num_attention_heads
+        head_size = getattr(model_config, "head_dim",
+                            int(hidden_size // num_attention_heads))
 
         # query_lens contains new KV caches that are added to vLLM.
         # so we will send them to decode instance
@@ -213,40 +186,27 @@ class SimpleConnector(KVConnectorBase):
                 break
 
             current_tokens = input_tokens_tensor[start_pos:end_pos]
-            current_request_id = request_ids[idx]
-            _, decode_kv_rank = self.parse_request_id(current_request_id)
-            starting_kv_group_rank = self._get_kv_group_rank(decode_kv_rank, 0, self.config)
-
-            for target_rank in range(self.config.tensor_parallel_multiplier):
 
-                keys, values = [], []
+            keys, values = [], []
 
-                for layer_id in range(start_layer, end_layer):
-                    kv_cache = kv_caches[layer_id - start_layer]
+            for layer_id in range(start_layer, end_layer):
+                kv_cache = kv_caches[layer_id - start_layer]
 
-                    current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
+                key_cache = kv_cache[0].reshape(-1, num_heads, head_size)
+                value_cache = kv_cache[1].reshape(-1, num_heads, head_size)
 
-                    num_heads_per_rank = num_heads // self.config.tensor_parallel_multiplier
-                    head_start = target_rank * num_heads_per_rank
-                    head_end = head_start + num_heads_per_rank
+                current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
 
-                    if not is_deepseek:
-                        key_cache = kv_cache[0].reshape(-1, num_heads, head_size)
-                        value_cache = kv_cache[1].reshape(-1, num_heads, head_size)
-                        keys.append(key_cache[current_slot_mapping, head_start:head_end].unsqueeze(0))
-                        values.append(value_cache[current_slot_mapping, head_start:head_end].unsqueeze(0))
-                    else:
-                        key_cache = kv_cache
-                        keys.append(key_cache[current_slot_mapping].unsqueeze(0))
-                        values.append(torch.empty(0))
+                keys.append(key_cache[current_slot_mapping].unsqueeze(0))
+                values.append(value_cache[current_slot_mapping].unsqueeze(0))
 
-                keys = torch.cat(keys, dim=0)
-                values = torch.cat(values, dim=0)
+            keys = torch.cat(keys, dim=0)
+            values = torch.cat(values, dim=0)
 
-                self.insert(starting_kv_group_rank, target_rank, current_tokens,
-                            torch.ones_like(current_tokens,
-                                            dtype=bool), keys, values,
-                            hidden_or_intermediate_states[start_pos:end_pos])
+            self.insert(current_tokens,
+                        torch.ones_like(current_tokens,
+                                        dtype=bool), keys, values,
+                        hidden_or_intermediate_states[start_pos:end_pos])
 
         logger.debug("[rank%d]: KV send DONE.", torch.distributed.get_rank())
 
@@ -267,7 +227,6 @@ class SimpleConnector(KVConnectorBase):
         seq_lens = model_input.attn_metadata.seq_lens
         num_prefill_tokens = model_input.attn_metadata.num_prefill_tokens
         slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
-        request_ids = list(model_input.request_ids_to_seq_ids.keys())
 
         hidden_or_intermediate_states_for_one_req = []
 
@@ -275,9 +234,6 @@ class SimpleConnector(KVConnectorBase):
         num_computed_tokens_list = []
         start_pos_list = []
 
-        model_config = model_executable.model.config
-        is_deepseek = "deepseek" in model_config.architectures[0].lower()
-
         # enumerate different requests
         # FIXME(Kuntai): This impl assumes that all requests are prefill.
         for idx, slen in enumerate(seq_lens):
@@ -297,15 +253,13 @@ class SimpleConnector(KVConnectorBase):
                 break
 
             current_tokens = input_tokens_tensor[start_pos:end_pos]
-            current_request_id = request_ids[idx]
-            prefill_rank, _ = self.parse_request_id(current_request_id)
             num_tokens = slen
 
             # collecting data for rebuilding the input
             input_tokens_list.append(current_tokens)
             start_pos_list.append(start_pos)
 
-            ret = self.select(prefill_rank, current_tokens,
+            ret = self.select(current_tokens,
                               torch.ones_like(current_tokens, dtype=bool))
             if ret[0] is None:
                 # didn't find any match.
@@ -337,25 +291,19 @@ class SimpleConnector(KVConnectorBase):
                 kv_cache = kv_caches[i - model_executable.model.start_layer]
                 layer = model_executable.model.layers[i]
 
-                if not is_deepseek:
-                    key_cache, value_cache = kv_cache[0], kv_cache[1]
-                    ops.reshape_and_cache_flash(
-                        keys[i - model_executable.model.start_layer].to(
-                            key_cache.device),
-                        values[i - model_executable.model.start_layer].to(
-                            value_cache.device),
-                        key_cache,
-                        value_cache,
-                        slot_mapping[start_pos:end_pos],
-                        layer.self_attn.attn.kv_cache_dtype,
-                        layer.self_attn.attn._k_scale,
-                        layer.self_attn.attn._v_scale,
-                    )
-                else:
-                    key_cache = kv_cache
-                    copy_from =keys[i - model_executable.model.start_layer].to(
-                            key_cache.device)
-                    kv_cache[slot_mapping[start_pos:end_pos]] = copy_from
+                key_cache, value_cache = kv_cache[0], kv_cache[1]
+                ops.reshape_and_cache_flash(
+                    keys[i - model_executable.model.start_layer].to(
+                        key_cache.device),
+                    values[i - model_executable.model.start_layer].to(
+                        value_cache.device),
+                    key_cache,
+                    value_cache,
+                    slot_mapping[start_pos:end_pos],
+                    layer.self_attn.attn.kv_cache_dtype,
+                    layer.self_attn.attn._k_scale,
+                    layer.self_attn.attn._v_scale,
+                )
 
             hidden_or_intermediate_states_for_one_req.append(hidden)
 
@@ -388,77 +336,3 @@ class SimpleConnector(KVConnectorBase):
             # MooncakePipe reuses data_pipe for signal_pipe, so we only have to
             # close the data_pipe.
             pass
-
-    @staticmethod
-    def parse_request_id(request_id):
-        # Regular expression to match the ranks
-        pattern = r"___prefill_kv_rank_(\d+)___decode_kv_rank_(\d+)"
-        
-        # Use re.search to find the pattern in the request_id
-        match = re.search(pattern, request_id)
-        
-        if match:
-            # Extract the ranks
-            prefill_rank = int(match.group(1))
-            decode_rank = int(match.group(2))
-            
-            return prefill_rank, decode_rank
-        else:
-            return None, None
-
-    
-
-    def _get_kv_group_rank(self, kv_rank: int, rank: int, config: KVTransferConfig) -> int:
-        if kv_rank < config.kv_producers_parallel_size:
-            return kv_rank
-        
-        kv_consumer_rank = kv_rank - config.kv_producers_parallel_size
-        return config.kv_producers_parallel_size + kv_consumer_rank * config.tensor_parallel_multiplier + rank % config.tensor_parallel_multiplier
-
-    def _broadcast_and_enhance_kv_config(self, rank: int, config: VllmConfig, world_group):
-        if rank == 0:
-            if self.config.kv_connector == "PyNcclConnector":
-                config_group = StatelessProcessGroup.create(
-                    host=self.config.kv_ip,
-                    port=self.config.kv_port,
-                    rank=self.config.kv_rank,
-                    world_size=self.config.kv_parallel_size,
-                )
-                parallel_configs = config_group.all_gather_obj({
-                    "kv_role": self.config.kv_role,
-                    "tensor_parallel_size": config.parallel_config.tensor_parallel_size,
-                    "pipeline_parallel_size": config.parallel_config.pipeline_parallel_size,
-                })
-                logger.debug("parallel_configs: %s", parallel_configs)
-                kv_config_enhanced = {
-                    "kv_producers_tensor_parallel_size": None,
-                    "kv_consumers_tensor_parallel_size": None,
-                    "kv_producers_pipeline_parallel_size": None,
-                    "kv_consumers_pipeline_parallel_size": None,
-                    "kv_producers_parallel_size": 0,
-                }
-                for parallel_config in parallel_configs:
-                    kv_role = parallel_config["kv_role"]
-                    assert parallel_config["pipeline_parallel_size"] == 1, f"Only pipeline parallel size 1 is supported for kv transfer instances"
-                    
-                    if kv_role == "kv_producer":
-                        kv_config_enhanced["kv_producers_parallel_size"] += 1
-                    if kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] is None:
-                        kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] = parallel_config["tensor_parallel_size"]
-                        kv_config_enhanced[f"{kv_role}s_pipeline_parallel_size"] = parallel_config["pipeline_parallel_size"]
-                    else:
-                        assert kv_config_enhanced[f"{kv_role}s_tensor_parallel_size"] == parallel_config["tensor_parallel_size"], f"All kv {kv_role}s should have the same tensor parallel size"
-                        assert kv_config_enhanced[f"{kv_role}s_pipeline_parallel_size"] == parallel_config["pipeline_parallel_size"], f"All kv {kv_role}s should have the same pipeline parallel size"
-                world_group.broadcast_object(kv_config_enhanced)
-
-            else:
-                raise NotImplementedError("MooncakeConnector is not supported in Dynamo patch")
-        else:
-            kv_config_enhanced = world_group.broadcast_object()
-        logger.info("kv_config_enhanced: %s", kv_config_enhanced)
-
-        self.config.kv_producers_tensor_parallel_size = kv_config_enhanced["kv_producers_tensor_parallel_size"]
-        self.config.kv_consumers_tensor_parallel_size = kv_config_enhanced["kv_consumers_tensor_parallel_size"]
-        self.config.kv_producers_pipeline_parallel_size = kv_config_enhanced["kv_producers_pipeline_parallel_size"]
-        self.config.kv_consumers_pipeline_parallel_size = kv_config_enhanced["kv_consumers_pipeline_parallel_size"]
-        self.config.kv_producers_parallel_size = kv_config_enhanced["kv_producers_parallel_size"]
diff --git a/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
index 4e528288..10bbfe1d 100644
--- a/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
+++ b/vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
@@ -11,8 +11,7 @@
 """
 import threading
 from collections import deque
-from concurrent.futures import ThreadPoolExecutor
-from typing import Deque, List, Optional, Union, Dict
+from typing import Deque, List, Optional, Union
 
 import torch
 
@@ -46,7 +45,7 @@ class SimpleBuffer(KVLookupBufferBase):
         self.buffer_cv = threading.Condition()
         self.signal_pipe = signal_pipe
         self.data_pipe = data_pipe
-        self.request_handling_thread: Optional[ThreadPoolExecutor] = None
+        self.request_handling_thread: Optional[threading.Thread] = None
 
         self.normal_signal = torch.tensor([0], device="cpu")
         self.end_signal = None
@@ -57,16 +56,10 @@ class SimpleBuffer(KVLookupBufferBase):
         # tokens_roi_sender: tokens and roi of the producer (in the buffer)
         # tokens_roi_recver: tokens and roi of the consumer (query)
 
-        target_rank_sender = tokens_roi_sender[0]
-        target_rank_recver = tokens_roi_recver[0]
-
-        if target_rank_sender.item() != target_rank_recver.item():
-            return 0
-        
-        tokens_sender = tokens_roi_sender[1]
-        tokens_recver = tokens_roi_recver[1]
-        roi_sender = tokens_roi_sender[2]
-        roi_recver = tokens_roi_recver[2]
+        tokens_sender = tokens_roi_sender[0]
+        tokens_recver = tokens_roi_recver[0]
+        roi_sender = tokens_roi_sender[1]
+        roi_recver = tokens_roi_recver[1]
 
         if tokens_recver is None:
             # consumer sends an empty request
@@ -86,14 +79,14 @@ class SimpleBuffer(KVLookupBufferBase):
 
         return 0
 
-    def _send_tensor_and_dec_size(self, tensor: Optional[torch.Tensor],
-                                  target_rank: int) -> None:
+    def _send_tensor_and_dec_size(self,
+                                  tensor: Optional[torch.Tensor]) -> None:
 
         assert tensor is not None, "Use self.data_pipe.send(None) instead"
         self.buffer_size -= tensor.element_size() * tensor.numel()
         if tensor.dtype == torch.bool:
             tensor = tensor.float()
-        self.data_pipe.send_tensor(tensor, target_rank)
+        self.data_pipe.send_tensor(tensor)
 
     def _get_element_size(self, data: Optional[Union[List, torch.Tensor]]):
 
@@ -106,7 +99,7 @@ class SimpleBuffer(KVLookupBufferBase):
 
         raise AssertionError(f"Unknown data type {type(data)}")
 
-    def _add_to_buffer(self, target_rank: int, input_tokens: torch.Tensor, roi: torch.Tensor,
+    def _add_to_buffer(self, input_tokens: torch.Tensor, roi: torch.Tensor,
                        key: torch.Tensor, value: torch.Tensor,
                        hidden: torch.Tensor):
 
@@ -121,7 +114,16 @@ class SimpleBuffer(KVLookupBufferBase):
         if isinstance(hidden, torch.Tensor):
             hidden = hidden.clone()
 
-        buffer_item = [torch.tensor(target_rank), input_tokens, roi, key, value, hidden]
+        buffer_item = [input_tokens, roi, key, value, hidden]
+        data_size = sum([self._get_element_size(data) for data in buffer_item])
+
+        with self.buffer_cv:
+            if self.buffer_size + data_size > self.buffer_size_threshold:
+                # log outside the while loop to avoid this message being logged
+                # repeatedly.
+                logger.debug("KV transfer buffer is full. Handling...")
+                while self.buffer_size + data_size > self.buffer_size_threshold:
+                    self.buffer_cv.wait()
 
             self.buffer_size += data_size
             self.buffer.append(buffer_item)
@@ -130,54 +132,49 @@ class SimpleBuffer(KVLookupBufferBase):
     def _is_end_signal(self, signal):
         return signal is None
 
-    def drop_select_handler(self, rank: int):
+    def drop_select_handler(self):
 
         try:
 
-            signal = self.signal_pipe.recv_tensor(rank)
-            if self._is_end_signal(signal):
-                logger.info("Received end signal!")
-                return
-            target_kv_rank = self.data_pipe.recv_tensor(rank)
-            # assert target_rank.item() == rank, "Target rank does not match"\
-            #     "the rank of the drop-select handler"
-            input_tokens = self.data_pipe.recv_tensor(rank)
-            roi = self.data_pipe.recv_tensor(rank)
-            assert roi is not None, "Please provide the roi when sending "\
-                "drop-select request"
-            roi = (roi > 0.5)
-            tokens_roi_recver = [target_kv_rank, input_tokens, roi]
-
-            matched_length = 0
-
-            # perform input tokens and roi matching
-            # FIXME: this matching is O(n), ideally it should be O(1)
-            # but this buffer size won't (and shouldn't) be too large so
-            # the fix is not urgent.
-            with self.buffer_lock:
-
-                for _ in range(len(self.buffer)):
-
-                    temp_length = self._matches(self.buffer[0],
-                                                tokens_roi_recver)
-                    if temp_length > 0:
-                        matched_length = temp_length
-                        break
-                    # rotate the element we just accessed to the end
-                    self.buffer.rotate(-1)
-
-                if matched_length > 0:
+            while True:
+                signal = self.signal_pipe.recv_tensor()
+                if self._is_end_signal(signal):
+                    logger.info("Received end signal!")
+                    break
+
+                input_tokens = self.data_pipe.recv_tensor()
+
+                roi = self.data_pipe.recv_tensor()
+                assert roi is not None, "Please provide the roi when sending "\
+                    "drop-select request"
+                roi = (roi > 0.5)
+                tokens_roi_recver = [input_tokens, roi]
+
+                def is_buffer_available(
+                    tokens_roi_recver: List[torch.Tensor], ) -> bool:
+                    # perform input tokens and roi matching
+                    # FIXME: this matching is O(n), ideally it should be O(1)
+                    # but this buffer size won't (and shouldn't) be too large so
+                    # the fix is not urgent.
+                    for _ in range(len(self.buffer)):
+                        if self._matches(self.buffer[0],
+                                         tokens_roi_recver) > 0:
+                            return True
+                        # rotate the element we just accessed to the end
+                        self.buffer.rotate(-1)
+                    return False
+
+                with self.buffer_cv:
+                    while not is_buffer_available(tokens_roi_recver):
+                        logger.debug(
+                            "KV transfer buffer is not available. Waiting...")
+                        self.buffer_cv.wait()
                     # need to clone the tensor
                     # in case the tensor is freed before sending finishes
                     matched_item = self.buffer.popleft()
-                    target_rank = matched_item[0].item()
-                    for tensor in matched_item[1:]:
-                        self._send_tensor_and_dec_size(tensor, rank)
-
-                else:
-                    # no match, just send None
-                    for _ in range(5):
-                        self.data_pipe.send_tensor(None, rank)
+                    for tensor in matched_item:
+                        self._send_tensor_and_dec_size(tensor)
+                    self.buffer_cv.notify()
 
         except RuntimeError as e:
             if 'Connection closed by peer' not in str(e):
@@ -186,10 +183,10 @@ class SimpleBuffer(KVLookupBufferBase):
         logger.debug("Closing drop_select_handler")
 
     def drop_select(
-            self, rank: int, kv_rank: int, input_tokens: Optional[torch.Tensor],
+            self, input_tokens: Optional[torch.Tensor],
             roi: Optional[torch.Tensor]) -> List[Optional[torch.Tensor]]:
 
-        assert not self.request_handling_thread, \
+        assert self.request_handling_thread is None, \
             "drop_select should be called by the KV cache consumer "\
             "(e.g. the decode vLLM instance)"
 
@@ -198,51 +195,40 @@ class SimpleBuffer(KVLookupBufferBase):
         if isinstance(roi, torch.Tensor):
             roi = roi.clone().float()
 
-        self.signal_pipe.send_tensor(self.normal_signal, rank)
+        self.signal_pipe.send_tensor(self.normal_signal)
+        self.data_pipe.send_tensor(input_tokens)
+        self.data_pipe.send_tensor(roi)
 
-        self.data_pipe.send_tensor(torch.tensor(kv_rank), rank)
-        self.data_pipe.send_tensor(input_tokens, rank)
-        self.data_pipe.send_tensor(roi, rank)
-
-        input_tokens = self.data_pipe.recv_tensor(rank)
-        roi = self.data_pipe.recv_tensor(rank)
+        input_tokens = self.data_pipe.recv_tensor()
+        roi = self.data_pipe.recv_tensor()
         if roi is not None:
             # convert from float tensor to bool tensor
             # as PyNccl does not support sending bool tensor
             roi = (roi > 0.5)
-        key = self.data_pipe.recv_tensor(rank)
-        value = self.data_pipe.recv_tensor(rank)
-        hidden = self.data_pipe.recv_tensor(rank)
+        key = self.data_pipe.recv_tensor()
+        value = self.data_pipe.recv_tensor()
+        hidden = self.data_pipe.recv_tensor()
 
         return [input_tokens, roi, key, value, hidden]
 
-    def full_handler(self):
-        time.sleep(0.001)
-
-    def insert(self, kv_group_rank: int, target_rank: int, input_tokens: torch.Tensor, roi: torch.Tensor,
+    def insert(self, input_tokens: torch.Tensor, roi: torch.Tensor,
                key: torch.Tensor, value: torch.Tensor,
                hidden: torch.Tensor) -> None:
 
-        if self.buffer_size > self.buffer_size_threshold:
-            # log outside the while loop to avoid this message being logged
-            # repeatedly.
-            logger.debug("KV transfer buffer is full. Handling...")
-        while self.buffer_size > self.buffer_size_threshold:
-            self.full_handler()
-
-        self._add_to_buffer(target_rank, input_tokens, roi, key, value, hidden)
+        self._add_to_buffer(input_tokens, roi, key, value, hidden)
 
         # when calling the insert, the current process is a sender
         # need to launch the request handler and start listening to request.
-        target_rank_global = target_rank + kv_group_rank
         if self.request_handling_thread is None:
-            self.request_handling_thread = ThreadPoolExecutor(max_workers=1)
-        self.request_handling_thread.submit(self.drop_select_handler, target_rank_global)
+            self.request_handling_thread = threading.Thread(
+                target=self.drop_select_handler)
+            self.request_handling_thread.start()
 
     def close(self):
 
-        if hasattr(self, "request_handling_thread") and self.request_handling_thread:
-            self.request_handling_thread.shutdown()
+        if hasattr(self, "request_handling_thread"
+                   ) and self.request_handling_thread is not None:
+            self.request_handling_thread.join()
 
         else:
             # TODO: have a explicit close signal and have a explicit way to
diff --git a/vllm/distributed/kv_transfer/kv_pipe/base.py b/vllm/distributed/kv_transfer/kv_pipe/base.py
index da2829cf..40589fb3 100644
--- a/vllm/distributed/kv_transfer/kv_pipe/base.py
+++ b/vllm/distributed/kv_transfer/kv_pipe/base.py
@@ -23,7 +23,7 @@ class KVPipeBase(ABC):
     """
 
     @abstractmethod
-    def send_tensor(self, tensor: Optional[torch.Tensor], target_rank: int = 0) -> None:
+    def send_tensor(self, tensor: Optional[torch.Tensor]) -> None:
         """Send a tensor, or None, via the pipe.
         
         Need to support sending None -- important for error handling.
@@ -41,7 +41,7 @@ class KVPipeBase(ABC):
         raise NotImplementedError
 
     @abstractmethod
-    def recv_tensor(self, src_rank: int) -> Optional[torch.Tensor]:
+    def recv_tensor(self) -> Optional[torch.Tensor]:
         """Receive a tensor (can be None) from the pipeline.
 
         Returns:
diff --git a/vllm/distributed/kv_transfer/kv_pipe/dynamo_nccl_pipe.py b/vllm/distributed/kv_transfer/kv_pipe/dynamo_nccl_pipe.py
deleted file mode 100644
index 3ee0fa78..00000000
--- a/vllm/distributed/kv_transfer/kv_pipe/dynamo_nccl_pipe.py
+++ /dev/null
@@ -1,124 +0,0 @@
-import logging
-import threading
-import typing
-import zmq
-import socket
-import time
-import torch
-
-from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import PyNcclPipe
-
-
-logger = logging.getLogger(__name__)
-
-
-class DynamoNcclDataPlane:
-    def __init__(
-        self,
-        data_pipe: PyNcclPipe,
-        hostname: str = "",
-        port: int = 0,
-    ) -> None:
-        
-        self.data_pipe = data_pipe
-        if not hostname:
-            hostname = socket.gethostname()
-        if port == 0:
-            raise ValueError("Port cannot be 0")
-        self._hostname = hostname
-        self._port = port
-        self.store = {}
-        self.context = zmq.Context()
-        self.rep_socket = self.context.socket(zmq.REP)
-        logger.info(f"Rank {self.rank} binding to {self._hostname}:{self._port}")
-        self.rep_socket.bind(f"tcp://{self._hostname}:{self._port}")
-        self._listener_thread = threading.Thread(target=self.listen_for_requests, daemon=True)
-        self._listener_thread.start()
-        self.req_sockets = {}
-        logger.info(f"Rank {self.rank} connected to the server")
-
-    @property
-    def rank(self):
-        return self.data_pipe.kv_group_rank
-    
-    def send_tensor(
-        self,
-        tensor: torch.Tensor,
-        tensor_id: str,
-        remote_address: typing.Optional[str] = None,
-    ):
-        logger.debug(f"Rank {self.rank} sending tensor {tensor_id} to {remote_address}")
-        return self._send_tensor(tensor, tensor_id, remote_address)
-
-    def recv_tensor(
-        self,
-        tensor_id: str,
-        remote_address: typing.Optional[str] = None,
-    ) -> torch.Tensor:
-        ret = self._recv_tensor(tensor_id, remote_address)
-        return ret
-
-    def _send_tensor(
-        self,
-        tensor: torch.Tensor,
-        tensor_id: str,
-        remote_address: typing.Optional[str] = None,
-    ):
-        logger.debug(f"Rank {self.rank} storing tensor with id {tensor_id} of shape {tensor.shape} and dtype {tensor.dtype}")
-        if remote_address is None:
-            self.store[tensor_id] = tensor
-        else:
-            # tensor_shape = "_".join(str(dim) for dim in tensor.shape)
-            # tensor_dtype = str(tensor.dtype)
-            if remote_address not in self.req_sockets:
-                self.req_sockets[remote_address] = self.context.socket(zmq.REQ)
-                self.req_sockets[remote_address].connect(f"tcp://{remote_address}")
-
-            req_socket = self.req_sockets[remote_address]
-            # req_socket.connect(f"tcp://{remote_address}")
-            req_socket.send_string(f"PUT {self.rank} {tensor_id}")
-            dst_rank = req_socket.recv_string()
-            logger.debug(f"Rank {self.rank} sending tensor {tensor_id} to rank {dst_rank}")
-            self.data_pipe.send_tensor(tensor, int(dst_rank))
-
-    def _recv_tensor(
-        self,
-        tensor_id: str,
-        remote_address: typing.Optional[str] = None,
-    ) -> torch.Tensor:
-        logger.debug(f"Rank {self.rank} receiving tensor")
-        if remote_address is not None:
-            raise NotImplementedError("Getting tensor from remote rank not implemented")
-        if tensor_id in self.store:
-            logger.debug(f"Popping tensor {tensor_id} from store")
-            future = self.store.pop(tensor_id)
-            tensor = future.result() # TODO ptarasiewicz we should run other request instead of wait
-            logger.debug(f"Rank {self.rank} received tensor")
-            return tensor
-            
-        logger.debug(f"Rank {self.rank} waiting for tensor {tensor_id}")
-        time.sleep(0.001)
-        return self._recv_tensor(tensor_id, remote_address)
-        # raise NotImplementedError("Tensor not found in store")
-
-    def _receive_tensor(
-        self,
-        tensor_id: str,
-        rank: int,
-    ):
-        future = self.data_pipe.recv_tensor(rank)
-        logger.debug(f"Rank {self.rank} storing tensor {tensor_id} in store")
-        self.store[tensor_id] = future
-
-    def listen_for_requests(self):
-        while True:
-            cmd, rank, tensor_id = self.rep_socket.recv_string().split()
-            logger.debug(f"Rank {self.rank} received request for tensor {tensor_id}")
-            self.rep_socket.send_string(f"{self.rank}")
-            if cmd == "GET":
-                raise NotImplementedError("Getting tensor from remote rank not implemented")
-            elif cmd == "PUT":
-                rank = int(rank)
-                # shape = [int(dim) for dim in shape.split("_")]
-                # dtype = getattr(torch, dtype)
-                self._receive_tensor(tensor_id, rank)
diff --git a/vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py b/vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
index 164ee924..e8bf607e 100644
--- a/vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
+++ b/vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
@@ -45,33 +45,35 @@ class PyNcclPipe(KVPipeBase):
     METADATA_DTYPE = torch.int64
 
     def __init__(self,
-                 kv_group_rank: int,
                  local_rank: int,
                  config: KVTransferConfig,
                  device: Optional[str] = None,
                  port_offset: int = 0):
         self.config = config
         self.local_rank = local_rank
-        self.kv_group_rank = kv_group_rank
+        self.kv_rank = self.config.kv_rank
         self.kv_parallel_size = self.config.kv_parallel_size
-        self.kv_world_size = self.config.kv_world_size
         if device is None:
             self.device = self._select_device(self.config.kv_buffer_device)
         else:
             self.device = self._select_device(device)
 
         # build distributed connection and send/recv implementation
-        logger.info("Creating process group for kv transfer with rank %d and world size %d, ip: %s, port: %d", self.kv_group_rank, self.kv_world_size, self.config.kv_ip, self.config.kv_port + port_offset)
+        store_timeout = self.config.get_from_extra_config("store_timeout", 300)
         self.group = StatelessProcessGroup.create(
             host=self.config.kv_ip,
             port=self.config.kv_port + port_offset,
-            rank=self.kv_group_rank,
-            world_size=self.kv_world_size,
+            rank=self.kv_rank,
+            world_size=self.kv_parallel_size,
+            store_timeout=store_timeout,
         )
         # add a barrier to make sure the connection is initiated properly
         self.group.barrier()
         impl = self._get_device_send_recv_impl(self.group)
         self.device_send_func, self.device_recv_func = impl
+        # set target rank
+        self.target_rank_for_send = (self.kv_rank + 1) % self.kv_parallel_size
+        self.target_rank_for_recv = (self.kv_rank - 1) % self.kv_parallel_size
 
         # transportation-related variables
         self.transport_thread: Optional[ThreadPoolExecutor] = None
@@ -145,16 +147,16 @@ class PyNcclPipe(KVPipeBase):
                            dtype=metadata["dtype"],
                            device=self.device)
 
-    def _send_metadata(self, metadata: Metadata, target_rank: int):
+    def _send_metadata(self, metadata: Metadata):
         """
         Send the metadata dictionary to the target rank.
 
         Parameters:
             - metadata: A dictionary with keys "dtype" and "shape".
         """
-        self.group.send_obj(metadata, target_rank)
+        self.group.send_obj(metadata, self.target_rank_for_send)
 
-    def _recv_metadata(self, src_rank: int) -> Metadata:
+    def _recv_metadata(self) -> Metadata:
         """
         Receive the metadata dictionary from the target rank.
 
@@ -162,9 +164,9 @@ class PyNcclPipe(KVPipeBase):
             - metadata: A dictionary with keys "dtype" and "shape" describing
               the tensor.
         """
-        return self.group.recv_obj(src_rank)
+        return self.group.recv_obj(self.target_rank_for_recv)
 
-    def _send_impl(self, tensor: Optional[torch.Tensor], target_rank: int) -> None:
+    def _send_impl(self, tensor: Optional[torch.Tensor]) -> None:
         """
         The actual implementation of sending the tensor and its metadata to the
         target rank.
@@ -174,12 +176,12 @@ class PyNcclPipe(KVPipeBase):
               being sent.
         """
         metadata = self._make_metadata(tensor)
-        self._send_metadata(metadata, target_rank)
+        self._send_metadata(metadata)
         if tensor is not None:
             self.device_send_func(tensor.to(self.device),
-                                  target_rank)
+                                  self.target_rank_for_send)
 
-    def _recv_impl(self, src_rank: int) -> Optional[torch.Tensor]:
+    def _recv_impl(self) -> Optional[torch.Tensor]:
         """
         The actual implementation of receiving a tensor and its metadata from
         the target rank.
@@ -187,22 +189,21 @@ class PyNcclPipe(KVPipeBase):
         Returns:
             - buffer: The received tensor, or None if no tensor is received.
         """
-        metadata = self._recv_metadata(src_rank)
+        metadata = self._recv_metadata()
         if metadata["dtype"] is None:
             return None
         buffer = self._prepare_recv_buffer(metadata)
-        self.device_recv_func(buffer, src_rank)
+        self.device_recv_func(buffer, self.target_rank_for_recv)
 
         return buffer
 
     def send_tensor_wrapper(self, tensor: Optional[torch.Tensor],
-                            tensor_size: int,
-                            target_rank: int) -> None:
+                            tensor_size: int) -> None:
         """
         Wrapper for _send_impl to handle exceptions and update buffer size.
         """
         try:
-            self._send_impl(tensor, target_rank)
+            self._send_impl(tensor)
 
             with self.buffer_size_lock:
                 self.buffer_size -= tensor_size
@@ -221,7 +222,7 @@ class PyNcclPipe(KVPipeBase):
             logger.debug("KV cache transfer pipe is full. Waiting...")
             time.sleep(0.05)
 
-    def send_tensor(self, tensor: Optional[torch.Tensor], target_rank: int) -> None:
+    def send_tensor(self, tensor: Optional[torch.Tensor]) -> None:
         """
         Sends a tensor and its metadata to the destination rank in a
         non-blocking way.
@@ -229,7 +230,6 @@ class PyNcclPipe(KVPipeBase):
         Parameters:
             - tensor: The tensor to send, or None if no tensor is being sent.
         """
-        logger.debug("Rank %d sending tensor of shape %s dtype %s to rank %d", self.kv_group_rank, tensor.shape if tensor is not None else "None", tensor.dtype if tensor is not None else "None", target_rank)
         if self.transport_thread is None:
             self.transport_thread = ThreadPoolExecutor(max_workers=1)
 
@@ -243,39 +243,32 @@ class PyNcclPipe(KVPipeBase):
         with self.buffer_size_lock:
             self.buffer_size += tensor_size
 
-        future = self.transport_thread.submit(self.send_tensor_wrapper, tensor,
-                                     tensor_size,
-                                     target_rank)
-        return future
+        self.transport_thread.submit(self.send_tensor_wrapper, tensor,
+                                     tensor_size)
 
-    def recv_tensor(self, src_rank: int) -> Optional[torch.Tensor]:
+    def recv_tensor(self) -> Optional[torch.Tensor]:
         """
         Receives a tensor and its metadata from the source rank. Blocking call.
 
         Returns:
             - tensor: The received tensor, or None if no tensor is received.
         """
-
-        logger.debug("Rank %d receiving tensor from rank %d", self.kv_group_rank, src_rank)
-
         if self.transport_thread is None:
             self.transport_thread = ThreadPoolExecutor(max_workers=1)
 
-        future = self.transport_thread.submit(self._recv_impl, src_rank)
+        future = self.transport_thread.submit(self._recv_impl)
 
-        return future
-
-        # try:
-        #     tensor = future.result()
-        # except Exception as e:
-        #     logger.error("Encountering exception in KV receiving thread")
-        #     logger.error("%s", e)
-        #     logger.error("My device: %s", self.device)
-        #     import traceback
-        #     traceback.print_exc()
-        #     raise e
+        try:
+            tensor = future.result()
+        except Exception as e:
+            logger.error("Encountering exception in KV receiving thread")
+            logger.error("%s", e)
+            logger.error("My device: %s", self.device)
+            import traceback
+            traceback.print_exc()
+            raise e
 
-        # return tensor
+        return tensor
 
     def close(self):
         """
diff --git a/vllm/distributed/kv_transfer/kv_transfer_agent.py b/vllm/distributed/kv_transfer/kv_transfer_agent.py
index cd90206f..1e80e0bd 100644
--- a/vllm/distributed/kv_transfer/kv_transfer_agent.py
+++ b/vllm/distributed/kv_transfer/kv_transfer_agent.py
@@ -35,7 +35,6 @@ class KVTransferAgent:
         rank: int,
         local_rank: int,
         config: "VllmConfig",
-        world_group,
     ):
 
         self.config = config
@@ -48,7 +47,7 @@ class KVTransferAgent:
             "TransferAgent should only be used when kv_connector is set."
 
         self.connector = KVConnectorFactory.create_connector(
-            rank, local_rank, config, world_group)
+            rank, local_rank, config)
 
     def send_kv_caches_and_hidden_states(
         self,
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index 6219b119..f897f195 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -771,8 +771,8 @@ _KV_TRANSFER: Optional[kv_transfer.KVTransferAgent] = None
 
 
 def get_kv_transfer_group() -> kv_transfer.KVTransferAgent:
-    #assert _KV_TRANSFER is not None, (
-    #    "disaggregated KV cache transfer parallel group is not initialized")
+    assert _KV_TRANSFER is not None, (
+        "disaggregated KV cache transfer parallel group is not initialized")
     return _KV_TRANSFER
 
 
@@ -979,17 +979,15 @@ def ensure_kv_transfer_initialized(vllm_config: "VllmConfig") -> None:
 
     if vllm_config.kv_transfer_config is None:
         return
-    print(f"vllm_config.kv_transfer_config.need_kv_parallel_group is {vllm_config.kv_transfer_config.need_kv_parallel_group}")
+
     if all([
             vllm_config.kv_transfer_config.is_kv_transfer_instance,
             _KV_TRANSFER is None
     ]):
-        print(f"xxxxxxxxxxxxxxxxxxxxx")
         _KV_TRANSFER = kv_transfer.KVTransferAgent(
             rank=get_world_group().rank,
             local_rank=get_world_group().local_rank,
-            config=vllm_config,
-            world_group=get_world_group())
+            config=vllm_config)
 
 
 def ensure_model_parallel_initialized(
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index 81e2c968..ca50f08a 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -2,17 +2,13 @@
 
 import copy
 import time
-import pickle
-import uuid
 from collections import Counter as collectionsCounter
 from collections import deque
-from collections import defaultdict
 from contextlib import contextmanager
 from dataclasses import dataclass
-from concurrent.futures import ThreadPoolExecutor
 from functools import partial
 from typing import (TYPE_CHECKING, Callable, ClassVar, Deque, Dict, Iterable,
-                    List, Mapping, NamedTuple, Optional, Tuple)
+                    List, Mapping, NamedTuple, Optional)
 from typing import Sequence as GenericSequence
 from typing import Set, Type, Union, cast, overload
 
@@ -65,9 +61,6 @@ from vllm.utils import (Counter, Device, deprecate_kwargs,
                         resolve_obj_by_qualname, weak_bind)
 from vllm.version import __version__ as VLLM_VERSION
 from vllm.worker.model_runner_base import InputProcessingError
-from vllm.remote_prefill import RemotePrefillRequest, RemotePrefillParams, MemoryTransferRequest
-from vllm.distributed.device_communicators.nixl import NixlMetadata
-
 
 logger = init_logger(__name__)
 _LOCAL_LOGGING_INTERVAL_SEC = 5
@@ -98,7 +91,7 @@ class OutputData(NamedTuple):
     # outputs from multiple steps.
     is_first_step_output: Optional[bool]
     skip: List[int]
-    remote_prefill_requests: Optional[List[RemotePrefillRequest]]
+
 
 class SchedulerContext:
 
@@ -112,14 +105,11 @@ class SchedulerContext:
 
         self.multi_step_stream_outputs: bool = multi_step_stream_outputs
 
-        self.remote_prefill_requests: List[RemotePrefillRequest] = []
-
     def append_output(self, outputs: List[SamplerOutput],
                       seq_group_metadata_list: List[SequenceGroupMetadata],
                       scheduler_outputs: SchedulerOutputs, is_async: bool,
                       is_last_step: bool,
-                      is_first_step_output: Optional[bool],
-                      remote_prefill_requests: Optional[List[RemotePrefillRequest]] = None):
+                      is_first_step_output: Optional[bool]):
         self.output_queue.append(
             OutputData(outputs=outputs,
                        seq_group_metadata_list=seq_group_metadata_list,
@@ -127,9 +117,7 @@ class SchedulerContext:
                        is_async=is_async,
                        is_last_step=is_last_step,
                        is_first_step_output=is_first_step_output,
-                       skip=[],
-                       remote_prefill_requests=remote_prefill_requests))
-
+                       skip=[]))
 
 
 class LLMEngine:
@@ -372,7 +360,7 @@ class LLMEngine:
             Scheduler = self.vllm_config.scheduler_config.scheduler_cls
         self.scheduler = [
             Scheduler(
-                self.model_config, self.scheduler_config, self.cache_config, self.lora_config,
+                self.scheduler_config, self.cache_config, self.lora_config,
                 self.parallel_config.pipeline_parallel_size,
                 self.async_callbacks[v_id]
                 if self.model_config.use_async_output_proc else None)
@@ -432,39 +420,6 @@ class LLMEngine:
         # Flag to set when an input fails to process and the engine should run
         # the next step without re-scheduling.
         self._skip_scheduling_next_step = False
-        self.engine_id = str(uuid.uuid4())
-        self._nixl_agents_names: Optional[List[str]] = None
-        if self.vllm_config.kv_transfer_config is not None and self.vllm_config.kv_transfer_config.kv_connector == "DynamoNixlConnector":
-            self._nixl_agents_names = self._initialize_nixl()
-
-        self._request_notif_counter = defaultdict(lambda: -self.parallel_config.tensor_parallel_size)
-        self._request_done_counter = defaultdict(lambda: -self.parallel_config.tensor_parallel_size)
-        self._finished_prefills = set()
-        self._finished_transfers = set()
-
-    @property
-    def is_nixl_initialized(self) -> bool:
-        return getattr(self, "_nixl_agents_names", None) is not None
-
-    def get_nixl_metadata(self) -> NixlMetadata:
-        if not self.is_nixl_initialized:
-            raise RuntimeError("Nixl is not initialized")
-        agent_metadata = self.model_executor.collective_rpc("get_nixl_agent_metadata")
-        kv_caches_base_addr = self.model_executor.collective_rpc("get_nixl_kv_caches_base_addr")
-        return NixlMetadata(engine_id=self.engine_id, agent_metadata=agent_metadata, kv_caches_base_addr=kv_caches_base_addr, num_blocks=self.cache_config.num_gpu_blocks)
-    
-    def add_remote_nixl_metadata(self, nixl_metadata: NixlMetadata) -> List[str]:
-        if not self.is_nixl_initialized:
-            raise RuntimeError("Nixl is not initialized")
-        engine_id = nixl_metadata.engine_id
-        agents_metadata = nixl_metadata.agent_metadata
-        kv_caches_base_addr = nixl_metadata.kv_caches_base_addr
-        num_blocks = nixl_metadata.num_blocks
-        return self.model_executor.collective_rpc("add_remote_nixl_metadata", args=(engine_id, agents_metadata, kv_caches_base_addr, num_blocks))
-
-    def _initialize_nixl(self) -> List[bytes]:
-        agents_names = self.model_executor.collective_rpc("initialize_nixl", args=(self.engine_id,))
-        return agents_names
 
     def _initialize_kv_caches(self) -> None:
         """Initialize the KV cache in the worker(s).
@@ -578,8 +533,6 @@ class LLMEngine:
         # Shutdown model executor when engine is garbage collected
         # Use getattr since __init__ can fail before the field is set
         if model_executor := getattr(self, "model_executor", None):
-            if self.is_nixl_initialized:
-                model_executor.collective_rpc("shutdown_nixl")
             model_executor.shutdown()
 
     def get_tokenizer_group(
@@ -632,14 +585,11 @@ class LLMEngine:
         prompt_adapter_request: Optional[PromptAdapterRequest],
         trace_headers: Optional[Mapping[str, str]] = None,
         priority: int = 0,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
     ) -> Optional[SequenceGroup]:
         """Add a processed request to the engine's request pool.
         return the created sequence group.
         """
         if isinstance(params, SamplingParams) and params.n > 1:
-            if remote_prefill_params is not None:
-                raise ValueError("Remote prefill params are not supported for multi-step sampling")
             ParallelSampleSequenceGroup.add_request(
                 request_id,
                 self,
@@ -657,8 +607,6 @@ class LLMEngine:
         # Create the sequences.
         block_size = self.cache_config.block_size
         seq_id = next(self.seq_counter)
-        if remote_prefill_params is not None and remote_prefill_params.is_remote_decode:
-            next(self.seq_counter) # empty sequence for staging
         eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)
 
         if is_encoder_decoder_inputs(processed_inputs):
@@ -669,7 +617,7 @@ class LLMEngine:
             encoder_inputs = None
 
         seq = Sequence(seq_id, decoder_inputs, block_size, eos_token_id,
-                       lora_request, prompt_adapter_request, remote_prefill_params)
+                       lora_request, prompt_adapter_request)
 
         encoder_seq = (None if encoder_inputs is None else Sequence(
             seq_id, encoder_inputs, block_size, eos_token_id, lora_request,
@@ -686,12 +634,8 @@ class LLMEngine:
                 trace_headers=trace_headers,
                 prompt_adapter_request=prompt_adapter_request,
                 encoder_seq=encoder_seq,
-                priority=priority,
-                remote_prefill_params=remote_prefill_params,
-            )
+                priority=priority)
         elif isinstance(params, PoolingParams):
-            if remote_prefill_params is not None:
-                raise ValueError("Remote prefill params are not supported for pooling")
             seq_group = self._create_sequence_group_with_pooling(
                 request_id,
                 seq,
@@ -762,7 +706,6 @@ class LLMEngine:
             trace_headers: Optional[Mapping[str, str]] = None,
             prompt_adapter_request: Optional[PromptAdapterRequest] = None,
             priority: int = 0,
-            remote_prefill_params: Optional[RemotePrefillParams] = None,
             *,
             inputs: Optional[PromptType] = None,  # DEPRECATED
     ) -> None:
@@ -855,7 +798,6 @@ class LLMEngine:
             prompt_adapter_request=prompt_adapter_request,
             trace_headers=trace_headers,
             priority=priority,
-            remote_prefill_params=remote_prefill_params,
         )
 
     def _validate_token_prompt(self, prompt: PromptType,
@@ -890,7 +832,6 @@ class LLMEngine:
         prompt_adapter_request: Optional[PromptAdapterRequest] = None,
         encoder_seq: Optional[Sequence] = None,
         priority: int = 0,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
     ) -> SequenceGroup:
         """Creates a SequenceGroup with SamplingParams."""
         max_logprobs = self.get_model_config().max_logprobs
@@ -926,9 +867,7 @@ class LLMEngine:
             prompt_adapter_request=prompt_adapter_request,
             encoder_seq=encoder_seq,
             priority=priority,
-            draft_size=draft_size,
-            remote_prefill_params=remote_prefill_params,
-        )
+            draft_size=draft_size)
 
         return seq_group
 
@@ -1095,11 +1034,11 @@ class LLMEngine:
             # When we process only one request, no pop is required
             # (since later we will process all of the rest)
             (outputs, seq_group_metadata_list, scheduler_outputs, is_async,
-             is_last_step, is_first_step_output, skip, remote_prefill_requests) = ctx.output_queue[0]
+             is_last_step, is_first_step_output, skip) = ctx.output_queue[0]
         else:
             (outputs, seq_group_metadata_list, scheduler_outputs, is_async,
              is_last_step, is_first_step_output,
-             skip, remote_prefill_requests) = ctx.output_queue.popleft()
+             skip) = ctx.output_queue.popleft()
 
         # Sanity check
         assert len(seq_group_metadata_list) == len(
@@ -1425,12 +1364,6 @@ class LLMEngine:
 
         # Clear outputs for each new scheduler iteration
         ctx.request_outputs.clear()
-        ctx.remote_prefill_requests.clear()
-
-        remote_prefill_seq_group_metadata_list: List[SequenceGroupMetadata] = []
-        running_seq_group_metadata_list: List[SequenceGroupMetadata] = []
-        remote_prefill_scheduled_seq_groups: List[ScheduledSequenceGroup] = []
-        running_scheduled_seq_groups: List[ScheduledSequenceGroup] = []
 
         # Skip the scheduler if there are any remaining steps in the seq groups.
         # This ensures that the scheduler is only called again when the current
@@ -1443,35 +1376,7 @@ class LLMEngine:
             # Schedule iteration
             (seq_group_metadata_list, scheduler_outputs,
              allow_async_output_proc
-             ) = self.scheduler[virtual_engine].schedule(self._finished_prefills, self._finished_transfers)
-            
-
-            # Separate remote prefill and running seq groups
-            for seq_group_metadata, scheduled_seq_group in zip(seq_group_metadata_list, scheduler_outputs.scheduled_seq_groups):
-                if seq_group_metadata.do_remote_prefill:
-                    remote_prefill_seq_group_metadata_list.append(seq_group_metadata)
-                    remote_prefill_scheduled_seq_groups.append(scheduled_seq_group)
-                else:
-                    running_seq_group_metadata_list.append(seq_group_metadata)
-                    running_scheduled_seq_groups.append(scheduled_seq_group)
-
-            seq_group_metadata_list = running_seq_group_metadata_list
-            scheduler_outputs.scheduled_seq_groups = running_scheduled_seq_groups
-            
-            # Send remote prefill requests before model execution
-            for seq_group_metadata, scheduled_seq_group in zip(remote_prefill_seq_group_metadata_list, remote_prefill_scheduled_seq_groups):
-                assert len(scheduled_seq_group.seq_group.seqs) == 1
-                assert self._nixl_agents_names
-                seq_id = scheduled_seq_group.seq_group.seqs[0].seq_id
-                block_table = seq_group_metadata.block_tables[seq_id]
-                remote_prefill_request = RemotePrefillRequest(
-                    request_id=seq_group_metadata.request_id,
-                    prompt_token_ids=scheduled_seq_group.seq_group.seqs[0].inputs.prompt_token_ids[:-1], # last one will be decoded on decode for sampling anyway
-                    sampling_params=scheduled_seq_group.seq_group.sampling_params,
-                    block_ids=block_table,
-                    engine_id=self.engine_id,
-                )
-                scheduled_seq_group.seq_group.remote_prefill_params.remote_prefill_request_callback(remote_prefill_request)
+             ) = self.scheduler[virtual_engine].schedule()
 
             ctx.seq_group_metadata_list = seq_group_metadata_list
             ctx.scheduler_outputs = scheduler_outputs
@@ -1526,31 +1431,8 @@ class LLMEngine:
                 execute_model_req.async_callback = self.async_callbacks[
                     virtual_engine]
 
-            # After model execution, we need to transfer the memory from the prefill to the decode
-            memory_transfer_reqs = []
-            for scheduled_seq_group, seq_group_metadata in zip(scheduler_outputs.scheduled_seq_groups, seq_group_metadata_list):
-                remote_prefill_params = scheduled_seq_group.seq_group.remote_prefill_params
-                if remote_prefill_params is not None and remote_prefill_params.is_remote_decode:
-                    assert len(scheduled_seq_group.seq_group.seqs) == 1
-                    req_id = scheduled_seq_group.seq_group.request_id
-                    seq_id = scheduled_seq_group.seq_group.seqs[0].seq_id
-                    block_table = seq_group_metadata.block_tables[seq_id]
-                    staging_block_ids = seq_group_metadata.block_tables[seq_id + 1]
-                    memory_transfer_req = MemoryTransferRequest(
-                        request_id=req_id,
-                        src_block_ids=block_table,
-                        staging_block_ids=staging_block_ids,
-                        dst_block_ids=remote_prefill_params.decode_block_ids,
-                        dst_engine_id=remote_prefill_params.decode_engine_id,
-                        notify_msg=req_id,
-                    )
-
-                    memory_transfer_reqs.append(memory_transfer_req)
-
-            execute_model_req.memory_transfer_requests = memory_transfer_reqs
-            
             try:
-                outputs, request_notif_counter, request_done_counter = self.model_executor.execute_model(
+                outputs = self.model_executor.execute_model(
                     execute_model_req=execute_model_req)
                 self._skip_scheduling_next_step = False
             except InputProcessingError as e:
@@ -1577,26 +1459,7 @@ class LLMEngine:
             if len(ctx.output_queue) > 0:
                 self._process_model_outputs(ctx=ctx)
             # No outputs in this case
-            execute_model_req = ExecuteModelRequest(
-                seq_group_metadata_list=[],
-                blocks_to_swap_in=[],
-                blocks_to_swap_out=[],
-                blocks_to_copy=[])
-
-            outputs, request_notif_counter, request_done_counter = self.model_executor.execute_model(
-                execute_model_req=execute_model_req)
-            
-        for req_id, notif_count in request_notif_counter.items():
-            self._request_notif_counter[req_id] += notif_count
-            if self._request_notif_counter[req_id] > -1:
-                self._finished_prefills.add(req_id)
-                del self._request_notif_counter[req_id]
-
-        for req_id, done_count in request_done_counter.items():
-            self._request_done_counter[req_id] += done_count
-            if self._request_done_counter[req_id] > -1:
-                self._finished_transfers.add(req_id)
-                del self._request_done_counter[req_id]
+            outputs = []
 
         # Finish the current step for all the sequence groups.
         if self.scheduler_config.is_multi_step:
@@ -1656,7 +1519,7 @@ class LLMEngine:
             # queued control plane messages, such as add/remove lora adapters.
             logger.debug("Stopping remote worker execution loop.")
             self.model_executor.stop_remote_worker_execution_loop()
-            
+
         return ctx.request_outputs
 
     def _abort_and_cache_schedule(
diff --git a/vllm/engine/multiprocessing/__init__.py b/vllm/engine/multiprocessing/__init__.py
index b3e9ba73..144dd822 100644
--- a/vllm/engine/multiprocessing/__init__.py
+++ b/vllm/engine/multiprocessing/__init__.py
@@ -14,17 +14,13 @@ from vllm.outputs import RequestOutput
 from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
 from vllm.utils import deprecate_kwargs
-from vllm.remote_prefill import RemotePrefillParams
-from vllm.distributed.device_communicators.nixl import NixlMetadata
+
 VLLM_RPC_SUCCESS_STR = "SUCCESS"
 
 IPC_INPUT_EXT = "_input_socket"
 IPC_OUTPUT_EXT = "_output_socket"
 IPC_HEALTH_EXT = "_health_socket"
 IPC_DATA_EXT = "_data_socket"
-IPC_REMOTE_PREFILL_REQUEST_EXT = "_remote_prefill_request_socket"
-IPC_REMOTE_NIXL_METADATA_EXT = "_remote_nixl_metadata_socket"
-IPC_METRICS_EXT = "_metrics_socket"
 
 
 class MQEngineDeadError(RuntimeError):
@@ -40,7 +36,6 @@ class RPCProcessRequest:
     trace_headers: Optional[Mapping[str, str]] = None
     prompt_adapter_request: Optional[PromptAdapterRequest] = None
     priority: int = 0
-    remote_prefill_params: Optional[RemotePrefillParams] = None
 
     @overload
     def __init__(
@@ -83,7 +78,6 @@ class RPCProcessRequest:
             trace_headers: Optional[Mapping[str, str]] = None,
             prompt_adapter_request: Optional[PromptAdapterRequest] = None,
             priority: int = 0,
-            remote_prefill_params: Optional[RemotePrefillParams] = None,
             *,
             inputs: Optional[PromptType] = None,  # DEPRECATED
     ) -> None:
@@ -101,7 +95,7 @@ class RPCProcessRequest:
         self.trace_headers = trace_headers
         self.prompt_adapter_request = prompt_adapter_request
         self.priority = priority
-        self.remote_prefill_params = remote_prefill_params
+
 
 @dataclass
 class RPCError:
@@ -122,7 +116,7 @@ class RPCStartupRequest(Enum):
 @dataclass
 class RPCStartupResponse:
     tracing_enabled: bool
-    nixl_metadata: Optional[bytes] = None
+
 
 class RPCUProfileRequest(Enum):
     START_PROFILE = 1
@@ -185,13 +179,3 @@ def ENGINE_DEAD_ERROR(
     return MQEngineDeadError(
         "Engine loop is not running. Inspect the stacktrace to "
         f"find the original error: {repr(error)}.")
-
-@dataclass
-class KvMetrics:
-    request_active_slots: int
-    request_total_slots: int
-    kv_active_blocks: int
-    kv_total_blocks: int
-    num_requests_waiting: int
-    gpu_cache_usage_perc: float
-    gpu_prefix_cache_hit_rate: float
diff --git a/vllm/engine/multiprocessing/client.py b/vllm/engine/multiprocessing/client.py
index 96af86b7..e2ae9486 100644
--- a/vllm/engine/multiprocessing/client.py
+++ b/vllm/engine/multiprocessing/client.py
@@ -8,7 +8,6 @@ from typing import (Any, AsyncGenerator, Dict, Iterator, List, Mapping,
                     Optional, Union, cast, overload)
 
 import cloudpickle
-import msgspec
 import psutil
 import zmq
 import zmq.asyncio
@@ -19,28 +18,23 @@ from zmq.asyncio import Socket
 from vllm import PoolingParams
 from vllm.config import DecodingConfig, ModelConfig, VllmConfig
 from vllm.core.scheduler import SchedulerOutputs
-from vllm.engine.arg_utils import AsyncEngineArgs
-from vllm.engine.metrics import Stats
 # yapf conflicts with isort for this block
 # yapf: disable
 from vllm.engine.async_llm_engine import (
     build_guided_decoding_logits_processor_async)
 from vllm.engine.multiprocessing import (ENGINE_DEAD_ERROR, IPC_DATA_EXT,
                                          IPC_HEALTH_EXT, IPC_INPUT_EXT,
-                                         IPC_OUTPUT_EXT, IPC_REMOTE_PREFILL_REQUEST_EXT,
-                                         RPC_REQUEST_T,
-                                         VLLM_RPC_SUCCESS_STR, IPC_REMOTE_NIXL_METADATA_EXT, RPCAbortRequest,
-                                         IPC_METRICS_EXT,
+                                         IPC_OUTPUT_EXT, RPC_REQUEST_T,
+                                         VLLM_RPC_SUCCESS_STR, RPCAbortRequest,
                                          RPCAdapterLoadedResponse, RPCError,
                                          RPCIsSleepingRequest,
                                          RPCIsSleepingResponse,
                                          RPCLoadAdapterRequest,
                                          RPCProcessRequest,
                                          RPCResetPrefixCacheRequest,
-                                         RPCSleepRequest,
-                                         RPCUProfileRequest, RPCWakeUpRequest,
-                                         RPCStartupRequest, RPCStartupResponse,
-                                         KvMetrics)
+                                         RPCSleepRequest, RPCStartupRequest,
+                                         RPCStartupResponse,
+                                         RPCUProfileRequest, RPCWakeUpRequest)
 from vllm.engine.protocol import EngineClient
 # yapf: enable
 from vllm.envs import VLLM_RPC_TIMEOUT
@@ -54,8 +48,6 @@ from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
 from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs
 from vllm.utils import deprecate_kwargs
-from vllm.remote_prefill import RemotePrefillParams, RemotePrefillRequest, RemotePrefillRequestCallback
-from vllm.distributed.device_communicators.nixl import NixlMetadata
 
 logger = init_logger(__name__)
 
@@ -101,7 +93,6 @@ class MQLLMEngineClient(EngineClient):
         self._errored_with: Optional[BaseException] = None
 
         # Get the configs.
-        self.vllm_config = engine_config
         self.model_config = engine_config.model_config
         self.decoding_config = engine_config.decoding_config
 
@@ -126,10 +117,6 @@ class MQLLMEngineClient(EngineClient):
         self.heartbeat_socket: Socket = self.context.socket(zmq.constants.PULL)
         self.heartbeat_socket.connect(f"{ipc_path}{IPC_HEALTH_EXT}")
 
-        # Metrics.
-        self.metrics_socket: Socket = self.context.socket(zmq.constants.PULL)
-        self.metrics_socket.connect(f"{ipc_path}{IPC_METRICS_EXT}")
-
         # IPC path for the data socket.
         self.data_ipc_path = f"{ipc_path}{IPC_DATA_EXT}"
 
@@ -144,27 +131,8 @@ class MQLLMEngineClient(EngineClient):
         # Loop to check health of the LLMEngine periodically.
         # Started after the MQLLMEngine is ready.
         self.health_loop: Optional[asyncio.Task] = None
-
-        # Loop to check metrics of the LLMEngine periodically.
-        # Started after the MQLLMEngine is ready.
-        self.metrics_loop: Optional[asyncio.Task] = None
-        self.metrics_publisher = None
-
         self._engine_process = psutil.Process(engine_pid)
 
-        self.nixl_metadata: Optional[NixlMetadata] = None
-        self.remote_prefill_request_socket: Socket = self.context.socket(zmq.constants.PULL)
-        self.remote_nixl_metadata_socket: Socket = self.context.socket(zmq.constants.PUSH)
-        self.remote_prefill_requests_callback: Dict[str, RemotePrefillRequestCallback] = {}
-        if self.using_nixl_connector:
-            self.remote_prefill_request_socket.connect(f"{ipc_path}{IPC_REMOTE_PREFILL_REQUEST_EXT}")
-            self.remote_nixl_metadata_socket.connect(f"{ipc_path}{IPC_REMOTE_NIXL_METADATA_EXT}")
-
-    
-    @property
-    def using_nixl_connector(self) -> bool:
-        return self.vllm_config.kv_transfer_config is not None and self.vllm_config.kv_transfer_config.kv_connector == "DynamoNixlConnector"
-
     @staticmethod
     def is_unsupported_config(vllm_config: VllmConfig):
         # Pipeline parallel not yet supported
@@ -214,61 +182,6 @@ class MQLLMEngineClient(EngineClient):
         except Exception as e:
             self._set_errored(e)
 
-    async def run_remote_prefill_request_handler_loop(self):
-        try:
-            while True:
-                if await self.remote_prefill_request_socket.poll(timeout=VLLM_RPC_TIMEOUT):
-                    frames = await self.remote_prefill_request_socket.recv(copy=False)
-                    remote_prefill_request = msgspec.msgpack.decode(frames.buffer, type=RemotePrefillRequest)
-                    await self.remote_prefill_requests_callback[remote_prefill_request.request_id](remote_prefill_request)
-        except asyncio.CancelledError:
-            logger.debug("Shutting down MQLLMEngineClient remote prefill request handler loop.")
-            
-    async def run_metrics_loop(self, timeout: int):
-        """Background loop that continually checks to ensure the engine process
-        is still alive.
-        """
-        try:
-            while True:
-                # Check if the engine process is running:
-                if not self._engine_process.is_running() or (
-                        self._engine_process.status() == psutil.STATUS_ZOMBIE):
-                    # NB: is_running() returns True for zombies
-                    self._set_errored(
-                        RuntimeError(
-                            f"Engine process (pid {self._engine_process.pid}) "
-                            "died."))
-                    break
-
-                if await self.metrics_socket.poll(timeout=timeout):
-                    # Metrics received- check the message
-                    message: Frame = await self.metrics_socket.recv(copy=False)
-                    metrics = pickle.loads(message.buffer)
-                    if self.metrics_publisher is not None and isinstance(
-                        metrics, KvMetrics
-                    ):
-                        self.metrics_publisher.publish(metrics.request_active_slots,
-                                                    metrics.request_total_slots,
-                                                    metrics.kv_active_blocks,
-                                                    metrics.kv_total_blocks,
-                                                    metrics.num_requests_waiting, 
-                                                    metrics.gpu_cache_usage_perc, 
-                                                    metrics.gpu_prefix_cache_hit_rate)
-                        logger.debug("Metrics successful.")
-
-                    # TODO: Investigate sending whole stats object
-
-        except asyncio.CancelledError:
-            logger.debug("Shutting down MQLLMEngineClient check metrics loop.")
-
-        except psutil.NoSuchProcess:
-            self._set_errored(
-                RuntimeError(
-                    f"Engine process (pid {self._engine_process.pid}) died."))
-
-        except Exception as e:
-            self._set_errored(e)
-
     async def run_output_handler_loop(self):
         """Get RequestOutputs from Engine and stream to Request Queues"""
 
@@ -370,26 +283,12 @@ class MQLLMEngineClient(EngineClient):
             # Wait until server is ready.
             response = await self._wait_for_server_rpc(socket)
 
-            if response.nixl_metadata is not None:
-                assert self.using_nixl_connector
-                self.nixl_metadata = msgspec.msgpack.decode(response.nixl_metadata, type=NixlMetadata)
-
             self.tracing_flag = response.tracing_enabled
 
             # Start health_loop.
             if self.health_loop is None:
                 self.health_loop = asyncio.create_task(
                     self.run_heartbeat_loop(timeout=VLLM_RPC_TIMEOUT))
-                
-            if self.using_nixl_connector:
-                self.remote_prefill_loop = asyncio.create_task(
-                    self.run_remote_prefill_request_handler_loop())
-                    
-            # Start metrics_loop.
-            if self.metrics_loop is None:
-                self.metrics_loop = asyncio.create_task(
-                    self.run_metrics_loop(timeout=VLLM_RPC_TIMEOUT))
-
 
     def close(self):
         """Destroy the ZeroMQ Context."""
@@ -399,8 +298,6 @@ class MQLLMEngineClient(EngineClient):
         # Cancel background tasks.
         if self.health_loop is not None:
             self.health_loop.cancel()
-        if self.metrics_loop is not None:
-            self.metrics_loop.cancel()
         if self.output_loop is not None:
             self.output_loop.cancel()
 
@@ -523,9 +420,6 @@ class MQLLMEngineClient(EngineClient):
         """
         if self._errored_with is not None:
             raise self._errored_with
-        
-    async def add_remote_nixl_metadata(self, nixl_metadata: NixlMetadata):
-        await self.remote_nixl_metadata_socket.send(msgspec.msgpack.encode(nixl_metadata), copy=False)
 
     @property
     def is_running(self) -> bool:
@@ -584,7 +478,6 @@ class MQLLMEngineClient(EngineClient):
         trace_headers: Optional[Mapping[str, str]] = None,
         prompt_adapter_request: Optional[PromptAdapterRequest] = None,
         priority: int = 0,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
         *,
         inputs: Optional[PromptType] = None  # DEPRECATED
     ) -> AsyncGenerator[RequestOutput, None]:
@@ -614,8 +507,7 @@ class MQLLMEngineClient(EngineClient):
 
         return self._process_request(prompt, sampling_params, request_id,
                                      lora_request, trace_headers,
-                                     prompt_adapter_request, priority,
-                                     remote_prefill_params)
+                                     prompt_adapter_request, priority)
 
     @overload
     def encode(
@@ -699,7 +591,6 @@ class MQLLMEngineClient(EngineClient):
         trace_headers: Optional[Mapping[str, str]] = None,
         prompt_adapter_request: Optional[PromptAdapterRequest] = None,
         priority: int = 0,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
     ) -> Union[AsyncGenerator[RequestOutput, None], AsyncGenerator[
             PoolingRequestOutput, None]]:
         """Send an RPCGenerateRequest to the RPCServer and stream responses."""
@@ -745,12 +636,6 @@ class MQLLMEngineClient(EngineClient):
             else:
                 lp_bytes = None
 
-            if remote_prefill_params is not None:
-                self.remote_prefill_requests_callback[request_id] = remote_prefill_params.remote_prefill_request_callback
-                remote_prefill_params.remote_prefill_request_callback = None
-            else:
-                remote_prefill_request_callback = None
-
             request_bytes = pickle.dumps(
                 RPCProcessRequest(
                     prompt=prompt,
@@ -760,11 +645,11 @@ class MQLLMEngineClient(EngineClient):
                     trace_headers=trace_headers,
                     prompt_adapter_request=prompt_adapter_request,
                     priority=priority,
-                    remote_prefill_params=remote_prefill_params,
                 ))
 
             # 3) Send the RPCGenerateRequest to the MQLLMEngine.
-            parts = (request_bytes, lp_bytes) if lp_bytes else (request_bytes,)
+            parts = (request_bytes,
+                     lp_bytes) if lp_bytes else (request_bytes, )
             await self.input_socket.send_multipart(parts, copy=False)
 
             # 4) Stream the RequestOutputs from the output queue. Note
@@ -854,6 +739,3 @@ class MQLLMEngineClient(EngineClient):
         # Raise on error, otherwise happily return None
         if isinstance(request_output, BaseException):
             raise request_output
-
-    def set_metrics_publisher(self, metrics_publisher):
-        self.metrics_publisher = metrics_publisher
diff --git a/vllm/engine/multiprocessing/engine.py b/vllm/engine/multiprocessing/engine.py
index e02dc265..739cbedc 100644
--- a/vllm/engine/multiprocessing/engine.py
+++ b/vllm/engine/multiprocessing/engine.py
@@ -3,12 +3,11 @@
 import pickle
 import signal
 from contextlib import contextmanager
-from typing import Iterator, List, Optional, Union, Dict
+from typing import Iterator, List, Optional, Union
 
 import cloudpickle
-import time
 import zmq
-import msgspec
+
 from vllm import AsyncEngineArgs, SamplingParams
 from vllm.config import VllmConfig
 from vllm.engine.llm_engine import LLMEngine
@@ -16,10 +15,8 @@ from vllm.engine.llm_engine import LLMEngine
 # yapf: disable
 from vllm.engine.multiprocessing import (ENGINE_DEAD_ERROR, IPC_DATA_EXT,
                                          IPC_HEALTH_EXT, IPC_INPUT_EXT,
-                                         REQUEST_OUTPUTS_T,
-                                         VLLM_RPC_SUCCESS_STR, IPC_REMOTE_PREFILL_REQUEST_EXT,
-                                         RPCAbortRequest,
-                                         IPC_OUTPUT_EXT, IPC_METRICS_EXT,
+                                         IPC_OUTPUT_EXT, REQUEST_OUTPUTS_T,
+                                         VLLM_RPC_SUCCESS_STR, RPCAbortRequest,
                                          RPCAdapterLoadedResponse, RPCError,
                                          RPCIsSleepingRequest,
                                          RPCIsSleepingResponse,
@@ -27,10 +24,8 @@ from vllm.engine.multiprocessing import (ENGINE_DEAD_ERROR, IPC_DATA_EXT,
                                          RPCProcessRequest,
                                          RPCResetPrefixCacheRequest,
                                          RPCSleepRequest, RPCStartupRequest,
-                                         RPCUProfileRequest, RPCWakeUpRequest,
-                                         RPCStartupRequest, RPCStartupResponse,
-                                         RPCUProfileRequest, IPC_REMOTE_NIXL_METADATA_EXT,
-                                         KvMetrics)
+                                         RPCStartupResponse,
+                                         RPCUProfileRequest, RPCWakeUpRequest)
 # yapf: enable
 from vllm.logger import init_logger
 from vllm.outputs import RequestOutput
@@ -38,88 +33,12 @@ from vllm.transformers_utils.config import (
     maybe_register_config_serialize_by_value)
 from vllm.usage.usage_lib import UsageContext
 from vllm.worker.model_runner_base import InputProcessingError
-from vllm.remote_prefill import RemotePrefillRequest
-from vllm.distributed.device_communicators.nixl import NixlMetadata
-
-from vllm.engine.metrics_types import StatLoggerBase, Stats, SupportsMetricsInfo
-from dataclasses import dataclass, field
 
 logger = init_logger(__name__)
 
 POLLING_TIMEOUT_MS = 10000
 HEALTHY_RESPONSE = (pickle.dumps(VLLM_RPC_SUCCESS_STR), )
 
-class KvStatLogger(StatLoggerBase):
-    def __init__(
-        self,
-        max_num_seqs: int,
-        num_total_gpu_blocks: int,
-        metrics_socket
-    ):
-        # Must query initialized scheduler for max infos
-        self.request_total_slots = max_num_seqs
-        self.kv_total_blocks = num_total_gpu_blocks
-        self.metrics_socket = metrics_socket
-
-        # KV metrics
-        self._send_kv_metrics(0, 0, 0, 0.0, 0.0)
-
-    def log(self, stats: Stats) -> None:
-        self._send_kv_metrics(
-            stats.num_running_sys,
-            int(stats.gpu_cache_usage_sys * self.kv_total_blocks),
-            stats.num_waiting_sys,
-            stats.gpu_cache_usage_sys,
-            stats.gpu_prefix_cache_hit_rate
-        )
-
-    def info(self, type: str, obj: SupportsMetricsInfo) -> None:
-        pass
-
-    def _send_kv_metrics(
-        self,
-        active_slots,
-        active_kv_blocks,
-        num_requests_waiting,
-        gpu_cache_usage_perc,
-        gpu_prefix_cache_hit_rate,
-    ):
-        if not self.metrics_socket.closed:
-            metrics_bytes = pickle.dumps(
-                KvMetrics(
-                    active_slots,
-                    self.request_total_slots,
-                    active_kv_blocks,
-                    self.kv_total_blocks,
-                    num_requests_waiting,
-                    gpu_cache_usage_perc,
-                    gpu_prefix_cache_hit_rate,
-                )
-            )
-            self.metrics_socket.send_multipart((metrics_bytes, ), copy=False)
-
-# TODO: Send entire stats object to the client
-# class StatLogger(StatLoggerBase):
-#     def __init__(
-#         self,
-#         metrics_socket
-#     ):
-#         self.metrics_socket = metrics_socket
-
-#     def log(self, stats: Stats) -> None:
-#         self._send_metrics(stats)
-
-#     def info(self, type: str, obj: SupportsMetricsInfo) -> None:
-#         pass
-
-#     def _send_metrics(self, stats: Stats):
-#         if not self.metrics_socket.closed:
-#             metrics_bytes = pickle.dumps(stats)
-#             self.metrics_socket.send_multipart((metrics_bytes, ), copy=False)
-
-
-
-
 
 class MQLLMEngine:
     """A multiprocessing wrapper for :class:`LLMEngine`.
@@ -182,37 +101,12 @@ class MQLLMEngine:
         self.heartbeat_socket = self.ctx.socket(zmq.constants.PUSH)
         self.heartbeat_socket.bind(f"{ipc_path}{IPC_HEALTH_EXT}")
 
-        # Send metrics back to client.
-        self.metrics_socket = self.ctx.socket(zmq.constants.PUSH)
-        self.metrics_socket.bind(f"{ipc_path}{IPC_METRICS_EXT}")
-
         # IPC path for the data socket.
         self.data_ipc_path = f"{ipc_path}{IPC_DATA_EXT}"
 
         # Error state.
         self._errored_with: Optional[BaseException] = None
 
-        self.remote_prefill_request_socket = self.ctx.socket(zmq.constants.PUSH)
-        self.remote_nixl_metadata_socket = self.ctx.socket(zmq.constants.PULL)
-        if self.engine.is_nixl_initialized:
-            self.remote_prefill_request_socket.bind(f"{ipc_path}{IPC_REMOTE_PREFILL_REQUEST_EXT}")
-            self.remote_nixl_metadata_socket.bind(f"{ipc_path}{IPC_REMOTE_NIXL_METADATA_EXT}")
-
-
-        # Attach logger for continuous metrics publishing
-        self.kv_stat_logger = KvStatLogger(
-            self.engine.scheduler_config.max_num_seqs,
-            self.engine.cache_config.num_gpu_blocks,
-            self.metrics_socket
-        )
-        self.engine.add_logger("kv_metrics", self.kv_stat_logger)
-        
-        # TODO investigate sending whole stats object
-        # self.general_stat_logger = StatLogger(
-        #     self.metrics_socket
-        # )
-        # self.engine.add_logger("general_metrics", self.general_stat_logger)
-
     @property
     def dead_error(self) -> BaseException:
         if self._errored_with is not None:
@@ -298,17 +192,8 @@ class MQLLMEngine:
                 # Handle the query from the Client.
                 if request == RPCStartupRequest.IS_SERVER_READY:
                     tracing_enabled = self.engine.is_tracing_enabled()
-            
-                    # Send nixl metadata to the client
-                    if self.engine.is_nixl_initialized:
-                        nixl_metadata = self.engine.get_nixl_metadata()
-                        encoded_nixl_metadata = msgspec.msgpack.encode(nixl_metadata)
-                        response = RPCStartupResponse(
-                            tracing_enabled=tracing_enabled,
-                            nixl_metadata=encoded_nixl_metadata)
-                    else:
-                        response = RPCStartupResponse(
-                            tracing_enabled=tracing_enabled)
+                    response = RPCStartupResponse(
+                        tracing_enabled=tracing_enabled)
 
             except Exception as e:
                 response = e
@@ -321,7 +206,6 @@ class MQLLMEngine:
 
         while True:
             if not self.engine.has_unfinished_requests():
-                logger.debug("No unfinished requests")
                 # Poll until there is work to do.
                 while self.input_socket.poll(timeout=POLLING_TIMEOUT_MS) == 0:
                     # When there's no work, check on engine health and send
@@ -365,13 +249,6 @@ class MQLLMEngine:
     def handle_new_input(self):
         """Handle new input from the socket"""
         try:
-            if self.engine.is_nixl_initialized:
-                while self.remote_nixl_metadata_socket.poll(timeout=0) != 0:
-                    frames = self.remote_nixl_metadata_socket.recv(copy=False)
-                    nixl_metadata = msgspec.msgpack.decode(frames.buffer, type=NixlMetadata)
-                    logger.debug("Adding remote nixl metadata for engine: %s", nixl_metadata.engine_id)
-                    self.engine.add_remote_nixl_metadata(nixl_metadata)
-
             while self.input_socket.poll(timeout=0) != 0:
                 frames = self.input_socket.recv_multipart(copy=False)
                 request = pickle.loads(frames[0].buffer)
@@ -420,11 +297,6 @@ class MQLLMEngine:
             self._send_outputs(rpc_err)
 
         try:
-            if request.remote_prefill_params is not None and request.remote_prefill_params.is_remote_prefill:
-                def remote_prefill_request_callback(request: RemotePrefillRequest):
-                    logger.debug("Sending remote prefill request: %s", request.request_id)
-                    self.remote_prefill_request_socket.send(msgspec.msgpack.encode(request), copy=False)
-                request.remote_prefill_params.remote_prefill_request_callback = remote_prefill_request_callback
             self.engine.add_request(
                 request_id=request_id,
                 prompt=request.prompt,
@@ -432,9 +304,7 @@ class MQLLMEngine:
                 lora_request=request.lora_request,
                 trace_headers=request.trace_headers,
                 prompt_adapter_request=request.prompt_adapter_request,
-                priority=request.priority,
-                remote_prefill_params=request.remote_prefill_params,
-            )
+                priority=request.priority)
 
             if self.log_requests:
                 logger.info("Added request %s.", request.request_id)
diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index 9c25d766..130dfe18 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -38,7 +38,6 @@ from vllm.sequence import Logprob
 from vllm.transformers_utils.tokenizer import AnyTokenizer, MistralTokenizer
 from vllm.transformers_utils.tokenizers import (maybe_serialize_tool_calls,
                                                 truncate_tool_call_ids)
-from vllm.remote_prefill import RemotePrefillParams
 
 logger = init_logger(__name__)
 
@@ -120,7 +119,6 @@ class OpenAIServingChat(OpenAIServing):
         self,
         request: ChatCompletionRequest,
         raw_request: Optional[Request] = None,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
     ) -> Union[AsyncGenerator[str, None], ChatCompletionResponse,
                ErrorResponse]:
         """
@@ -259,7 +257,6 @@ class OpenAIServingChat(OpenAIServing):
                         trace_headers=trace_headers,
                         prompt_adapter_request=prompt_adapter_request,
                         priority=request.priority,
-                        remote_prefill_params=remote_prefill_params,
                     )
 
                 generators.append(generator)
diff --git a/vllm/envs.py b/vllm/envs.py
index ba87d071..b2937462 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -94,10 +94,6 @@ if TYPE_CHECKING:
     VLLM_DP_MASTER_PORT: int = 0
     VLLM_MARLIN_USE_ATOMIC_ADD: bool = False
     VLLM_V0_USE_OUTLINES_CACHE: bool = False
-    VLLM_KV_CAPI_PATH: Optional[str] = None
-    VLLM_KV_NAMESPACE: Optional[str] = None
-    VLLM_KV_COMPONENT: Optional[str] = None
-    VLLM_WORKER_ID: Optional[int] = None
 
 
 def get_default_cache_root():
@@ -622,29 +618,6 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # an environment with potentially malicious users.
     "VLLM_V0_USE_OUTLINES_CACHE":
     lambda: os.environ.get("VLLM_V0_USE_OUTLINES_CACHE", "0") == "1",
-    # When on a Nvidia GPU aligns single entries (within a page) so they are 256
-    # byte aligned for better performance, this increases the memory usage of
-    # the cache. Currently this only affects MLA that results in non-256
-    # byte aligned entries. This matches the alignment the CUDA runtime uses
-    # for all allocations. Currently this primarily affects MLA, for most other
-    # models the alignment is already naturally aligned to 256 bytes.
-    "VLLM_CUDA_MEM_ALIGN_KV_CACHE":
-    lambda: bool(int(os.getenv("VLLM_CUDA_MEM_ALIGN_KV_CACHE", "1"))),
-
-    # Path to the C API Library
-    "VLLM_KV_CAPI_PATH":
-    lambda: os.environ.get("VLLM_KV_CAPI_PATH", None),
-
-    # Identifiers to publish KV related information
-    "VLLM_KV_NAMESPACE":
-    lambda: os.environ.get("VLLM_KV_NAMESPACE", None),
-    "VLLM_KV_COMPONENT":
-    lambda: os.environ.get("VLLM_KV_COMPONENT", None),
-
-    # Worker ID used for identifying workers in distributed settings
-    "VLLM_WORKER_ID":
-    lambda: int(os.getenv("VLLM_WORKER_ID", "0"))
-    if "VLLM_WORKER_ID" in os.environ else None,
 }
 
 # end-env-vars-definition
diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py
index 1f435029..d66f61a8 100644
--- a/vllm/model_executor/models/deepseek_v2.py
+++ b/vllm/model_executor/models/deepseek_v2.py
@@ -590,9 +590,6 @@ class DeepseekV2Model(nn.Module):
         cache_config = vllm_config.cache_config
         quant_config = vllm_config.quant_config
 
-        self.config = config
-
-        self.padding_idx = config.pad_token_id
         self.vocab_size = config.vocab_size
 
         if get_pp_group().is_first_rank:
diff --git a/vllm/outputs.py b/vllm/outputs.py
index d0fd2cd3..7a20c340 100644
--- a/vllm/outputs.py
+++ b/vllm/outputs.py
@@ -6,16 +6,16 @@ from collections.abc import Sequence as GenericSequence
 from dataclasses import dataclass
 from typing import Generic, Optional, Union
 
-import msgspec
 import torch
 from typing_extensions import TypeVar, deprecated
 
 from vllm.lora.request import LoRARequest
 from vllm.multimodal.inputs import MultiModalPlaceholderDict
-from vllm.sampling_params import RequestOutputKind, SamplingParams
+from vllm.sampling_params import RequestOutputKind
 from vllm.sequence import (PromptLogprobs, RequestMetrics, SampleLogprobs,
                            SequenceGroup, SequenceGroupBase, SequenceStatus)
 
+
 @dataclass
 class CompletionOutput:
     """The output data of one completion output of a request.
diff --git a/vllm/remote_prefill.py b/vllm/remote_prefill.py
deleted file mode 100644
index 957f55de..00000000
--- a/vllm/remote_prefill.py
+++ /dev/null
@@ -1,54 +0,0 @@
-from dataclasses import dataclass
-from typing import Callable, Optional, List, Coroutine
-
-import msgspec
-
-from vllm.sampling_params import SamplingParams
-
-
-class RemotePrefillRequest(
-        msgspec.Struct,
-        omit_defaults=True,  # type: ignore[call-arg]
-        # required for @cached_property.
-        dict=True):
-    """The request data of one remote prefill output of a request.
-
-    Args:
-        request_id: The unique ID of the request.
-        prompt: The prompt string of the request.
-    """
-    request_id: str
-    prompt_token_ids: List[int]
-    sampling_params: SamplingParams
-    block_ids: List[int]
-    engine_id: str
-
-
-class MemoryTransferRequest(
-        msgspec.Struct,
-        array_like=True,  # type: ignore[call-arg]
-        omit_defaults=True):  # type: ignore[call-arg]
-    """The request data of one memory transfer output of a request.
-
-    Args:
-        request_id: The unique ID of the request.
-    """
-    request_id: str
-    src_block_ids: List[int]
-    staging_block_ids: List[int]
-    dst_block_ids: List[int]
-    dst_engine_id: str
-    notify_msg: str
-
-
-RemotePrefillRequestCallback = Callable[[RemotePrefillRequest], None]
-
-
-@dataclass
-class RemotePrefillParams:
-    """Remote prefill parameters for text generation."""
-    is_remote_prefill: bool = False
-    is_remote_decode: bool = False
-    decode_block_ids: Optional[List[int]] = None
-    decode_engine_id: Optional[str] = None
-    remote_prefill_request_callback: Optional[RemotePrefillRequestCallback] = None
\ No newline at end of file
diff --git a/vllm/sampling_params.py b/vllm/sampling_params.py
index 6fc71b70..9b474a37 100644
--- a/vllm/sampling_params.py
+++ b/vllm/sampling_params.py
@@ -103,7 +103,7 @@ class RequestOutputKind(Enum):
     DELTA = 1
     # Do not return intermediate RequestOuputs
     FINAL_ONLY = 2
-    
+
 
 class SamplingParams(
         msgspec.Struct,
diff --git a/vllm/sequence.py b/vllm/sequence.py
index 554027c1..61867b02 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -9,7 +9,7 @@ from collections.abc import Mapping
 from collections.abc import Sequence as GenericSequence
 from dataclasses import dataclass, field
 from functools import reduce
-from typing import Any, Callable, Optional, Union, List
+from typing import Any, Callable, Optional, Union
 
 import msgspec
 import torch
@@ -20,7 +20,6 @@ from vllm.multimodal import MultiModalDataDict, MultiModalPlaceholderDict
 from vllm.pooling_params import PoolingParams
 from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import RequestOutputKind, SamplingParams
-from vllm.remote_prefill import RemotePrefillParams, MemoryTransferRequest
 
 VLLM_TOKEN_ID_ARRAY_TYPE = "l"
 
@@ -60,14 +59,13 @@ class SequenceStatus(enum.IntEnum):
     """Status of a sequence."""
     WAITING = 0
     RUNNING = 1
-    REMOTE_PREFILLING = 2
-    SWAPPED = 3
-    # Note: anything after SWAPPED (3) will be considered
+    SWAPPED = 2
+    # Note: anything after SWAPPED (2) will be considered
     # as a finished status.
-    FINISHED_STOPPED = 4
-    FINISHED_LENGTH_CAPPED = 5
-    FINISHED_ABORTED = 6
-    FINISHED_IGNORED = 7
+    FINISHED_STOPPED = 3
+    FINISHED_LENGTH_CAPPED = 4
+    FINISHED_ABORTED = 5
+    FINISHED_IGNORED = 6
 
     @staticmethod
     def is_finished(status: "SequenceStatus") -> bool:
@@ -419,7 +417,6 @@ class Sequence:
         eos_token_id: Optional[int] = None,
         lora_request: Optional[LoRARequest] = None,
         prompt_adapter_request: Optional[PromptAdapterRequest] = None,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
     ) -> None:
         self.seq_id = seq_id
         self.inputs = SingletonInputsAdapter(inputs)
@@ -427,7 +424,7 @@ class Sequence:
         self.eos_token_id = eos_token_id
         self.lora_request = lora_request
         self.prompt_adapter_request = prompt_adapter_request
-        self.remote_prefill_params = remote_prefill_params
+
         self.data = SequenceData.from_seqs(self.prompt_token_ids)
         self.output_logprobs: SampleLogprobs = []
         self.output_text = ""
@@ -650,25 +647,25 @@ class SequenceGroup:
         trace_headers: OpenTelemetry trace headers.
         prompt_adapter_request: Prompt Adapter request.
         priority: User-defined priority of the request.
-        remote_prefill_params: Remote prefill parameters.
+        draft_size: The number of speculative tokens plus one from the target 
+                    model; equal to max number of tokens a step can generate
+                    for single-draft speculative decoding but larger than 
+                    that for multi-draft SD (currently not supported).
     """
 
-    def __init__(
-        self,
-        request_id: str,
-        seqs: List[Sequence],
-        arrival_time: float,
-        sampling_params: Optional[SamplingParams] = None,
-        lora_request: Optional[LoRARequest] = None,
-        pooling_params: Optional[PoolingParams] = None,
-        pooled_data: Optional[torch.Tensor] = None,
-        encoder_seq: Optional[Sequence] = None,
-        trace_headers: Optional[Mapping[str, str]] = None,
-        prompt_adapter_request: Optional[PromptAdapterRequest] = None,
-        priority: int = 0,
-        draft_size: int = 1,
-        remote_prefill_params: Optional[RemotePrefillParams] = None,
-    ) -> None:
+    def __init__(self,
+                 request_id: str,
+                 seqs: list[Sequence],
+                 arrival_time: float,
+                 sampling_params: Optional[SamplingParams] = None,
+                 lora_request: Optional[LoRARequest] = None,
+                 pooling_params: Optional[PoolingParams] = None,
+                 pooled_data: Optional[torch.Tensor] = None,
+                 encoder_seq: Optional[Sequence] = None,
+                 trace_headers: Optional[Mapping[str, str]] = None,
+                 prompt_adapter_request: Optional[PromptAdapterRequest] = None,
+                 priority: int = 0,
+                 draft_size: int = 1) -> None:
         self.request_id = request_id
         self.seqs = seqs
         self.first_seq = seqs[0]
@@ -694,7 +691,7 @@ class SequenceGroup:
         self.encoder_seq = encoder_seq
         self.trace_headers = trace_headers
         self.priority = priority
-        self.remote_prefill_params = remote_prefill_params
+
         self.cached_request_output = None
 
     @property
@@ -943,9 +940,6 @@ class SequenceGroupMetadata(
             query tokens for prefill, we don't need sampling.
         token_chunk_size: The number of tokens to be processed (per sequence).
             None if chunking is not required.
-        do_remote_prefill: True if remote prefill is required.
-        do_remote_decode: True if remote decode is required.
-        decode_memory_desc: The memory descriptor for the decoder blocks.
         lora_request: LoRA request.
         computed_block_nums: The block numbers that are already computed,
             used in prefix caching.
@@ -985,9 +979,6 @@ class SequenceGroupMetadata(
     cross_block_table: Optional[list[int]] = None
     prompt_adapter_request: Optional[PromptAdapterRequest] = None
     token_chunk_size: Optional[int] = None
-    do_remote_prefill: bool = False
-    do_remote_decode: bool = False
-    decode_memory_desc: Optional[bytes] = None
 
     ### Stateful fields that are lazily defined. ###
     # The number of speculative tokens adopted in this request.
@@ -1338,8 +1329,6 @@ class ExecuteModelRequest(
     last_sampled_token_ids: Optional[torch.Tensor] = None
     # Async callback
     async_callback: Optional[Callable] = None
-    # The memory transfer requests.
-    memory_transfer_requests: Optional[List[MemoryTransferRequest]] = None
 
     @property
     def is_first_multi_step(self) -> bool:
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 3cabae5c..473bd901 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -1712,17 +1712,15 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         # NOTE: The receive operation is blocking
         bypass_model_exec = False
         if self.need_recv_kv(model_input, kv_caches):
-            kv_transfer = get_kv_transfer_group()
-            if kv_transfer is not None:
-                hidden_or_intermediate_states, bypass_model_exec, model_input = \
-                    kv_transfer.recv_kv_caches_and_hidden_states(
-                        # model is used to know which layer the current worker
-                        # is working on, so that we can receive KV for only those
-                        # layers.
-                        model_executable,
-                        model_input,
-                        kv_caches=kv_caches
-                    )
+            hidden_or_intermediate_states, bypass_model_exec, model_input = \
+                get_kv_transfer_group().recv_kv_caches_and_hidden_states(
+                    # model is used to know which layer the current worker
+                    # is working on, so that we can receive KV for only those
+                    # layers.
+                    model_executable,
+                    model_input,
+                    kv_caches=kv_caches
+                )
 
         multi_modal_kwargs = model_input.multi_modal_kwargs or {}
         seqlen_agnostic_kwargs = {
@@ -1758,17 +1756,15 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         # Sending KV cache in distributed KV cache transfer setting
         # NOTE: the send operation is non-blocking
         if self.need_send_kv(model_input, kv_caches):
-            kv_transfer = get_kv_transfer_group()
-            if kv_transfer is not None:
-                kv_transfer().send_kv_caches_and_hidden_states(
-                    # model_executable is used to know which layer the current
-                    # worker is working on, so that we can send KV for only those
-                    # layers.
-                    model_executable,
-                    model_input,
-                    kv_caches,
-                    hidden_or_intermediate_states,
-                )
+            get_kv_transfer_group().send_kv_caches_and_hidden_states(
+                # model_executable is used to know which layer the current
+                # worker is working on, so that we can send KV for only those
+                # layers.
+                model_executable,
+                model_input,
+                kv_caches,
+                hidden_or_intermediate_states,
+            )
 
         # Compute the logits in the last pipeline stage.
         if not get_pp_group().is_last_rank:
@@ -1851,9 +1847,6 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
 
         if self.vllm_config.kv_transfer_config is None:
             return False
-        
-        if self.vllm_config.kv_transfer_config.kv_connector == "DynamoNixlConnector":
-            return False
 
         prefill_meta = model_input.attn_metadata.prefill_metadata
 
@@ -1879,9 +1872,6 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
 
         if self.vllm_config.kv_transfer_config is None:
             return False
-        
-        if self.vllm_config.kv_transfer_config.kv_connector == "DynamoNixlConnector":
-            return False
 
         prefill_meta = model_input.attn_metadata.prefill_metadata
 
diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py
index 50b07bdc..ad94a6a4 100644
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -2,7 +2,7 @@
 """A GPU worker class."""
 import gc
 import os
-from typing import Dict, List, Optional, Set, Tuple, Type, Union, TYPE_CHECKING, Any
+from typing import Dict, List, Optional, Set, Tuple, Type, Union
 
 import torch
 import torch.distributed
@@ -31,8 +31,6 @@ from vllm.worker.model_runner import GPUModelRunnerBase, ModelRunner
 from vllm.worker.pooling_model_runner import PoolingModelRunner
 from vllm.worker.worker_base import (LocalOrDistributedWorkerBase, WorkerBase,
                                      WorkerInput)
-from vllm.distributed.device_communicators.nixl import DynamoNixlConnector
-
 
 logger = init_logger(__name__)
 
@@ -309,46 +307,6 @@ class Worker(LocalOrDistributedWorkerBase):
             self._init_cache_engine()
         self._warm_up_model()
 
-    def initialize_nixl(self, engine_id: str) -> List[bytes]:
-
-        # TODO ptarasiewicz nixl can also support DRAM
-        assert self.device_config.device_type == "cuda", "Currently only CUDA is supported for Nixl connector"
-
-        self.nixl_connector = DynamoNixlConnector(self.vllm_config, engine_id, self.local_rank) # TODO ptarasiewicz: rank or local_rank?
-        assert len(self.cache_engine) == 1, "Only one cache engine is supported for now"
-        self.nixl_connector.register_kv_caches(self.cache_engine[0].gpu_cache)
-        return self.nixl_connector.agent_name
-    
-    def get_nixl_agent_metadata(self) -> bytes:
-        assert self.nixl_connector is not None, "Nixl connector is not initialized"
-        return self.nixl_connector.get_agent_metadata()
-
-    def add_remote_nixl_metadata(self, engine_id: str, agents_metadata: List[bytes], kv_caches_base_addr: List[List[Tuple[int, int]]], num_blocks: int) -> str:
-        assert self.nixl_connector is not None, "Nixl connector is not initialized"
-        agent_name = self.nixl_connector.add_remote_agent(engine_id, agents_metadata, len(agents_metadata), kv_caches_base_addr, num_blocks) # TODO ptarasiewicz: rank or local_rank?
-        return agent_name
-    
-    def transfer_nixl_memory(self, src_descs: List[bytes], dst_descs: List[bytes], remote_agent_name: List[str], notify_msg: str) -> None:
-        assert self.nixl_connector is not None, "Nixl connector is not initialized"
-        self.nixl_connector.transfer_mem(src_descs[self.local_rank], dst_descs[self.local_rank], remote_agent_name, notify_msg) # TODO ptarasiewicz: rank or local_rank?
-
-    def get_nixl_kv_caches_base_addr(self) -> List[bytes]:
-        assert self.nixl_connector is not None, "Nixl connector is not initialized"
-        return self.nixl_connector.kv_caches_base_addr[self.nixl_connector.engine_id]
-        
-    def _transfer_blocks(self, worker_input: WorkerInput) -> None:
-                
-        if not self.is_driver_worker:
-            torch.cuda.synchronize() # to make sure that the blocks are ready, on driver worker we transfer after sampling, so there's no need to synchronize
-
-        if worker_input.src_block_ids is not None:
-            for src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg in zip(worker_input.src_block_ids, worker_input.staging_block_ids, worker_input.dst_block_ids, worker_input.dst_engine_id, worker_input.notify_msg):
-                self.nixl_connector.transfer_mem(src_block_ids, staging_block_ids, dst_block_ids, dst_engine_id, notify_msg)
-
-    def shutdown_nixl(self) -> None:
-        assert self.nixl_connector is not None, "Nixl connector is not initialized"
-        self.nixl_connector.shutdown()
-
     def _init_cache_engine(self):
         assert self.cache_config.num_gpu_blocks is not None
         self.cache_engine = [
@@ -410,8 +368,6 @@ class Worker(LocalOrDistributedWorkerBase):
         blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,
                                       device=self.device,
                                       dtype=torch.int64).view(-1, 2)
-        
-        mem_transfer_reqs = execute_model_req.memory_transfer_requests or []
 
         return WorkerInput(
             num_seq_groups=num_seq_groups,
@@ -420,11 +376,6 @@ class Worker(LocalOrDistributedWorkerBase):
             blocks_to_copy=blocks_to_copy,
             virtual_engine=virtual_engine,
             num_steps=num_steps,
-            src_block_ids=[r.src_block_ids for r in mem_transfer_reqs],
-            staging_block_ids=[r.staging_block_ids for r in mem_transfer_reqs],
-            dst_block_ids=[r.dst_block_ids for r in mem_transfer_reqs],
-            dst_engine_id=[r.dst_engine_id for r in mem_transfer_reqs],
-            notify_msg=[r.notify_msg for r in mem_transfer_reqs],
         )
 
     @torch.inference_mode()
diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py
index 3baa79fa..e5662e69 100644
--- a/vllm/worker/worker_base.py
+++ b/vllm/worker/worker_base.py
@@ -9,7 +9,6 @@ from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 import cloudpickle
 import torch
 import torch.nn as nn
-from collections import defaultdict
 
 from vllm.config import (ObservabilityConfig, VllmConfig,
                          set_current_vllm_config)
@@ -25,7 +24,6 @@ from vllm.utils import (enable_trace_function_call_for_thread,
 from vllm.worker.model_runner_base import (BroadcastableModelInput,
                                            ModelRunnerBase,
                                            ModelRunnerInputBase)
-from vllm.distributed.device_communicators.nixl import DynamoNixlConnector
 
 logger = init_logger(__name__)
 
@@ -57,9 +55,6 @@ class WorkerBase:
         from vllm.platforms import current_platform
         self.current_platform = current_platform
 
-        self.nixl_connector: Optional[DynamoNixlConnector] = None
-
-    @abstractmethod
     def init_device(self) -> None:
         """Initialize device state, such as loading the model or other on-device
         memory allocations.
@@ -226,12 +221,6 @@ class WorkerInput:
     virtual_engine: int = 0
     num_steps: int = 1
 
-    src_block_ids: Optional[List[List[int]]] = None
-    staging_block_ids: Optional[List[List[int]]] = None
-    dst_block_ids: Optional[List[List[int]]] = None
-    dst_engine_id: Optional[List[str]] = None
-    notify_msg: Optional[List[str]] = None
-
     @classmethod
     def from_broadcasted_tensor_dict(
         cls: Type["WorkerInput"],
@@ -248,11 +237,6 @@ class WorkerInput:
             blocks_to_copy=tensor_dict.pop("blocks_to_copy"),
             virtual_engine=tensor_dict["virtual_engine"],
             num_steps=tensor_dict.pop("num_steps"),
-            src_block_ids=tensor_dict.pop("src_block_ids"),
-            staging_block_ids=tensor_dict.pop("staging_block_ids"),
-            dst_block_ids=tensor_dict.pop("dst_block_ids"),
-            dst_engine_id=tensor_dict.pop("dst_engine_id"),
-            notify_msg=tensor_dict.pop("notify_msg"),
         )
 
     def as_broadcastable_tensor_dict(
@@ -267,11 +251,6 @@ class WorkerInput:
             "blocks_to_copy": self.blocks_to_copy,
             "virtual_engine": self.virtual_engine,
             "num_steps": self.num_steps,
-            "src_block_ids": self.src_block_ids,
-            "staging_block_ids": self.staging_block_ids,
-            "dst_block_ids": self.dst_block_ids,
-            "dst_engine_id": self.dst_engine_id,
-            "notify_msg": self.notify_msg,
         }
 
         return tensor_dict
@@ -342,16 +321,13 @@ class LocalOrDistributedWorkerBase(WorkerBase):
             return None
 
         worker_input = WorkerInput.from_broadcasted_tensor_dict(broadcast_data)
-        if worker_input.num_seq_groups > 0:
-            model_input = (
-                self.model_runner.make_model_input_from_broadcasted_tensor_dict(
-                    broadcast_data))
+        model_input = (
+            self.model_runner.make_model_input_from_broadcasted_tensor_dict(
+                broadcast_data))
 
-            kwargs = extract_previous_hidden_states(broadcast_data)
+        kwargs = extract_previous_hidden_states(broadcast_data)
 
-            return model_input, worker_input, kwargs
-        else:
-            return None, worker_input, {}
+        return model_input, worker_input, kwargs
 
     def _get_driver_input_and_broadcast(
         self, execute_model_req: ExecuteModelRequest
@@ -427,87 +403,49 @@ class LocalOrDistributedWorkerBase(WorkerBase):
         self.execute_worker(worker_input)
 
         # If there is no input, we don't need to execute the model.
-        if worker_input.num_seq_groups > 0:
-
-            intermediate_tensors = None
-            orig_model_execute_time = 0.0
-            if not get_pp_group().is_first_rank:
-                intermediate_tensors = IntermediateTensors(
-                    get_pp_group().recv_tensor_dict(
-                        all_gather_group=get_tp_group()))
-                if (self.observability_config is not None
-                        and self.observability_config.collect_model_execute_time):
-                    orig_model_execute_time = intermediate_tensors.tensors.get(
-                        "model_execute_time", torch.tensor(0)).item()
-
-            output = self.model_runner.execute_model(
-                model_input=model_input,
-                kv_caches=self.kv_cache[worker_input.virtual_engine]
-                if self.kv_cache is not None else None,
-                intermediate_tensors=intermediate_tensors,
-                num_steps=num_steps,
-                **kwargs,
-            )
+        if worker_input.num_seq_groups == 0:
+            return []
 
-            model_execute_time = time.perf_counter() - start_time
-            if not get_pp_group().is_last_rank:
-                # output is IntermediateTensors
-                assert isinstance(output, IntermediateTensors)
-                if (self.observability_config is not None
-                        and self.observability_config.collect_model_execute_time):
-                    output.tensors["model_execute_time"] = torch.tensor(
-                        model_execute_time + orig_model_execute_time)
-                get_pp_group().send_tensor_dict(output.tensors,
-                                                all_gather_group=get_tp_group())
-                return [None]
+        intermediate_tensors = None
+        orig_model_execute_time = 0.0
+        if not get_pp_group().is_first_rank:
+            intermediate_tensors = IntermediateTensors(
+                get_pp_group().recv_tensor_dict(
+                    all_gather_group=get_tp_group()))
             if (self.observability_config is not None
-                    and self.observability_config.collect_model_execute_time
-                    and output is not None):
-                for o in output:
-                    o.model_execute_time = (orig_model_execute_time +
-                                            model_execute_time)
-
-            self._transfer_blocks(worker_input)
-
-        else:
-            output = []
-
-        # collect kv transfer notifications from non driver workers
+                    and self.observability_config.collect_model_execute_time):
+                orig_model_execute_time = intermediate_tensors.tensors.get(
+                    "model_execute_time", torch.tensor(0)).item()
 
-        if self.nixl_connector is not None:
-            new_notifs = self.nixl_connector.get_new_notifs()
-            rank = get_tp_group().rank
-            all_new_notifs = [new_notifs]
-            if rank > 0:
-                get_tp_group().send_object(new_notifs, dst=0)
-            else:
-                for i in range(1, get_tp_group().world_size):
-                    all_new_notifs.append(get_tp_group().recv_object(src=i))
-
-            request_notif_counter = defaultdict(int)
-            for notifs in all_new_notifs:
-                for req_ids in notifs.values():
-                    for req_id in req_ids:
-                        request_notif_counter[req_id] += 1
-
-            if request_notif_counter:
-                logger.debug("Request notif counter: %s", request_notif_counter)
-
-            request_done_counter = defaultdict(int)
-            for req_id in self.nixl_connector.get_done_tranfers():
-                request_done_counter[req_id] += 1
+        output = self.model_runner.execute_model(
+            model_input=model_input,
+            kv_caches=self.kv_cache[worker_input.virtual_engine]
+            if self.kv_cache is not None else None,
+            intermediate_tensors=intermediate_tensors,
+            num_steps=num_steps,
+            **kwargs,
+        )
 
-            if request_done_counter:
-                logger.debug("Request done counter: %s", request_done_counter)
+        model_execute_time = time.perf_counter() - start_time
+        if not get_pp_group().is_last_rank:
+            # output is IntermediateTensors
+            assert isinstance(output, IntermediateTensors)
+            if (self.observability_config is not None
+                    and self.observability_config.collect_model_execute_time):
+                output.tensors["model_execute_time"] = torch.tensor(
+                    model_execute_time + orig_model_execute_time)
+            get_pp_group().send_tensor_dict(output.tensors,
+                                            all_gather_group=get_tp_group())
+            return [None]
+        if (self.observability_config is not None
+                and self.observability_config.collect_model_execute_time
+                and output is not None):
+            for o in output:
+                o.model_execute_time = (orig_model_execute_time +
+                                        model_execute_time)
 
-        else:
-            request_notif_counter = {}
-            request_done_counter = {}
         # output is List[SamplerOutput]
-        return output, request_notif_counter, request_done_counter
-    
-    def _transfer_blocks(self, worker_input: WorkerInput) -> None:
-        pass
+        return output
 
     def _execute_model_spmd(
         self,
diff --git a/vllm_v0.8.1-dynamo-kv-disagg-patch.patch b/vllm_v0.8.1-dynamo-kv-disagg-patch.patch
deleted file mode 100644
index 2fe31548..00000000
--- a/vllm_v0.8.1-dynamo-kv-disagg-patch.patch
+++ /dev/null
@@ -1,6496 +0,0 @@
-diff --git a/.buildkite/release-pipeline.yaml b/.buildkite/release-pipeline.yaml
-index 18f582b6..37cdab9e 100644
---- a/.buildkite/release-pipeline.yaml
-+++ b/.buildkite/release-pipeline.yaml
-@@ -82,7 +82,7 @@ steps:
-       queue: cpu_queue_postmerge
-     commands:
-       - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
--      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg GIT_REPO_CHECK=1 --tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:$(buildkite-agent meta-data get release-version) --tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:latest --progress plain -f Dockerfile.cpu ."
-+      - "DOCKER_BUILDKIT=1 docker build --build-arg max_jobs=16 --build-arg GIT_REPO_CHECK=1 --tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:$(buildkite-agent meta-data get release-version) --progress plain -f Dockerfile.cpu ."
-       - "docker push public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:$(buildkite-agent meta-data get release-version)"
-     env:
-       DOCKER_BUILDKIT: "1"
-diff --git a/.buildkite/run-tpu-v1-test.sh b/.buildkite/run-tpu-v1-test.sh
-index 6562942e..82f40c65 100755
---- a/.buildkite/run-tpu-v1-test.sh
-+++ b/.buildkite/run-tpu-v1-test.sh
-@@ -19,19 +19,17 @@ docker run --privileged --net host --shm-size=16G -it \
-     vllm-tpu /bin/bash -c "python3 -m pip install git+https://github.com/thuml/depyf.git \
-     && python3 -m pip install pytest \
-     && python3 -m pip install lm_eval[api]==0.4.4 \
--    && export VLLM_USE_V1=1 \
--    && export VLLM_XLA_CHECK_RECOMPILATION=1 \
-     && echo TEST_1 \
--    && python3 /workspace/vllm/tests/tpu/test_compilation.py \
-+    && VLLM_USE_V1=1 python3 /workspace/vllm/tests/tpu/test_compilation.py \
-     && echo TEST_2 \
--    && pytest -v -s /workspace/vllm/tests/v1/tpu/test_basic.py \
-+    && VLLM_USE_V1=1 pytest -v -s /workspace/vllm/tests/v1/tpu/test_basic.py \
-     && echo TEST_3 \
--    && pytest -v -s /workspace/vllm/tests/entrypoints/llm/test_accuracy.py::test_lm_eval_accuracy_v1_engine \
-+    && VLLM_USE_V1=1 pytest -v -s /workspace/vllm/tests/entrypoints/llm/test_accuracy.py::test_lm_eval_accuracy_v1_engine \
-     && echo TEST_4 \
--    && pytest -s -v /workspace/vllm/tests/tpu/test_quantization_accuracy.py \
-+    && VLLM_USE_V1=1 pytest -s -v /workspace/vllm/tests/tpu/test_quantization_accuracy.py \
-     && echo TEST_5 \
--    && python3 /workspace/vllm/examples/offline_inference/tpu.py" \
--
-+    && VLLM_USE_V1=1 python3 /workspace/vllm/examples/offline_inference/tpu.py" \
-+    
- 
- # TODO: This test fails because it uses RANDOM_SEED sampling
- # && VLLM_USE_V1=1 pytest -v -s /workspace/vllm/tests/tpu/test_custom_dispatcher.py \
-diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml
-index 730f272b..230dd838 100644
---- a/.buildkite/test-pipeline.yaml
-+++ b/.buildkite/test-pipeline.yaml
-@@ -136,10 +136,6 @@ steps:
-   - examples/offline_inference/rlhf_colocate.py
-   - tests/examples/offline_inference/data_parallel.py
-   commands:
--  # test with tp=2 and external_dp=2
--  - VLLM_USE_V1=0 torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
--  - torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
--  # test with internal dp
-   - python3 ../examples/offline_inference/data_parallel.py
-   - pytest -v -s distributed/test_utils.py
-   - pytest -v -s compile/test_basic_correctness.py
-@@ -299,7 +295,6 @@ steps:
-   # these tests need to be separated, cannot combine
-   - pytest -v -s compile/piecewise/test_simple.py
-   - pytest -v -s compile/piecewise/test_toy_llama.py
--  - pytest -v -s compile/test_pass_manager.py
- 
- - label: PyTorch Fullgraph Test # 18min
-   source_file_dependencies:
-@@ -516,6 +511,8 @@ steps:
-   - entrypoints/llm/test_collective_rpc.py
-   commands:
-   - pytest -v -s entrypoints/llm/test_collective_rpc.py
-+  - VLLM_USE_V1=1 torchrun --nproc-per-node=2 distributed/test_torchrun_example.py
-+  - torchrun --nproc-per-node=2 distributed/test_torchrun_example.py
-   - pytest -v -s ./compile/test_basic_correctness.py
-   - pytest -v -s ./compile/test_wrapper.py
-   - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py | grep 'Same node test passed'
-diff --git a/.github/ISSUE_TEMPLATE/800-misc-discussion.yml b/.github/ISSUE_TEMPLATE/800-misc-discussion.yml
-new file mode 100644
-index 00000000..79e6e908
---- /dev/null
-+++ b/.github/ISSUE_TEMPLATE/800-misc-discussion.yml
-@@ -0,0 +1,28 @@
-+name:  Misc/random discussions that do not fit into the above categories.
-+description: Submit a discussion as you like. Note that developers are heavily overloaded and we mainly rely on community users to answer these issues.
-+title: "[Misc]: "
-+labels: ["misc"]
-+
-+body:
-+- type: markdown
-+  attributes:
-+    value: >
-+      #### Before submitting an issue, please make sure the issue hasn't been already addressed by searching through [the existing and past issues](https://github.com/vllm-project/vllm/issues?q=is%3Aissue+sort%3Acreated-desc+).
-+- type: textarea
-+  attributes:
-+    label: Anything you want to discuss about vllm.
-+    description: >
-+      Anything you want to discuss about vllm.
-+  validations:
-+    required: true
-+- type: markdown
-+  attributes:
-+    value: >
-+      Thanks for contributing !
-+- type: checkboxes
-+  id: askllm
-+  attributes:
-+    label: Before submitting a new issue...
-+    options:
-+      - label: Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.
-+        required: true
-diff --git a/.github/ISSUE_TEMPLATE/config.yml b/.github/ISSUE_TEMPLATE/config.yml
-index fa40268d..3ba13e0c 100644
---- a/.github/ISSUE_TEMPLATE/config.yml
-+++ b/.github/ISSUE_TEMPLATE/config.yml
-@@ -1,5 +1 @@
- blank_issues_enabled: false
--contact_links:
--  - name: Questions
--    url: https://discuss.vllm.ai
--    about: Ask questions and discuss with other vLLM community members
-diff --git a/README.md b/README.md
-index d829b057..20906f4f 100644
---- a/README.md
-+++ b/README.md
-@@ -10,17 +10,9 @@ Easy, fast, and cheap LLM serving for everyone
- </h3>
- 
- <p align="center">
--| <a href="https://docs.vllm.ai"><b>Documentation</b></a> | <a href="https://vllm.ai"><b>Blog</b></a> | <a href="https://arxiv.org/abs/2309.06180"><b>Paper</b></a> | <a href="https://x.com/vllm_project"><b>Twitter/X</b></a> | <a href="https://discuss.vllm.ai"><b>User Forum</b></a> | <a href="https://slack.vllm.ai"><b>Developer Slack</b></a> |
-+| <a href="https://docs.vllm.ai"><b>Documentation</b></a> | <a href="https://vllm.ai"><b>Blog</b></a> | <a href="https://arxiv.org/abs/2309.06180"><b>Paper</b></a> | <a href="https://x.com/vllm_project"><b>Twitter/X</b></a> | <a href="https://slack.vllm.ai"><b>Developer Slack</b></a> |
- </p>
- 
-----
--
--[2025/03] We are collaborating with Ollama to host an [Inference Night](https://lu.ma/vllm-ollama) at Y Combinator in San Francisco on Thursday, March 27, at 6 PM. Discuss all things inference local or data center!
--
--[2025/04] We're hosting our first-ever *vLLM Asia Developer Day* in Singapore on *April 3rd*! This is a full-day event (9 AM - 9 PM SGT) in partnership with SGInnovate, AMD, and Embedded LLM. Meet the vLLM team and learn about LLM inference for RL, MI300X, and more! [Register Now](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)
--
-----
--
- *Latest News* 
- 
- - [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
-@@ -28,7 +20,19 @@ Easy, fast, and cheap LLM serving for everyone
- - [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- - [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).
- - [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
--- [2024/12] vLLM joins [PyTorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
-+- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
-+- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
-+- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
-+- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
-+- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
-+- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
-+- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
-+- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
-+- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
-+- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
-+- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
-+- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
-+- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).
- 
- ---
- 
-@@ -139,11 +143,10 @@ If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs
- 
- ## Contact Us
- 
--- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
--- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
--- coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
--- For security disclosures, please use GitHub's [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
--- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)
-+- For technical questions and feature requests, please use GitHub issues or discussions.
-+- For discussing with fellow users and coordinating contributions and development, please use Slack.
-+- For security disclosures, please use GitHub's security advisory feature.
-+- For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.
- 
- ## Media Kit
- 
-diff --git a/benchmarks/README.md b/benchmarks/README.md
-index d41de1ca..3225a4b0 100644
---- a/benchmarks/README.md
-+++ b/benchmarks/README.md
-@@ -42,7 +42,7 @@ become available.
-     </tr>
-     <tr>
-       <td><strong>HuggingFace</strong></td>
--      <td style="text-align: center;"></td>
-+      <td style="text-align: center;"></td>
-       <td style="text-align: center;"></td>
-       <td>Specify your dataset path on HuggingFace</td>
-     </tr>
-@@ -60,8 +60,8 @@ become available.
- : to be supported
- 
- : Partial support. Currently, HuggingFaceDataset only supports dataset formats
--similar to `lmms-lab/LLaVA-OneVision-Data` and `Aeala/ShareGPT_Vicuna_unfiltered`.
--If you need support for other dataset formats, please consider contributing.
-+similar to `lmms-lab/LLaVA-OneVision-Data`. If you need support for other dataset
-+formats, please consider contributing.
- 
- **Note**: VisionArenas `dataset-name` should be set to `hf`
- 
-@@ -139,57 +139,6 @@ python3 vllm/benchmarks/benchmark_serving.py \
-   --num-prompts "${NUM_PROMPTS}"
- ```
- 
--### HuggingFaceDataset Examples
--
--Currently, HuggingFaceDataset only supports dataset formats
--similar to `lmms-lab/LLaVA-OneVision-Data` and `Aeala/ShareGPT_Vicuna_unfiltered`. If you need support for other dataset
--formats, please consider contributing.
--
--```bash
--# need a model with vision capability here
--vllm serve Qwen/Qwen2-VL-7B-Instruct --disable-log-requests
--```
--
--**`lmms-lab/LLaVA-OneVision-Data`**
--
--```bash
--MODEL_NAME="Qwen/Qwen2-VL-7B-Instruct"
--NUM_PROMPTS=10
--BACKEND="openai-chat"
--DATASET_NAME="hf"
--DATASET_PATH="lmms-lab/LLaVA-OneVision-Data"
--DATASET_SPLIT='train'
--DATASET_SUBSET='chart2text(cauldron)'
--python3 vllm/benchmarks/benchmark_serving.py \
--  --backend "${BACKEND}" \
--  --model "${MODEL_NAME}" \
--  --endpoint "/v1/chat/completions" \
--  --dataset-name "${DATASET_NAME}" \
--  --dataset-path "${DATASET_PATH}" \
--  --hf-split "${DATASET_SPLIT}" \
--  --num-prompts "${NUM_PROMPTS}" \
--  --hf-subset "${DATASET_SUBSET}"
--```
--
--**`Aeala/ShareGPT_Vicuna_unfiltered`**
--
--```bash
--MODEL_NAME="Qwen/Qwen2-VL-7B-Instruct"
--NUM_PROMPTS=10
--BACKEND="openai-chat"
--DATASET_NAME="hf"
--DATASET_PATH="Aeala/ShareGPT_Vicuna_unfiltered"
--DATASET_SPLIT='train'
--python3 vllm/benchmarks/benchmark_serving.py \
--  --backend "${BACKEND}" \
--  --model "${MODEL_NAME}" \
--  --endpoint "/v1/chat/completions" \
--  --dataset-name "${DATASET_NAME}" \
--  --dataset-path "${DATASET_PATH}" \
--  --hf-split "${DATASET_SPLIT}" \
--  --num-prompts "${NUM_PROMPTS}" \
--```
--
- ---
- ## Example - Offline Throughput Benchmark
- 
-diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
-index 0f13c79a..09c8e23e 100644
---- a/benchmarks/backend_request_func.py
-+++ b/benchmarks/backend_request_func.py
-@@ -63,7 +63,7 @@ async def async_request_tgi(
-             "temperature": 0.01,  # TGI does not accept 0.0 temperature.
-             "top_p": 0.99,  # TGI does not accept 1.0 top_p.
-             "truncate": request_func_input.prompt_len,
--            "ignore_eos_token": request_func_input.ignore_eos,
-+            # TGI does not accept ignore_eos flag.
-         }
-         payload = {
-             "inputs": request_func_input.prompt,
-@@ -71,10 +71,6 @@ async def async_request_tgi(
-         }
-         output = RequestFuncOutput()
-         output.prompt_len = request_func_input.prompt_len
--        if request_func_input.ignore_eos:
--            output.output_tokens = request_func_input.output_len
--        else:
--            output.output_tokens = None
- 
-         ttft = 0.0
-         st = time.perf_counter()
-diff --git a/benchmarks/benchmark_dataset.py b/benchmarks/benchmark_dataset.py
-index 0567875f..55109dab 100644
---- a/benchmarks/benchmark_dataset.py
-+++ b/benchmarks/benchmark_dataset.py
-@@ -17,7 +17,6 @@ SampleRequest instances, similar to the approach used in ShareGPT.
- import base64
- import io
- import json
--import logging
- import random
- from abc import ABC, abstractmethod
- from collections.abc import Mapping
-@@ -36,8 +35,6 @@ from vllm.lora.utils import get_adapter_absolute_path
- from vllm.multimodal import MultiModalDataDict
- from vllm.transformers_utils.tokenizer import AnyTokenizer, get_lora_tokenizer
- 
--logger = logging.getLogger(__name__)
--
- # -----------------------------------------------------------------------------
- # Data Classes
- # -----------------------------------------------------------------------------
-@@ -64,6 +61,9 @@ class SampleRequest:
- class BenchmarkDataset(ABC):
-     DEFAULT_SEED = 0
- 
-+    # num_requests has default 1000 in both the benchmark_serving.py and
-+    # benchmark_throughput.py
-+
-     def __init__(
-         self,
-         dataset_path: Optional[str] = None,
-@@ -90,8 +90,8 @@ class BenchmarkDataset(ABC):
-             mm_content: Optional[MultiModalDataDict] = None) -> list[dict]:
-         """
-         Transform a prompt and optional multimodal content into a chat format.
--        This method is used for chat models that expect a specific conversation
--        format.
-+        This method is used for chat models that expect a specific 
-+        conversation format.
-         """
-         content = [{"text": prompt, "type": "text"}]
-         if mm_content is not None:
-@@ -101,10 +101,10 @@ class BenchmarkDataset(ABC):
-     def load_data(self) -> None:
-         """
-         Load data from the dataset path into self.data.
--
-+        
-         This method must be overridden by subclasses since the method to load
-         data will vary depending on the dataset format and source.
--
-+        
-         Raises:
-             NotImplementedError: If a subclass does not implement this method.
-         """
-@@ -121,18 +121,18 @@ class BenchmarkDataset(ABC):
-         """
-         Optionally select a random LoRA request and return its associated
-         tokenizer.
--
-+        
-         This method is used when LoRA parameters are provided.  It randomly
-         selects a LoRA based on max_loras and retrieves a cached tokenizer for
-         that LoRA if available. Otherwise, it returns the base tokenizer.
--
-+        
-         Args:
-             tokenizer (PreTrainedTokenizerBase): The base tokenizer to use if no
-             LoRA is selected.  max_loras (Optional[int]): The maximum number of
-             LoRAs available. If None, LoRA is not used.  lora_path
-             (Optional[str]): Path to the LoRA parameters on disk. If None, LoRA
-             is not used.
--
-+        
-         Returns:
-             tuple[Optional[LoRARequest], AnyTokenizer]: A tuple where the first
-             element is a LoRARequest (or None if not applicable) and the second
-@@ -160,39 +160,21 @@ class BenchmarkDataset(ABC):
-                num_requests: int) -> list[SampleRequest]:
-         """
-         Abstract method to generate sample requests from the dataset.
--
-+        
-         Subclasses must override this method to implement dataset-specific logic
-         for generating a list of SampleRequest objects.
--
-+        
-         Args:
-             tokenizer (PreTrainedTokenizerBase): The tokenizer to be used
-              for processing the dataset's text.
-             num_requests (int): The number of sample requests to generate.
--
-+        
-         Returns:
-             list[SampleRequest]: A list of sample requests generated from the
-             dataset.
-         """
-         raise NotImplementedError("sample must be implemented in subclasses.")
- 
--    def maybe_oversample_requests(self, requests: list[SampleRequest],
--                                  num_requests: int) -> None:
--        """
--        Oversamples the list of requests if its size is less than the desired
--        number.
--
--        Args:
--            requests (List[SampleRequest]): The current list of sampled
--            requests.  num_requests (int): The target number of requests.
--        """
--        if len(requests) < num_requests:
--            random.seed(self.random_seed)
--            additional = random.choices(requests,
--                                        k=num_requests - len(requests))
--            requests.extend(additional)
--            logger.info("Oversampled requests to reach %d total samples.",
--                        num_requests)
--
- 
- # -----------------------------------------------------------------------------
- # Utility Functions and Global Caches
-@@ -294,16 +276,15 @@ class RandomDataset(BenchmarkDataset):
-     ) -> None:
-         super().__init__(**kwargs)
- 
--    def sample(
--        self,
--        tokenizer: PreTrainedTokenizerBase,
--        num_requests: int,
--        prefix_len: int = DEFAULT_PREFIX_LEN,
--        range_ratio: float = DEFAULT_RANGE_RATIO,
--        input_len: int = DEFAULT_INPUT_LEN,
--        output_len: int = DEFAULT_OUTPUT_LEN,
--        **kwargs,
--    ) -> list[SampleRequest]:
-+    def sample(self,
-+               tokenizer: PreTrainedTokenizerBase,
-+               num_requests: int,
-+               prefix_len: int = DEFAULT_PREFIX_LEN,
-+               range_ratio: float = DEFAULT_RANGE_RATIO,
-+               input_len: int = DEFAULT_INPUT_LEN,
-+               output_len: int = DEFAULT_OUTPUT_LEN,
-+               **kwargs) -> list[SampleRequest]:
-+
-         vocab_size = tokenizer.vocab_size
- 
-         prefix_token_ids = (np.random.randint(
-@@ -365,24 +346,20 @@ class ShareGPTDataset(BenchmarkDataset):
-         random.seed(self.random_seed)
-         random.shuffle(self.data)
- 
--    def sample(
--        self,
--        tokenizer: PreTrainedTokenizerBase,
--        num_requests: int,
--        lora_path: Optional[str] = None,
--        max_loras: Optional[int] = None,
--        output_len: Optional[int] = None,
--        enable_multimodal_chat: bool = False,
--        **kwargs,
--    ) -> list:
-+    def sample(self,
-+               tokenizer: PreTrainedTokenizerBase,
-+               num_requests: int,
-+               lora_path: Optional[str] = None,
-+               max_loras: Optional[int] = None,
-+               output_len: Optional[int] = None,
-+               enable_multimodal_chat: bool = False,
-+               **kwargs) -> list:
-         samples: list = []
-         for entry in self.data:
-             if len(samples) >= num_requests:
-                 break
--            prompt, completion = (
--                entry["conversations"][0]["value"],
--                entry["conversations"][1]["value"],
--            )
-+            prompt, completion = entry["conversations"][0]["value"],\
-+                entry["conversations"][1]["value"]
- 
-             lora_request, tokenizer = self.get_random_lora_request(
-                 tokenizer=tokenizer, max_loras=max_loras, lora_path=lora_path)
-@@ -406,7 +383,6 @@ class ShareGPTDataset(BenchmarkDataset):
-                     expected_output_len=new_output_len,
-                     lora_request=lora_request,
-                 ))
--        self.maybe_oversample_requests(samples, num_requests)
-         return samples
- 
- 
-@@ -439,20 +415,19 @@ class SonnetDataset(BenchmarkDataset):
-         with open(self.dataset_path, encoding="utf-8") as f:
-             self.data = f.readlines()
- 
--    def sample(
--        self,
--        tokenizer,
--        num_requests: int,
--        prefix_len: int = DEFAULT_PREFIX_LEN,
--        input_len: int = DEFAULT_INPUT_LEN,
--        output_len: int = DEFAULT_OUTPUT_LEN,
--        return_prompt_formatted: bool = False,
--        **kwargs,
--    ) -> list:
-+    def sample(self,
-+               tokenizer,
-+               num_requests: int,
-+               prefix_len: int = DEFAULT_PREFIX_LEN,
-+               input_len: int = DEFAULT_INPUT_LEN,
-+               output_len: int = DEFAULT_OUTPUT_LEN,
-+               return_prompt_formatted: bool = False,
-+               **kwargs) -> list:
-         # Calculate average token length for a poem line.
-         tokenized_lines = [tokenizer(line).input_ids for line in self.data]
-         avg_len = sum(len(tokens)
--                      for tokens in tokenized_lines) / len(tokenized_lines)
-+                      for tokens in \
-+                        tokenized_lines) / len(tokenized_lines)
- 
-         # Build the base prompt.
-         base_prompt = "Pick as many lines as you can from these poem lines:\n"
-@@ -531,14 +506,12 @@ class BurstGPTDataset(BenchmarkDataset):
-         # Convert the dataframe to a list of lists.
-         return data.values.tolist()
- 
--    def sample(
--        self,
--        tokenizer: PreTrainedTokenizerBase,
--        num_requests: int,
--        max_loras: Optional[int] = None,
--        lora_path: Optional[str] = None,
--        **kwargs,
--    ) -> list[SampleRequest]:
-+    def sample(self,
-+               tokenizer: PreTrainedTokenizerBase,
-+               num_requests: int,
-+               max_loras: Optional[int] = None,
-+               lora_path: Optional[str] = None,
-+               **kwargs) -> list[SampleRequest]:
-         samples = []
-         data = self._sample_loaded_data(num_requests=num_requests)
-         for i in range(num_requests):
-@@ -571,6 +544,7 @@ class HuggingFaceDataset(BenchmarkDataset):
-     Dataset class for processing a HuggingFace dataset with conversation data
-     and optional images.
-     """
-+    DEFAULT_NUM_REQUESTS = 1000
- 
-     def __init__(
-         self,
-@@ -644,7 +618,6 @@ class HuggingFaceDataset(BenchmarkDataset):
-                     expected_output_len=output_len,
-                     multi_modal_data=mm_content,
-                 ))
--        self.maybe_oversample_requests(sampled_requests, num_requests)
-         return sampled_requests
- 
- 
-@@ -659,6 +632,7 @@ class VisionArenaDataset(HuggingFaceDataset):
-     """
- 
-     DEFAULT_OUTPUT_LEN = 128
-+    DEFAULT_NUM_REQUESTS = 1000
-     VISION_ARENA_DATASET_PATH = "lmarena-ai/vision-arena-bench-v0.1"
- 
-     def __init__(
-@@ -683,14 +657,12 @@ class VisionArenaDataset(HuggingFaceDataset):
-         )
-         self.data = dataset.shuffle(seed=self.random_seed)
- 
--    def sample(
--        self,
--        tokenizer: PreTrainedTokenizerBase,
--        num_requests: int,
--        output_len: Optional[int] = None,
--        enable_multimodal_chat: bool = False,
--        **kwargs,
--    ) -> list:
-+    def sample(self,
-+               tokenizer: PreTrainedTokenizerBase,
-+               num_requests: int,
-+               output_len: Optional[int] = None,
-+               enable_multimodal_chat: bool = False,
-+               **kwargs) -> list:
-         output_len = (output_len
-                       if output_len is not None else self.DEFAULT_OUTPUT_LEN)
-         sampled_requests = []
-@@ -713,5 +685,4 @@ class VisionArenaDataset(HuggingFaceDataset):
-                     expected_output_len=output_len,
-                     multi_modal_data=mm_content,
-                 ))
--        self.maybe_oversample_requests(sampled_requests, num_requests)
-         return sampled_requests
-diff --git a/benchmarks/benchmark_serving_structured_output.py b/benchmarks/benchmark_serving_structured_output.py
-index c79a93fa..444bda2a 100644
---- a/benchmarks/benchmark_serving_structured_output.py
-+++ b/benchmarks/benchmark_serving_structured_output.py
-@@ -999,12 +999,11 @@ if __name__ == "__main__":
-                         type=float,
-                         default=1.0,
-                         help="Ratio of Structured Outputs requests")
--    parser.add_argument(
--        "--structured-output-backend",
--        type=str,
--        choices=["outlines", "lm-format-enforcer", "xgrammar", "guidance"],
--        default="xgrammar",
--        help="Backend to use for structured outputs")
-+    parser.add_argument("--structured-output-backend",
-+                        type=str,
-+                        choices=["outlines", "lm-format-enforcer", "xgrammar"],
-+                        default="xgrammar",
-+                        help="Backend to use for structured outputs")
- 
-     args = parser.parse_args()
-     main(args)
-diff --git a/cmake/external_projects/vllm_flash_attn.cmake b/cmake/external_projects/vllm_flash_attn.cmake
-index afd7c47e..f2d01099 100644
---- a/cmake/external_projects/vllm_flash_attn.cmake
-+++ b/cmake/external_projects/vllm_flash_attn.cmake
-@@ -38,7 +38,7 @@ else()
-   FetchContent_Declare(
-           vllm-flash-attn
-           GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git
--          GIT_TAG dc9d410b3e2d6534a4c70724c2515f4def670a22
-+          GIT_TAG 9bfa9869829d8c593527eb34c5271d0090f7ccc9 
-           GIT_PROGRESS TRUE
-           # Don't share the vllm-flash-attn build between build types
-           BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn
-diff --git a/csrc/quantization/fused_kernels/layernorm_utils.cuh b/csrc/quantization/fused_kernels/layernorm_utils.cuh
-index b5cea98f..cec6b54e 100644
---- a/csrc/quantization/fused_kernels/layernorm_utils.cuh
-+++ b/csrc/quantization/fused_kernels/layernorm_utils.cuh
-@@ -24,7 +24,7 @@ __device__ void compute_rms(float* rms, scalar_t const* __restrict__ input,
-   // sum of squares
-   float ss = 0.0f;
- 
--  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-     float x = static_cast<float>(input[token_offset + i]);
-     if constexpr (has_residual) {
-       x += static_cast<float>(residual[token_offset + i]);
-@@ -58,7 +58,7 @@ __device__ void compute_dynamic_per_token_scales(
-   constexpr scalar_out_t qmax{std::numeric_limits<scalar_out_t>::max()};
- 
-   float block_absmax_val_maybe = 0.0f;
--  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-     float x = static_cast<float>(input[token_offset + i]);
-     if constexpr (has_residual) {
-       x += static_cast<float>(residual[token_offset + i]);
-@@ -103,7 +103,7 @@ __device__ void norm_and_quant(scalar_out_t* __restrict__ output,
-   int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);
-   ;
- 
--  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < hidden_size; i += blockDim.x) {
-     float x = static_cast<float>(input[token_offset + i]);
-     if constexpr (has_residual) {
-       x += static_cast<float>(residual[token_offset + i]);
-@@ -142,7 +142,7 @@ __device__ void compute_rms(float* rms, scalar_t const* __restrict__ input,
-   int32_t const num_vec_elems = hidden_size >> 2;
- 
- #pragma unroll 4
--  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-     vec4_t<scalar_t> in = vec_input[i];
- 
-     vec4_t<float> x;
-@@ -206,7 +206,7 @@ __device__ void compute_dynamic_per_token_scales(
-   float block_absmax_val_maybe = 0.0f;
- 
- #pragma unroll 4
--  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-     vec4_t<scalar_t> in = vec_input[i];
-     vec4_t<scalar_t> const w = vec_weight[i];
- 
-@@ -286,7 +286,7 @@ __device__ void norm_and_quant(scalar_out_t* __restrict__ output,
- // TODO(luka/varun) extract into type-agnostic vectorized quant function to
- //  replace scaled_fp8_conversion_vec
- #pragma unroll 4
--  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-+  for (int32_t i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {
-     vec4_t<scalar_t> const in = vec_input[i];
-     vec4_t<scalar_t> const w = vec_weight[i];
- 
-diff --git a/csrc/quantization/gguf/dequantize.cuh b/csrc/quantization/gguf/dequantize.cuh
-index 41fc032f..c012262e 100644
---- a/csrc/quantization/gguf/dequantize.cuh
-+++ b/csrc/quantization/gguf/dequantize.cuh
-@@ -101,10 +101,10 @@ static __global__ void dequantize_block(const void * __restrict__ vx, dst_t * __
- template<typename dst_t>
- static __global__ void dequantize_block_q2_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_q2_K * x = (const block_q2_K *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int n   = tid/32;
-     const int l   = tid - 32*n;
-     const int is  = 8*n + l/16;
-@@ -123,10 +123,10 @@ static __global__ void dequantize_block_q2_K(const void * __restrict__ vx, dst_t
- template<typename dst_t>
- static __global__ void dequantize_block_q3_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i = blockIdx.x;
-+    const int i = blockIdx.x;
-     const block_q3_K * x = (const block_q3_K *) vx;
- 
--    const auto r = threadIdx.x/4;
-+    const int r = threadIdx.x/4;
-     const int tid = r/2;
-     const int is0 = r%2;
-     const int l0 = 16*is0 + 4*(threadIdx.x%4);
-@@ -164,10 +164,10 @@ template<typename dst_t>
- static __global__ void dequantize_block_q4_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {
-     const block_q4_K * x = (const block_q4_K *) vx;
- 
--    const auto i = blockIdx.x;
-+    const int i = blockIdx.x;
- 
-     // assume 32 threads
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il  = tid/8;
-     const int ir  = tid%8;
-     const int is  = 2*il;
-@@ -197,10 +197,10 @@ template<typename dst_t>
- static __global__ void dequantize_block_q5_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {
-     const block_q5_K * x = (const block_q5_K *) vx;
- 
--    const auto i = blockIdx.x;
-+    const int i = blockIdx.x;
- 
-     // assume 64 threads - this is very slightly better than the one below
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il  = tid/16;   // il is in 0...3
-     const int ir  = tid%16;   // ir is in 0...15
-     const int is  = 2*il;     // is is in 0...6
-@@ -231,10 +231,10 @@ template<typename dst_t>
- static __global__ void dequantize_block_q6_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {
-     const block_q6_K * x = (const block_q6_K *) vx;
- 
--    const auto i = blockIdx.x;
-+    const int i = blockIdx.x;
- 
-     // assume 64 threads - this is very slightly better than the one below
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int ip  = tid/32;   // ip is 0 or 1
-     const int il  = tid - 32*ip; // 0...32
-     const int is  = 8*ip + il/16;
-@@ -256,10 +256,10 @@ static __global__ void dequantize_block_q6_K(const void * __restrict__ vx, dst_t
- template<typename dst_t>
- static __global__ void dequantize_block_iq2_xxs(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq2_xxs * x = (const block_iq2_xxs  *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 8*il;
-@@ -275,10 +275,10 @@ static __global__ void dequantize_block_iq2_xxs(const void * __restrict__ vx, ds
- template<typename dst_t>
- static __global__ void dequantize_block_iq2_xs(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq2_xs * x = (const block_iq2_xs *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 8*il;
-@@ -293,10 +293,10 @@ static __global__ void dequantize_block_iq2_xs(const void * __restrict__ vx, dst
- template<typename dst_t>
- static __global__ void dequantize_block_iq2_s(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq2_s * x = (const block_iq2_s *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 8*il;
-@@ -309,10 +309,10 @@ static __global__ void dequantize_block_iq2_s(const void * __restrict__ vx, dst_
- template<typename dst_t>
- static __global__ void dequantize_block_iq3_xxs(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq3_xxs * x = (const block_iq3_xxs  *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 8*il;
-@@ -332,10 +332,10 @@ static __global__ void dequantize_block_iq3_xxs(const void * __restrict__ vx, ds
- template<typename dst_t>
- static __global__ void dequantize_block_iq3_s(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq3_s * x = (const block_iq3_s *) vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 8*il;
-@@ -399,10 +399,10 @@ static __global__ void dequantize_block_iq1_m(const void * __restrict__ vx, dst_
- template<typename dst_t>
- static __global__ void dequantize_block_iq4_nl(const void * __restrict__ vx, dst_t * __restrict__ yy) {
- 
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq4_nl * x = (const block_iq4_nl *) vx + i*(QK_K/QK4_NL);
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 4*il;
-@@ -417,10 +417,10 @@ static __global__ void dequantize_block_iq4_nl(const void * __restrict__ vx, dst
- 
- template<typename dst_t>
- static __global__ void dequantize_block_iq4_xs(const void * __restrict__ vx, dst_t * __restrict__ yy) {
--    const auto i   = blockIdx.x;
-+    const int i   = blockIdx.x;
-     const block_iq4_xs * x = (const block_iq4_xs *)vx;
- 
--    const auto tid = threadIdx.x;
-+    const int tid = threadIdx.x;
-     const int il = tid/8; // 0...3
-     const int ib = tid%8; // 0...7
-     dst_t * y = yy + i*QK_K + 32*ib + 4*il;
-@@ -565,4 +565,4 @@ static to_fp16_cuda_t ggml_get_to_fp16_cuda(int64_t type) {
-         default:
-             return nullptr;
-     }
--}
-+}
-\ No newline at end of file
-diff --git a/csrc/quantization/gguf/gguf_kernel.cu b/csrc/quantization/gguf/gguf_kernel.cu
-index b0f31c45..46b716bb 100644
---- a/csrc/quantization/gguf/gguf_kernel.cu
-+++ b/csrc/quantization/gguf/gguf_kernel.cu
-@@ -19,11 +19,11 @@ template <typename scalar_t>
- static __global__ void quantize_q8_1(const scalar_t* __restrict__ x,
-                                      void* __restrict__ vy, const int kx,
-                                      const int kx_padded) {
--  const auto ix = blockDim.x * blockIdx.x + threadIdx.x;
-+  const int ix = blockDim.x * blockIdx.x + threadIdx.x;
-   if (ix >= kx_padded) {
-     return;
-   }
--  const auto iy = blockDim.y * blockIdx.y + threadIdx.y;
-+  const int iy = blockDim.y * blockIdx.y + threadIdx.y;
-   const int i_padded = iy * kx_padded + ix;
- 
-   block_q8_1* y = (block_q8_1*)vy;
-diff --git a/csrc/quantization/gguf/mmq.cuh b/csrc/quantization/gguf/mmq.cuh
-index 7c89918c..e2b93680 100644
---- a/csrc/quantization/gguf/mmq.cuh
-+++ b/csrc/quantization/gguf/mmq.cuh
-@@ -14,10 +14,10 @@ static __device__ __forceinline__ void mul_mat_q(
- 
-     const int & ncols_dst = ncols_y;
- 
--    const auto row_dst_0 = blockIdx.x*mmq_y;
-+    const int row_dst_0 = blockIdx.x*mmq_y;
-     const int & row_x_0 = row_dst_0;
- 
--    const auto col_dst_0 = blockIdx.y*mmq_x;
-+    const int col_dst_0 = blockIdx.y*mmq_x;
-     const int & col_y_0 = col_dst_0;
- 
-     int   * tile_x_ql = nullptr;
-@@ -39,7 +39,7 @@ static __device__ __forceinline__ void mul_mat_q(
- 
- #pragma unroll
-         for (int ir = 0; ir < qr && ib0 + ir * blocks_per_warp/qr < blocks_per_row_x; ++ir) {
--            const auto kqs = ir*WARP_SIZE_GGUF + threadIdx.x;
-+            const int kqs = ir*WARP_SIZE_GGUF + threadIdx.x;
-             const int kbxd = kqs / QI8_1;
- 
- #pragma unroll
-@@ -53,7 +53,7 @@ static __device__ __forceinline__ void mul_mat_q(
- #pragma unroll
-             for (int ids0 = 0; ids0 < mmq_x; ids0 += nwarps * QI8_1) {
-                 const int ids = (ids0 + threadIdx.y * QI8_1 + threadIdx.x / (WARP_SIZE_GGUF/QI8_1)) % mmq_x;
--                const auto kby = threadIdx.x % (WARP_SIZE_GGUF/QI8_1);
-+                const int kby = threadIdx.x % (WARP_SIZE_GGUF/QI8_1);
-                 const int col_y_eff = min(col_y_0 + ids, ncols_y-1);
- 
-                 // if the sum is not needed it's faster to transform the scale to f32 ahead of time
-@@ -87,14 +87,14 @@ static __device__ __forceinline__ void mul_mat_q(
- 
- #pragma unroll
-     for (int j = 0; j < mmq_x; j += nwarps) {
--        const auto col_dst = col_dst_0 + j + threadIdx.y;
-+        const int col_dst = col_dst_0 + j + threadIdx.y;
-         if (col_dst >= ncols_dst) {
-             return;
-         }
- 
- #pragma unroll
-         for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {
--            const auto row_dst = row_dst_0 + threadIdx.x + i;
-+            const int row_dst = row_dst_0 + threadIdx.x + i;
-             if (row_dst >= nrows_dst) {
-                 continue;
-             }
-diff --git a/csrc/quantization/gguf/mmvq.cuh b/csrc/quantization/gguf/mmvq.cuh
-index 687cb0a3..d83f2974 100644
---- a/csrc/quantization/gguf/mmvq.cuh
-+++ b/csrc/quantization/gguf/mmvq.cuh
-@@ -1,7 +1,7 @@
- // copied and adapted from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/mmvq.cu
- template <typename scalar_t, int qk, int qi, typename block_q_t, int vdr, vec_dot_q_cuda_t vec_dot_q_cuda>
- static __global__ void mul_mat_vec_q(const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst, const int ncols, const int nrows) {
--    const auto row = blockIdx.x*blockDim.y + threadIdx.y;
-+    const int row = blockIdx.x*blockDim.y + threadIdx.y;
- 
-     if (row >= nrows) {
-         return;
-@@ -16,7 +16,7 @@ static __global__ void mul_mat_vec_q(const void * __restrict__ vx, const void *
-     const block_q_t  * x = (const block_q_t  *) vx;
-     const block_q8_1 * y = (const block_q8_1 *) vy;
- 
--    for (auto i = threadIdx.x / (qi/vdr); i < blocks_per_row; i += blocks_per_warp) {
-+    for (int i = threadIdx.x / (qi/vdr); i < blocks_per_row; i += blocks_per_warp) {
-         const int ibx = row*blocks_per_row + i; // x block index
- 
-         const int iby = i * (qk/QK8_1); // y block index that aligns with ibx
-diff --git a/csrc/quantization/gguf/moe.cuh b/csrc/quantization/gguf/moe.cuh
-index 2dbafc0f..e499f53a 100644
---- a/csrc/quantization/gguf/moe.cuh
-+++ b/csrc/quantization/gguf/moe.cuh
-@@ -19,10 +19,10 @@ static __device__ __forceinline__ void moe_q(
- 
-   const int ncols_dst = ncols_y * top_k;
- 
--  const auto row_dst_0 = blockIdx.x * mmq_y;
-+  const int row_dst_0 = blockIdx.x * mmq_y;
-   const int& row_x_0 = row_dst_0;
- 
--  const auto col_dst_0 = blockIdx.y * mmq_x;
-+  const int col_dst_0 = blockIdx.y * mmq_x;
- 
-   int token_offs[mmq_x / nwarps];
-   for (int i = 0; i < mmq_x; i += nwarps) {
-@@ -56,7 +56,7 @@ static __device__ __forceinline__ void moe_q(
-     const int n_per_r = ((qk * blocks_per_warp) / qr);
- #pragma unroll
-     for (int ir = 0; ir < qr && ib0 * qk + ir * n_per_r < ncols_x; ++ir) {
--      const auto kqs = ir * WARP_SIZE_GGUF + threadIdx.x;
-+      const int kqs = ir * WARP_SIZE_GGUF + threadIdx.x;
-       const int kbxd = kqs / QI8_1;
- 
- #pragma unroll
-@@ -73,7 +73,7 @@ static __device__ __forceinline__ void moe_q(
-       }
- 
-       if (threadIdx.x < n_per_r / QK8_1) {
--        const auto kby = threadIdx.x % (WARP_SIZE_GGUF / QI8_1);
-+        const int kby = threadIdx.x % (WARP_SIZE_GGUF / QI8_1);
-         const int col_y_eff = token_offs[threadIdx.y] / top_k;
-         const int block_x =
-             ib0 * (qk / QK8_1) + ir * (WARP_SIZE_GGUF / QI8_1) + kby;
-@@ -119,7 +119,7 @@ static __device__ __forceinline__ void moe_q(
- 
- #pragma unroll
-     for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {
--      const auto row_dst = row_dst_0 + threadIdx.x + i;
-+      const int row_dst = row_dst_0 + threadIdx.x + i;
-       if (row_dst >= nrows_dst) {
-         continue;
-       }
-diff --git a/csrc/quantization/gptq/q_gemm.cu b/csrc/quantization/gptq/q_gemm.cu
-index 6fad16e1..538cb584 100644
---- a/csrc/quantization/gptq/q_gemm.cu
-+++ b/csrc/quantization/gptq/q_gemm.cu
-@@ -199,12 +199,12 @@ __global__ void gemm_half_q_half_gptq_4bit_kernel(
-   MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   // Block
--  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
--  auto offset_m = blockIdx.y * m_count;
--  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;
-+  int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
-+  int offset_m = blockIdx.y * m_count;
-+  int offset_k = blockIdx.z * BLOCK_KN_SIZE;
- 
-   [[maybe_unused]] int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-   [[maybe_unused]] int end_m = min(offset_m + m_count, size_m);
-@@ -337,12 +337,12 @@ __global__ void gemm_half_q_half_gptq_2bit_kernel(
-   MatrixView_q2_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   // Block
--  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
--  auto offset_m = blockIdx.y * m_count;
--  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;
-+  int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
-+  int offset_m = blockIdx.y * m_count;
-+  int offset_k = blockIdx.z * BLOCK_KN_SIZE;
- 
-   [[maybe_unused]] int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-   [[maybe_unused]] int end_m = min(offset_m + m_count, size_m);
-@@ -458,12 +458,12 @@ __global__ void gemm_half_q_half_gptq_3bit_kernel(
-   MatrixView_q3_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   // Block
--  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
--  auto offset_m = blockIdx.y * m_count;
--  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;
-+  int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
-+  int offset_m = blockIdx.y * m_count;
-+  int offset_k = blockIdx.z * BLOCK_KN_SIZE;
- 
-   [[maybe_unused]] int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-   [[maybe_unused]] int end_m = min(offset_m + m_count, size_m);
-@@ -586,12 +586,12 @@ __global__ void gemm_half_q_half_gptq_8bit_kernel(
-   MatrixView_q8_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   // Block
--  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
--  auto offset_m = blockIdx.y * m_count;
--  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;
-+  int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
-+  int offset_m = blockIdx.y * m_count;
-+  int offset_k = blockIdx.z * BLOCK_KN_SIZE;
- 
-   [[maybe_unused]] int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-   [[maybe_unused]] int end_m = min(offset_m + m_count, size_m);
-@@ -765,14 +765,14 @@ __global__ void reconstruct_exllama_8bit_kernel(
-   MatrixView_q8_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;
--  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
-+  int offset_k = BLOCK_KN_SIZE * blockIdx.y;
-+  int offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
- 
-   int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
- 
-   // Preload remapping table
-   __shared__ int perm[BLOCK_KN_SIZE];
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   if (b_q_perm) {
-     if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];
-@@ -862,14 +862,14 @@ __global__ void reconstruct_exllama_4bit_kernel(
-   MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;
--  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
-+  int offset_k = BLOCK_KN_SIZE * blockIdx.y;
-+  int offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
- 
-   int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
- 
-   // Preload remapping table
-   __shared__ int perm[BLOCK_KN_SIZE];
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   if (b_q_perm) {
-     if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];
-@@ -967,14 +967,14 @@ __global__ void reconstruct_exllama_3bit_kernel(
-   MatrixView_q3_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;
--  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
-+  int offset_k = BLOCK_KN_SIZE * blockIdx.y;
-+  int offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
- 
-   int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
- 
-   // Preload remapping table
-   __shared__ int perm[BLOCK_KN_SIZE];
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   if (b_q_perm) {
-     if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];
-@@ -1065,14 +1065,14 @@ __global__ void reconstruct_exllama_2bit_kernel(
-   MatrixView_q2_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
-   MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
- 
--  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;
--  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
-+  int offset_k = BLOCK_KN_SIZE * blockIdx.y;
-+  int offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;
- 
-   int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
- 
-   // Preload remapping table
-   __shared__ int perm[BLOCK_KN_SIZE];
--  auto t = threadIdx.x;
-+  int t = threadIdx.x;
- 
-   if (b_q_perm) {
-     if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];
-@@ -1181,11 +1181,11 @@ __global__ void gemm_half_q_half_alt_4bit_kernel(
-   int zero_width = width / 8;
-   int vec_height = height * 4;
-   const int blockwidth2 = BLOCK_KN_SIZE / 2;
--  auto b = blockIdx.y * BLOCK_M_SIZE_MAX;
-+  int b = blockIdx.y * BLOCK_M_SIZE_MAX;
-   int b_end = min(BLOCK_M_SIZE_MAX, batch - b);
--  auto h = BLOCK_KN_SIZE * blockIdx.z / 8;
-+  int h = BLOCK_KN_SIZE * blockIdx.z / 8;
-   int h_end = min(BLOCK_KN_SIZE / 8, height - h) * 4;
--  auto w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
-+  int w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
- 
-   __shared__ half2 blockvec[BLOCK_M_SIZE_MAX][blockwidth2];
-   if (threadIdx.x < h_end) {
-@@ -1197,8 +1197,8 @@ __global__ void gemm_half_q_half_alt_4bit_kernel(
-   }
- 
-   __shared__ half2 deq2[256][8];
--  auto val = threadIdx.x / 8;
--  auto off = threadIdx.x % 8;
-+  int val = threadIdx.x / 8;
-+  int off = threadIdx.x % 8;
-   for (; val < 256; val += BLOCK_KN_SIZE / 8) {
-     deq2[val][off] =
-         __halves2half2(__int2half_rn(val & 0xF), __int2half_rn(val >> 4));
-@@ -1280,11 +1280,11 @@ __global__ void gemm_half_q_half_alt_8bit_kernel(
-   int zero_width = width / 4;
-   int vec_height = height * 2;
-   const int blockwidth2 = BLOCK_KN_SIZE / 2;
--  auto b = blockIdx.y * BLOCK_M_SIZE_MAX;
-+  int b = blockIdx.y * BLOCK_M_SIZE_MAX;
-   int b_end = min(BLOCK_M_SIZE_MAX, batch - b);
--  auto h = BLOCK_KN_SIZE * blockIdx.z / 4;
-+  int h = BLOCK_KN_SIZE * blockIdx.z / 4;
-   int h_end = min(BLOCK_KN_SIZE / 4, height - h) * 2;
--  auto w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
-+  int w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
- 
-   __shared__ half2 blockvec[BLOCK_M_SIZE_MAX][blockwidth2];
-   if (threadIdx.x < h_end) {
-@@ -1393,8 +1393,8 @@ __global__ void reconstruct_gptq_kernel(const uint32_t* __restrict__ w,
-                                         half* __restrict__ out) {
-   // Start of block
- 
--  auto column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
--  auto row = blockIdx.y * 32 / bit;
-+  int column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
-+  int row = blockIdx.y * 32 / bit;
-   if (column >= width) return;
- 
-   // Views
-@@ -1425,8 +1425,8 @@ __global__ void reconstruct_gptq_3bit_kernel(
-     const int height, const int width, const int group,
-     half* __restrict__ out) {
-   // Start of block
--  auto column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
--  auto row = blockIdx.y * 32;
-+  int column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;
-+  int row = blockIdx.y * 32;
-   if (column >= width) return;
- 
-   // Views
-@@ -1542,7 +1542,7 @@ void gemm_half_q_half_cuda(cublasHandle_t cublas_handle, const half* a,
- 
- __global__ void shuffle_4bit_kernel(uint32_t* __restrict__ b_q_weight,
-                                     const int size_k, const int size_n) {
--  auto n = blockIdx.x * THREADS_X + threadIdx.x;
-+  int n = blockIdx.x * THREADS_X + threadIdx.x;
-   if (n >= size_n) return;
-   int k = 0;
-   uint32_t* b_ptr = b_q_weight + n;
-@@ -1555,7 +1555,7 @@ __global__ void shuffle_4bit_kernel(uint32_t* __restrict__ b_q_weight,
- 
- __global__ void shuffle_8bit_kernel(uint32_t* __restrict__ b_q_weight,
-                                     const int size_k, const int size_n) {
--  auto n = blockIdx.x * THREADS_X + threadIdx.x;
-+  int n = blockIdx.x * THREADS_X + threadIdx.x;
-   if (n >= size_n) return;
-   int k = 0;
-   uint32_t* b_ptr = b_q_weight + n;
-@@ -1568,7 +1568,7 @@ __global__ void shuffle_8bit_kernel(uint32_t* __restrict__ b_q_weight,
- 
- __global__ void shuffle_2bit_kernel(uint32_t* __restrict__ b_q_weight,
-                                     const int size_k, const int size_n) {
--  auto n = blockIdx.x * THREADS_X + threadIdx.x;
-+  int n = blockIdx.x * THREADS_X + threadIdx.x;
-   if (n >= size_n) return;
-   int k = 0;
-   uint32_t* b_ptr = b_q_weight + n;
-@@ -1581,7 +1581,7 @@ __global__ void shuffle_2bit_kernel(uint32_t* __restrict__ b_q_weight,
- 
- __global__ void shuffle_3bit_kernel(uint32_t* __restrict__ b_q_weight,
-                                     const int size_k, const int size_n) {
--  auto n = blockIdx.x * THREADS_X + threadIdx.x;
-+  int n = blockIdx.x * THREADS_X + threadIdx.x;
-   if (n >= size_n) return;
-   int k = 0;
-   uint32_t* b_ptr = b_q_weight + n;
-@@ -1599,9 +1599,9 @@ __global__ void make_sequential_4bit_kernel(const uint32_t* __restrict__ w,
-   const uint64_t* w2 = (uint64_t*)w;
-   uint64_t* w_new2 = (uint64_t*)w_new;
-   int w2_stride = w_width >> 1;
--  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-+  int w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-   if (w2_column >= w2_stride) return;
--  auto w_new2_row = blockIdx.y;
-+  int w_new2_row = blockIdx.y;
-   int q_perm_idx = w_new2_row << 3;
-   uint64_t dst = 0;
- 
-@@ -1630,9 +1630,9 @@ __global__ void make_sequential_2bit_kernel(const uint32_t* __restrict__ w,
-   const uint64_t* w2 = (uint64_t*)w;
-   uint64_t* w_new2 = (uint64_t*)w_new;
-   int w2_stride = w_width >> 1;
--  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-+  int w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-   if (w2_column >= w2_stride) return;
--  auto w_new2_row = blockIdx.y;
-+  int w_new2_row = blockIdx.y;
-   int q_perm_idx = w_new2_row << 4;
-   uint64_t dst = 0;
- 
-@@ -1658,10 +1658,10 @@ __global__ void make_sequential_3bit_kernel(const uint32_t* __restrict__ w,
-                                             uint32_t* __restrict__ w_new,
-                                             const int* __restrict__ q_perm,
-                                             const int w_width) {
--  auto w_column = THREADS_X * blockIdx.x + threadIdx.x;
-+  int w_column = THREADS_X * blockIdx.x + threadIdx.x;
-   if (w_column >= w_width) return;
--  auto w_new_row = blockIdx.y * 3;
--  auto q_perm_idx = blockIdx.y << 5;
-+  int w_new_row = blockIdx.y * 3;
-+  int q_perm_idx = blockIdx.y << 5;
-   uint32_t dst[3] = {0, 0, 0};
- 
- #pragma unroll
-@@ -1744,9 +1744,9 @@ __global__ void make_sequential_8bit_kernel(const uint32_t* __restrict__ w,
-   const uint64_t* w2 = (uint64_t*)w;
-   uint64_t* w_new2 = (uint64_t*)w_new;
-   int w2_stride = w_width >> 1;
--  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-+  int w2_column = THREADS_X * blockIdx.x + threadIdx.x;
-   if (w2_column >= w2_stride) return;
--  auto w_new2_row = blockIdx.y;
-+  int w_new2_row = blockIdx.y;
-   int q_perm_idx = w_new2_row << 2;
-   uint64_t dst = 0;
- 
-diff --git a/csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu b/csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
-index ec0bf2c3..b520f8c3 100644
---- a/csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
-+++ b/csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
-@@ -55,11 +55,11 @@ struct GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-     this_block_B_base_ptr = params.B_ptr + blockIdx.y * Ntile * params.K +
-                             blockIdx.z * params.SplitK * 4;
- 
--    const auto lane_id = threadIdx.x % WARP_SIZE;
-+    const int lane_id = threadIdx.x % WARP_SIZE;
- 
-     // For matrix A, a block load/store Mtile(row) x 32(col) elements in
-     // multiple iters, 8x4 warp load/store 8(row) x 32(col) elements per iter
--    const auto Aldg_row_base_idx = threadIdx.x / 4;
-+    const int Aldg_row_base_idx = threadIdx.x / 4;
-     Aldg_col_idx = (threadIdx.x % 4) * LDG_ELEMENT_CNT_A;
-     const int Aldg_base_offset = Aldg_row_base_idx * params.K + Aldg_col_idx;
- 
-@@ -67,7 +67,7 @@ struct GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-     // elements of N32K16 packing in multiple iters, 4x8 warp load/store 4(row)
-     // * 128(col) per iter
-     Bldg_col_idx = (threadIdx.x % 8) * LDG_ELEMENT_CNT_B;
--    const auto Bldg_row_base_idx = threadIdx.x / 8;
-+    const int Bldg_row_base_idx = threadIdx.x / 8;
-     const int Bldg_base_offset =
-         Bldg_row_base_idx * params.K * 4 + Bldg_col_idx;
- 
-@@ -89,7 +89,7 @@ struct GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-     B_ldg_guard = 0;
-   #pragma unroll
-     for (int i = 0; i < (Mtile + M_SIZE_ONE_LOAD - 1) / M_SIZE_ONE_LOAD; ++i) {
--      auto m_idx = blockIdx.x * Mtile + Aldg_row_base_idx + i * M_SIZE_ONE_LOAD;
-+      int m_idx = blockIdx.x * Mtile + Aldg_row_base_idx + i * M_SIZE_ONE_LOAD;
-       if (m_idx < params.M) {
-         A_ldg_guard |= (1u << i);
-       }
-@@ -98,8 +98,8 @@ struct GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-     const int N_padded = (params.N + 31) / 32 * 32;
-   #pragma unroll
-     for (int i = 0; i < (Ntile + N_SIZE_ONE_LOAD - 1) / N_SIZE_ONE_LOAD; ++i) {
--      auto n_idx = blockIdx.y * Ntile + (Bldg_row_base_idx / 8) * 32 +
--                   i * N_SIZE_ONE_LOAD;
-+      int n_idx = blockIdx.y * Ntile + (Bldg_row_base_idx / 8) * 32 +
-+                  i * N_SIZE_ONE_LOAD;
-       if (n_idx < N_padded) {
-         B_ldg_guard |= (1u << i);
-       }
-@@ -355,7 +355,7 @@ struct ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-   __device__ void fused_splitk_reduce() {
-     // need splitk-reduce if enable splitk
-     if (gridDim.z > 1) {
--      auto blk_red_idx = blockIdx.x * gridDim.y + blockIdx.y;
-+      int blk_red_idx = blockIdx.x * gridDim.y + blockIdx.y;
-       // Wait for all previous blocks in the splitk direction to accumulate the
-       // results into C_tmp
-       if (threadIdx.x == 0) {
-@@ -371,7 +371,7 @@ struct ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
-       }
-       __syncthreads();
- 
--      auto C_tmp_base_offset = blk_red_idx * Mtile * Ntile + threadIdx.x * 4;
-+      int C_tmp_base_offset = blk_red_idx * Mtile * Ntile + threadIdx.x * 4;
-       if (blockIdx.z != 0) {
-         // expecting that temporary register here reuses the previous A&B frag
-         // register
-@@ -456,7 +456,7 @@ struct ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {
- 
-     FType* C_base_ptr = this_block_C_base_ptr + store_c_base_offset;
-     // C_tile lds and stg
--    auto m_base_idx = store_c_row_base_idx + blockIdx.x * Mtile;
-+    int m_base_idx = store_c_row_base_idx + blockIdx.x * Mtile;
-     bool n_guard = (store_c_col_idx + blockIdx.y * Ntile) < params.N;
-     if (WARP_NTILE == 32) {
-       int lds_c_base_offset = warp_id * Mtile * WARP_NTILE +
-@@ -580,9 +580,9 @@ __global__ void __launch_bounds__(BLOCK)
-   int sts_stage_idx = 0;
-   int lds_stage_idx = 0;
- 
--  auto tb_k_slice = blockIdx.z * params.SplitK + params.SplitK <= params.K
--                        ? params.SplitK
--                        : params.K - blockIdx.z * params.SplitK;
-+  int tb_k_slice = blockIdx.z * params.SplitK + params.SplitK <= params.K
-+                       ? params.SplitK
-+                       : params.K - blockIdx.z * params.SplitK;
-   int k_tiles = (tb_k_slice + 31) / 32;
-   int first_k_tile = tb_k_slice - (k_tiles - 1) * 32;
- 
-@@ -777,13 +777,13 @@ __global__ void restore_N32_K16_dequantize_rhs_w8a16_perc_kernel(
-     const QT* qdata, const FT* scales, const FT* zeros, FT* fdata,
-     const int N_32align, const int N, const int K) {
-   __shared__ FT smem[64 * 32];
--  auto warp_id = threadIdx.x / 32;
--  auto lane_id = threadIdx.x % 32;
--  const auto src_row_idx = blockIdx.x * 8 + lane_id / 4;
-+  int warp_id = threadIdx.x / 32;
-+  int lane_id = threadIdx.x % 32;
-+  const int src_row_idx = blockIdx.x * 8 + lane_id / 4;
-   const int src_col_idx =
-       blockIdx.y * 64 * 4 + warp_id * 16 * 4 + (lane_id % 4) * 16;
-   const int src_offset = src_row_idx * K * 4 + src_col_idx;
--  auto params_nidx = blockIdx.x * 32 + (lane_id / 4) * 4;
-+  int params_nidx = blockIdx.x * 32 + (lane_id / 4) * 4;
- 
-   QT qval_reg[16];
-   const QT* pdata = qdata + src_offset;
-@@ -829,8 +829,8 @@ __global__ void restore_N32_K16_dequantize_rhs_w8a16_perc_kernel(
-         *reinterpret_cast<uint4*>(smem + lds_base_offset + i * 32 * 32);
-   }
- 
--  const auto dst_row_base_kidx = blockIdx.y * 64 + threadIdx.x / 4;
--  const auto dst_col_nidx = blockIdx.x * 32 + (threadIdx.x % 4) * 8;
-+  const int dst_row_base_kidx = blockIdx.y * 64 + threadIdx.x / 4;
-+  const int dst_col_nidx = blockIdx.x * 32 + (threadIdx.x % 4) * 8;
-   #pragma unroll
-   for (int i = 0; i < 2; ++i) {
-     int dst_row_kidx = dst_row_base_kidx + i * 32;
-@@ -1008,4 +1008,4 @@ torch::Tensor allspark_w8a16_gemm(
- 
- TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {
-   m.impl("allspark_w8a16_gemm", &allspark_w8a16_gemm);
--}
-+}
-\ No newline at end of file
-diff --git a/csrc/quantization/gptq_allspark/allspark_repack.cu b/csrc/quantization/gptq_allspark/allspark_repack.cu
-index ea8eccf0..82929c94 100644
---- a/csrc/quantization/gptq_allspark/allspark_repack.cu
-+++ b/csrc/quantization/gptq_allspark/allspark_repack.cu
-@@ -13,8 +13,8 @@ __global__ void __launch_bounds__(128)
-         const uint8_t* B, const FType* B_scale, const FType* B_zero,
-         uint8_t* B_result, FType* B_scale_result, FType* B_zero_result,
-         const int K, const int N, const int N_32align) {
--  const auto lane_id = threadIdx.x % 32;
--  const auto warp_id = threadIdx.x / 32;
-+  const int lane_id = threadIdx.x % 32;
-+  const int warp_id = threadIdx.x / 32;
- 
-   if (blockIdx.x != gridDim.x - 1) {
-     // Load B
-@@ -50,7 +50,7 @@ __global__ void __launch_bounds__(128)
-     }
- 
-     // Store B
--    const auto dst_row_base_idx = blockIdx.y * (128 / 4) + (lane_id / 8) * 8;
-+    const int dst_row_base_idx = blockIdx.y * (128 / 4) + (lane_id / 8) * 8;
-     const int dst_col_idx =
-         blockIdx.x * (64 * 4) + warp_id * 64 + (lane_id % 8) * 8;
-     for (int i = 0; i < 8; ++i) {
-@@ -65,7 +65,7 @@ __global__ void __launch_bounds__(128)
-   } else {
-     // Load B_scale and B_zero
-     FType b_scale_reg, b_zero_reg;
--    auto src_offset = blockIdx.y * 128 + threadIdx.x;
-+    int src_offset = blockIdx.y * 128 + threadIdx.x;
-     ldg16_cg_0(b_scale_reg, B_scale + src_offset, src_offset < N);
-     if (B_zero != nullptr)
-       ldg16_cg_0(b_zero_reg, B_zero + src_offset, src_offset < N);
-diff --git a/csrc/quantization/gptq_allspark/allspark_utils.cuh b/csrc/quantization/gptq_allspark/allspark_utils.cuh
-index 83141301..80456c25 100644
---- a/csrc/quantization/gptq_allspark/allspark_utils.cuh
-+++ b/csrc/quantization/gptq_allspark/allspark_utils.cuh
-@@ -62,7 +62,7 @@ template <typename FType, int BLOCK, int N_MATRIX>
- __global__ void f16_gemm_splitk_reduce_kernel(const FType* C_split, FType* C,
-                                               uint32_t n, uint32_t n_matrix,
-                                               uint32_t matrix_size) {
--  auto idx = blockIdx.x * BLOCK + threadIdx.x;
-+  int idx = blockIdx.x * BLOCK + threadIdx.x;
- 
-   if (idx >= matrix_size) {
-     return;
-@@ -407,4 +407,4 @@ static __device__ half2 inline num2num2(const half x) {
-   return __half2half2(x);
- }
- 
--}  // namespace allspark
-+}  // namespace allspark
-\ No newline at end of file
-diff --git a/docs/source/deployment/frameworks/lws.md b/docs/source/deployment/frameworks/lws.md
-index 4e9a03b5..349fa83f 100644
---- a/docs/source/deployment/frameworks/lws.md
-+++ b/docs/source/deployment/frameworks/lws.md
-@@ -7,192 +7,5 @@ A major use case is for multi-host/multi-node distributed inference.
- 
- vLLM can be deployed with [LWS](https://github.com/kubernetes-sigs/lws) on Kubernetes for distributed model serving.
- 
--## Prerequisites
--
--* At least two Kubernetes nodes, each with 8 GPUs, are required.
--* Install LWS by following the instructions found [here](https://lws.sigs.k8s.io/docs/installation/).
--
--## Deploy and Serve
--
--Deploy the following yaml file `lws.yaml`
--
--```yaml
--apiVersion: leaderworkerset.x-k8s.io/v1
--kind: LeaderWorkerSet
--metadata:
--  name: vllm
--spec:
--  replicas: 2
--  leaderWorkerTemplate:
--    size: 2
--    restartPolicy: RecreateGroupOnPodRestart
--    leaderTemplate:
--      metadata:
--        labels:
--          role: leader
--      spec:
--        containers:
--          - name: vllm-leader
--            image: docker.io/vllm/vllm-openai:latest
--            env:
--              - name: HUGGING_FACE_HUB_TOKEN
--                value: <your-hf-token>
--            command:
--              - sh
--              - -c
--              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE); 
--                 python3 -m vllm.entrypoints.openai.api_server --port 8080 --model meta-llama/Meta-Llama-3.1-405B-Instruct --tensor-parallel-size 8 --pipeline_parallel_size 2"
--            resources:
--              limits:
--                nvidia.com/gpu: "8"
--                memory: 1124Gi
--                ephemeral-storage: 800Gi
--              requests:
--                ephemeral-storage: 800Gi
--                cpu: 125
--            ports:
--              - containerPort: 8080
--            readinessProbe:
--              tcpSocket:
--                port: 8080
--              initialDelaySeconds: 15
--              periodSeconds: 10
--            volumeMounts:
--              - mountPath: /dev/shm
--                name: dshm
--        volumes:
--        - name: dshm
--          emptyDir:
--            medium: Memory
--            sizeLimit: 15Gi
--    workerTemplate:
--      spec:
--        containers:
--          - name: vllm-worker
--            image: docker.io/vllm/vllm-openai:latest
--            command:
--              - sh
--              - -c
--              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)"
--            resources:
--              limits:
--                nvidia.com/gpu: "8"
--                memory: 1124Gi
--                ephemeral-storage: 800Gi
--              requests:
--                ephemeral-storage: 800Gi
--                cpu: 125
--            env:
--              - name: HUGGING_FACE_HUB_TOKEN
--                value: <your-hf-token>
--            volumeMounts:
--              - mountPath: /dev/shm
--                name: dshm   
--        volumes:
--        - name: dshm
--          emptyDir:
--            medium: Memory
--            sizeLimit: 15Gi
-----
--apiVersion: v1
--kind: Service
--metadata:
--  name: vllm-leader
--spec:
--  ports:
--    - name: http
--      port: 8080
--      protocol: TCP
--      targetPort: 8080
--  selector:
--    leaderworkerset.sigs.k8s.io/name: vllm
--    role: leader
--  type: ClusterIP
--```
--
--```bash
--kubectl apply -f lws.yaml
--```
--
--Verify the status of the pods:
--
--```bash
--kubectl get pods
--```
--
--Should get an output similar to this:
--
--```bash
--NAME       READY   STATUS    RESTARTS   AGE
--vllm-0     1/1     Running   0          2s
--vllm-0-1   1/1     Running   0          2s
--vllm-1     1/1     Running   0          2s
--vllm-1-1   1/1     Running   0          2s
--```
--
--Verify that the distributed tensor-parallel inference works:
--
--```bash
--kubectl logs vllm-0 |grep -i "Loading model weights took" 
--```
--
--Should get something similar to this:
--
--```text
--INFO 05-08 03:20:24 model_runner.py:173] Loading model weights took 0.1189 GB
--(RayWorkerWrapper pid=169, ip=10.20.0.197) INFO 05-08 03:20:28 model_runner.py:173] Loading model weights took 0.1189 GB
--```
--
--## Access ClusterIP service
--
--```bash
--# Listen on port 8080 locally, forwarding to the targetPort of the service's port 8080 in a pod selected by the service
--kubectl port-forward svc/vllm-leader 8080:8080
--```
--
--The output should be similar to the following:
--
--```text
--Forwarding from 127.0.0.1:8080 -> 8080
--Forwarding from [::1]:8080 -> 8080
--```
--
--## Serve the model
--
--Open another terminal and send a request
--
--```text
--curl http://localhost:8080/v1/completions \
---H "Content-Type: application/json" \
---d '{
--    "model": "meta-llama/Meta-Llama-3.1-405B-Instruct",
--    "prompt": "San Francisco is a",
--    "max_tokens": 7,
--    "temperature": 0
--}'
--```
--
--The output should be similar to the following
--
--```text
--{
--  "id": "cmpl-1bb34faba88b43f9862cfbfb2200949d",
--  "object": "text_completion",
--  "created": 1715138766,
--  "model": "meta-llama/Meta-Llama-3.1-405B-Instruct",
--  "choices": [
--    {
--      "index": 0,
--      "text": " top destination for foodies, with",
--      "logprobs": null,
--      "finish_reason": "length",
--      "stop_reason": null
--    }
--  ],
--  "usage": {
--    "prompt_tokens": 5,
--    "total_tokens": 12,
--    "completion_tokens": 7
--  }
--}
--```
-+Please see [this guide](https://github.com/kubernetes-sigs/lws/tree/main/docs/examples/vllm) for more details on
-+deploying vLLM on Kubernetes using LWS.
-diff --git a/docs/source/features/quantization/bnb.md b/docs/source/features/quantization/bnb.md
-index b81d89c4..7525e8e7 100644
---- a/docs/source/features/quantization/bnb.md
-+++ b/docs/source/features/quantization/bnb.md
-@@ -25,7 +25,7 @@ import torch
- # unsloth/tinyllama-bnb-4bit is a pre-quantized checkpoint.
- model_id = "unsloth/tinyllama-bnb-4bit"
- llm = LLM(model=model_id, dtype=torch.bfloat16, trust_remote_code=True, \
--quantization="bitsandbytes")
-+quantization="bitsandbytes", load_format="bitsandbytes")
- ```
- 
- ## Inflight quantization: load as 4bit quantization
-@@ -35,7 +35,7 @@ from vllm import LLM
- import torch
- model_id = "huggyllama/llama-7b"
- llm = LLM(model=model_id, dtype=torch.bfloat16, trust_remote_code=True, \
--quantization="bitsandbytes")
-+quantization="bitsandbytes", load_format="bitsandbytes")
- ```
- 
- ## OpenAI Compatible Server
-@@ -43,5 +43,5 @@ quantization="bitsandbytes")
- Append the following to your 4bit model arguments:
- 
- ```console
----quantization bitsandbytes
-+--quantization bitsandbytes --load-format bitsandbytes
- ```
-diff --git a/docs/source/serving/distributed_serving.md b/docs/source/serving/distributed_serving.md
-index 591acc2c..b36a3dcb 100644
---- a/docs/source/serving/distributed_serving.md
-+++ b/docs/source/serving/distributed_serving.md
-@@ -83,7 +83,7 @@ Since this is a ray cluster of **containers**, all the following commands should
- 
- Then, on any node, use `docker exec -it node /bin/bash` to enter the container, execute `ray status` and `ray list nodes` to check the status of the Ray cluster. You should see the right number of nodes and GPUs.
- 
--After that, on any node, use `docker exec -it node /bin/bash` to enter the container again. **In the container**, you can use vLLM as usual, just as you have all the GPUs on one node: vLLM will be able to leverage GPU resources of all nodes in the Ray cluster, and therefore, only run the `vllm` command on this node but not other nodes. The common practice is to set the tensor parallel size to the number of GPUs in each node, and the pipeline parallel size to the number of nodes. For example, if you have 16 GPUs in 2 nodes (8 GPUs per node), you can set the tensor parallel size to 8 and the pipeline parallel size to 2:
-+After that, on any node, use `docker exec -it node /bin/bash` to enter the container again. **In the container**, you can use vLLM as usual, just as you have all the GPUs on one node. The common practice is to set the tensor parallel size to the number of GPUs in each node, and the pipeline parallel size to the number of nodes. For example, if you have 16 GPUs in 2 nodes (8 GPUs per node), you can set the tensor parallel size to 8 and the pipeline parallel size to 2:
- 
- ```console
-  vllm serve /path/to/the/model/in/the/container \
-diff --git a/docs/source/serving/openai_compatible_server.md b/docs/source/serving/openai_compatible_server.md
-index a6ec05f4..0880a453 100644
---- a/docs/source/serving/openai_compatible_server.md
-+++ b/docs/source/serving/openai_compatible_server.md
-@@ -29,11 +29,6 @@ completion = client.chat.completions.create(
- print(completion.choices[0].message)
- ```
- 
--:::{tip}
--vLLM supports some parameters that are not supported by OpenAI, `top_k` for example.
--You can pass these parameters to vLLM using the OpenAI client in the `extra_body` parameter of your requests, i.e. `extra_body={"top_k": 50}` for `top_k`.
--:::
--
- ## Supported APIs
- 
- We currently support the following OpenAI APIs:
-diff --git a/examples/offline_inference/lora_with_quantization_inference.py b/examples/offline_inference/lora_with_quantization_inference.py
-index ab235ddd..a4097350 100644
---- a/examples/offline_inference/lora_with_quantization_inference.py
-+++ b/examples/offline_inference/lora_with_quantization_inference.py
-@@ -83,6 +83,7 @@ def initialize_engine(model: str, quantization: str,
-         engine_args = EngineArgs(model=model,
-                                  quantization=quantization,
-                                  qlora_adapter_name_or_path=lora_repo,
-+                                 load_format="bitsandbytes",
-                                  enable_lora=True,
-                                  max_lora_rank=64)
-     else:
-diff --git a/examples/offline_inference/mistral-small.py b/examples/offline_inference/pixtral.py
-similarity index 92%
-rename from examples/offline_inference/mistral-small.py
-rename to examples/offline_inference/pixtral.py
-index 43be2aa8..5379f456 100644
---- a/examples/offline_inference/mistral-small.py
-+++ b/examples/offline_inference/pixtral.py
-@@ -6,16 +6,14 @@ import argparse
- from vllm import LLM
- from vllm.sampling_params import SamplingParams
- 
--# This script is an offline demo for running Mistral-Small-3.1
-+# This script is an offline demo for running Mistral-Small-3
- #
- # If you want to run a server/client setup, please follow this code:
- #
- # - Server:
- #
- # ```bash
--# vllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 \
--#   --tokenizer-mode mistral --config-format mistral --load-format mistral \
--#   --limit-mm-per-prompt 'image=4' --max-model-len 16384
-+# vllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer-mode mistral --limit-mm-per-prompt 'image=4' --max-model-len 16384
- # ```
- #
- # - Client:
-@@ -53,8 +51,6 @@ def run_simple_demo(args: argparse.Namespace):
-     llm = LLM(
-         model=model_name,
-         tokenizer_mode="mistral",
--        config_format="mistral",
--        load_format="mistral",
-         max_model_len=4096,
-         max_num_seqs=2,
-         disable_mm_preprocessor_cache=args.disable_mm_preprocessor_cache,
-@@ -95,8 +91,6 @@ def run_advanced_demo(args: argparse.Namespace):
-     llm = LLM(
-         model=model_name,
-         tokenizer_mode="mistral",
--        config_format="mistral",
--        load_format="mistral",
-         limit_mm_per_prompt={"image": max_img_per_msg},
-         max_model_len=max_img_per_msg * max_tokens_per_img,
-         disable_mm_preprocessor_cache=args.disable_mm_preprocessor_cache,
-diff --git a/examples/offline_inference/reproduciblity.py b/examples/offline_inference/reproduciblity.py
-deleted file mode 100644
-index d0197bf6..00000000
---- a/examples/offline_inference/reproduciblity.py
-+++ /dev/null
-@@ -1,36 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--import os
--
--from vllm import LLM, SamplingParams
--
--# vLLM does not guarantee the reproducibility of the results by default,
--# for the sake of performance. You need to do the following to achieve
--# reproducible results:
--# 1. Turn off multiprocessing to make the scheduling deterministic.
--#    NOTE(woosuk): This is not needed and will be ignored for V0.
--os.environ["VLLM_ENABLE_V1_MULTIPROCESSING"] = "0"
--# 2. Fix the global seed for reproducibility. The default seed is None, which is
--# not reproducible.
--SEED = 42
--
--# NOTE(woosuk): Even with the above two settings, vLLM only provides
--# reproducibility when it runs on the same hardware and the same vLLM version.
--# Also, the online serving API (`vllm serve`) does not support reproducibility
--# because it is almost impossible to make the scheduling deterministic in the
--# online serving setting.
--
--llm = LLM(model="facebook/opt-125m", seed=SEED)
--
--prompts = [
--    "Hello, my name is",
--    "The president of the United States is",
--    "The capital of France is",
--    "The future of AI is",
--]
--sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
--
--outputs = llm.generate(prompts, sampling_params)
--for output in outputs:
--    prompt = output.prompt
--    generated_text = output.outputs[0].text
--    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
-diff --git a/examples/offline_inference/vision_language.py b/examples/offline_inference/vision_language.py
-index 1cc25627..3849bd37 100644
---- a/examples/offline_inference/vision_language.py
-+++ b/examples/offline_inference/vision_language.py
-@@ -169,6 +169,7 @@ def run_gemma3(questions: list[str], modality: str) -> ModelRequestData:
-         model=model_name,
-         max_model_len=2048,
-         max_num_seqs=2,
-+        # Default is False; setting it to True is not supported in V1 yet
-         mm_processor_kwargs={"do_pan_and_scan": True},
-         disable_mm_preprocessor_cache=args.disable_mm_preprocessor_cache,
-     )
-diff --git a/examples/offline_inference/vision_language_multi_image.py b/examples/offline_inference/vision_language_multi_image.py
-index 98a73916..3a17e5ba 100644
---- a/examples/offline_inference/vision_language_multi_image.py
-+++ b/examples/offline_inference/vision_language_multi_image.py
-@@ -91,6 +91,8 @@ def load_gemma3(question: str, image_urls: list[str]) -> ModelRequestData:
-         model=model_name,
-         max_model_len=8192,
-         max_num_seqs=2,
-+        # Default is False; setting it to True is not supported in V1 yet
-+        mm_processor_kwargs={"do_pan_and_scan": True},
-         limit_mm_per_prompt={"image": len(image_urls)},
-     )
- 
-diff --git a/examples/online_serving/disaggregated_prefill.sh b/examples/online_serving/disaggregated_prefill.sh
-index 6925dc8a..2bb2824c 100644
---- a/examples/online_serving/disaggregated_prefill.sh
-+++ b/examples/online_serving/disaggregated_prefill.sh
-@@ -8,9 +8,6 @@ set -xe
- echo " Warning: The usage of disaggregated prefill is experimental and subject to change "
- sleep 1
- 
--# meta-llama/Meta-Llama-3.1-8B-Instruct or deepseek-ai/DeepSeek-V2-Lite
--MODEL_NAME=${HF_MODEL_NAME:-meta-llama/Meta-Llama-3.1-8B-Instruct}
--
- # Trap the SIGINT signal (triggered by Ctrl+C)
- trap 'cleanup' INT
- 
-@@ -47,20 +44,18 @@ wait_for_server() {
- # You can also adjust --kv-ip and --kv-port for distributed inference.
- 
- # prefilling instance, which is the KV producer
--CUDA_VISIBLE_DEVICES=0 vllm serve $MODEL_NAME \
-+CUDA_VISIBLE_DEVICES=0 vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct \
-     --port 8100 \
-     --max-model-len 100 \
-     --gpu-memory-utilization 0.8 \
--    --trust-remote-code \
-     --kv-transfer-config \
-     '{"kv_connector":"PyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2}' &
- 
- # decoding instance, which is the KV consumer
--CUDA_VISIBLE_DEVICES=1 vllm serve $MODEL_NAME \
-+CUDA_VISIBLE_DEVICES=1 vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct \
-     --port 8200 \
-     --max-model-len 100 \
-     --gpu-memory-utilization 0.8 \
--    --trust-remote-code \
-     --kv-transfer-config \
-     '{"kv_connector":"PyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2}' &
- 
-@@ -83,7 +78,7 @@ sleep 1
- output1=$(curl -X POST -s http://localhost:8000/v1/completions \
- -H "Content-Type: application/json" \
- -d '{
--"model": "'"$MODEL_NAME"'",
-+"model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
- "prompt": "San Francisco is a",
- "max_tokens": 10,
- "temperature": 0
-@@ -92,7 +87,7 @@ output1=$(curl -X POST -s http://localhost:8000/v1/completions \
- output2=$(curl -X POST -s http://localhost:8000/v1/completions \
- -H "Content-Type: application/json" \
- -d '{
--"model": "'"$MODEL_NAME"'",
-+"model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
- "prompt": "Santa Clara is a",
- "max_tokens": 10,
- "temperature": 0
-diff --git a/requirements/common.txt b/requirements/common.txt
-index 2d52858a..d08ef253 100644
---- a/requirements/common.txt
-+++ b/requirements/common.txt
-@@ -18,7 +18,6 @@ pillow  # Required for image processing
- prometheus-fastapi-instrumentator >= 7.0.0
- tiktoken >= 0.6.0  # Required for DBRX tokenizer
- lm-format-enforcer >= 0.10.11, < 0.11
--llguidance >= 0.7.2, < 0.8.0; platform_machine == "x86_64" or platform_machine == "arm64" or platform_machine == "aarch64"
- outlines == 0.1.11
- lark == 1.2.2
- xgrammar == 0.1.16; platform_machine == "x86_64" or platform_machine == "aarch64"
-diff --git a/requirements/cpu.txt b/requirements/cpu.txt
-index e4a7f9ac..b4e6abb6 100644
---- a/requirements/cpu.txt
-+++ b/requirements/cpu.txt
-@@ -3,8 +3,7 @@
- 
- # Dependencies for CPUs
- torch==2.6.0+cpu; platform_machine == "x86_64"
--torch==2.6.0; platform_system == "Darwin"
--torch==2.5.1; platform_machine == "ppc64le" or platform_machine == "aarch64"
-+torch==2.5.1; platform_machine == "ppc64le" or platform_machine == "aarch64" or platform_system == "Darwin"
- torch==2.7.0.dev20250304; platform_machine == "s390x"
- 
- # required for the image processor of minicpm-o-2_6, this must be updated alongside torch
-diff --git a/requirements/rocm-build.txt b/requirements/rocm-build.txt
-index 6af78da4..a0731c51 100644
---- a/requirements/rocm-build.txt
-+++ b/requirements/rocm-build.txt
-@@ -1,10 +1,10 @@
- # Common dependencies
- -r common.txt
- 
----extra-index-url https://download.pytorch.org/whl/rocm6.2.4
--torch==2.6.0
--torchvision==0.21.0
--torchaudio==2.6.0
-+--extra-index-url https://download.pytorch.org/whl/rocm6.2
-+torch==2.5.1
-+torchvision==0.20.1
-+torchaudio==2.5.1
- 
- cmake>=3.26
- packaging
-diff --git a/requirements/test.in b/requirements/test.in
-index e75f15c0..faa4564e 100644
---- a/requirements/test.in
-+++ b/requirements/test.in
-@@ -30,7 +30,7 @@ matplotlib # required for qwen-vl test
- mistral_common[opencv] >= 1.5.4 # required for pixtral test
- datamodel_code_generator # required for minicpm3 test
- lm-eval[api]==0.4.4 # required for model evaluation test
--transformers==4.48.2
-+transformers==4.48.2 
- # quantization
- bitsandbytes>=0.45.3
- buildkite-test-collector==0.1.9
-diff --git a/requirements/tpu.txt b/requirements/tpu.txt
-index 35d5db6c..7246fc19 100644
---- a/requirements/tpu.txt
-+++ b/requirements/tpu.txt
-@@ -17,9 +17,9 @@ ray[data]
- --find-links https://storage.googleapis.com/libtpu-releases/index.html
- --find-links https://storage.googleapis.com/jax-releases/jax_nightly_releases.html
- --find-links https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html
--torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250319-cp39-cp39-linux_x86_64.whl ; python_version == "3.9"
--torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250319-cp310-cp310-linux_x86_64.whl ; python_version == "3.10"
--torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250319-cp311-cp311-linux_x86_64.whl ; python_version == "3.11"
--torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250319-cp39-cp39-linux_x86_64.whl ; python_version == "3.9"
--torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250319-cp310-cp310-linux_x86_64.whl ; python_version == "3.10"
--torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250319-cp311-cp311-linux_x86_64.whl ; python_version == "3.11"
-+torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250314%2Bcxx11-cp39-cp39-linux_x86_64.whl ; python_version == "3.9"
-+torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250314%2Bcxx11-cp310-cp310-linux_x86_64.whl ; python_version == "3.10"
-+torch @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.8.0.dev20250314%2Bcxx11-cp311-cp311-linux_x86_64.whl ; python_version == "3.11"
-+torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250314%2Bcxx11-cp39-cp39-linux_x86_64.whl ; python_version == "3.9"
-+torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250314%2Bcxx11-cp310-cp310-linux_x86_64.whl ; python_version == "3.10"
-+torch_xla[tpu, pallas] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.8.0.dev20250314%2Bcxx11-cp311-cp311-linux_x86_64.whl ; python_version == "3.11"
-diff --git a/tests/compile/test_pass_manager.py b/tests/compile/test_pass_manager.py
-index bdbd104f..70920ab1 100644
---- a/tests/compile/test_pass_manager.py
-+++ b/tests/compile/test_pass_manager.py
-@@ -4,38 +4,34 @@ import pickle
- 
- import pytest
- import torch
-+from torch._inductor.codecache import BypassFxGraphCache
- 
--from vllm.compilation.inductor_pass import CallableInductorPass, InductorPass
-+from vllm.compilation.config import CompilationConfig
-+from vllm.compilation.inductor_pass import (CallableInductorPass,
-+                                            as_inductor_pass)
- from vllm.compilation.pass_manager import PostGradPassManager
--from vllm.config import CompilationConfig
- 
- 
- def simple_callable(graph: torch.fx.Graph):
-     pass
- 
- 
--callable_uuid = CallableInductorPass(simple_callable,
--                                     InductorPass.hash_source(__file__))
-+@as_inductor_pass(files=(__file__, ))
-+def callable_decorated(graph: torch.fx.Graph):
-+    pass
- 
- 
- @pytest.mark.parametrize(
-     "works, callable",
--    [
--        (False, simple_callable),
--        (True, callable_uuid),
--        (True, CallableInductorPass(simple_callable)),
--    ],
--)
-+    [(False, simple_callable), (True, callable_decorated),
-+     (True, CallableInductorPass(simple_callable, "simple_callable"))])
- def test_pass_manager(works: bool, callable):
-     config = CompilationConfig().pass_config
-+    pass_manager = PostGradPassManager([callable])
-+    pass_manager.configure(config)  # Adds default passes
- 
--    pass_manager = PostGradPassManager()
--    pass_manager.configure(config)
--
--    # Try to add the callable to the pass manager
-     if works:
--        pass_manager.add(callable)
-         pickle.dumps(pass_manager)
-     else:
--        with pytest.raises(AssertionError):
--            pass_manager.add(callable)
-+        with pytest.raises(BypassFxGraphCache):
-+            pickle.dumps(pass_manager)
-diff --git a/tests/distributed/test_torchrun_example.py b/tests/distributed/test_torchrun_example.py
-index 0420a645..4ef33932 100644
---- a/tests/distributed/test_torchrun_example.py
-+++ b/tests/distributed/test_torchrun_example.py
-@@ -9,8 +9,6 @@ import torch.distributed as dist
- from vllm import LLM, SamplingParams
- from vllm.distributed.parallel_state import get_world_group
- 
--dist.init_process_group(backend="gloo")
--
- # Create prompts
- prompts = [
-     "Hello, my name is",
-diff --git a/tests/entrypoints/llm/test_guided_generate.py b/tests/entrypoints/llm/test_guided_generate.py
-index 5f1a91cb..97ee027b 100644
---- a/tests/entrypoints/llm/test_guided_generate.py
-+++ b/tests/entrypoints/llm/test_guided_generate.py
-@@ -14,9 +14,7 @@ from vllm.outputs import RequestOutput
- from vllm.sampling_params import GuidedDecodingParams, SamplingParams
- 
- MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
--GUIDED_DECODING_BACKENDS = [
--    "outlines", "lm-format-enforcer", "xgrammar", "guidance"
--]
-+GUIDED_DECODING_BACKENDS = ["outlines", "lm-format-enforcer", "xgrammar"]
- 
- 
- @pytest.fixture(scope="module")
-diff --git a/tests/kernels/test_flash_attn.py b/tests/kernels/test_flash_attn.py
-index 572563c0..95424e25 100644
---- a/tests/kernels/test_flash_attn.py
-+++ b/tests/kernels/test_flash_attn.py
-@@ -15,7 +15,6 @@ NUM_HEADS = [(4, 4), (8, 2), (16, 2)]
- HEAD_SIZES = [128, 256]
- BLOCK_SIZES = [16, 32]
- DTYPES = [torch.float16, torch.bfloat16]
--QDTYPES = [None, torch.float8_e4m3fn]
- # one value large enough to test overflow in index calculation.
- # one value small enough to test the schema op check
- NUM_BLOCKS = [32768, 2048]
-@@ -86,7 +85,6 @@ def ref_paged_attn(
- @pytest.mark.parametrize("num_blocks", NUM_BLOCKS)
- @pytest.mark.parametrize("sliding_window", [None, 256])
- @pytest.mark.parametrize("fa_version", [2, 3])
--@pytest.mark.parametrize("q_dtype", QDTYPES)
- @torch.inference_mode()
- def test_flash_attn_with_paged_kv(
-     use_out: bool,
-@@ -99,15 +97,11 @@ def test_flash_attn_with_paged_kv(
-     num_blocks: int,
-     sliding_window: Optional[int],
-     fa_version: int,
--    q_dtype: Optional[torch.dtype],
- ) -> None:
-     torch.set_default_device("cuda")
-     if not is_fa_version_supported(fa_version):
-         pytest.skip(f"Flash attention version {fa_version} not supported due "
-                     f"to: \"{fa_version_unsupported_reason(fa_version)}\"")
--    if q_dtype is not None and (dtype != torch.bfloat16 or fa_version == 2):
--        pytest.skip("Flash attention with quantized inputs is only "
--                    "supported on version 3 with bfloat16 base type")
- 
-     current_platform.seed_everything(0)
-     num_seqs = len(kv_lens)
-@@ -136,28 +130,10 @@ def test_flash_attn_with_paged_kv(
- 
-     q = query.unsqueeze(1)
-     out = torch.empty_like(q) if use_out else None
--
--    maybe_quantized_query = q
--    maybe_quantized_key_cache = key_cache
--    maybe_quantized_value_cache = value_cache
--    q_descale = None
--    k_descale = None
--    v_descale = None
--    if q_dtype is not None:
--        # QKV are drawn from N(0, 1): no need for a fp8 scaling factor
--        maybe_quantized_query = query.to(q_dtype)
--        maybe_quantized_key_cache = key_cache.to(q_dtype)
--        maybe_quantized_value_cache = value_cache.to(q_dtype)
--
--        scale_shape = (num_seqs, num_kv_heads)
--        q_descale = torch.ones(scale_shape, dtype=torch.float32)
--        k_descale = torch.ones(scale_shape, dtype=torch.float32)
--        v_descale = torch.ones(scale_shape, dtype=torch.float32)
--
-     output = flash_attn_with_kvcache(
--        q=maybe_quantized_query,
--        k_cache=maybe_quantized_key_cache,
--        v_cache=maybe_quantized_value_cache,
-+        q=q,
-+        k_cache=key_cache,
-+        v_cache=value_cache,
-         out=out,
-         softmax_scale=scale,
-         causal=True,
-@@ -166,17 +142,10 @@ def test_flash_attn_with_paged_kv(
-         softcap=soft_cap if soft_cap is not None else 0,
-         window_size=window_size,
-         fa_version=fa_version,
--        q_descale=q_descale,
--        k_descale=k_descale,
--        v_descale=v_descale,
-     )
-     output = output if not use_out else out
-     output = output.squeeze(1)
- 
--    atol, rtol = 1.5e-2, 1e-2
--    if q_dtype is not None:
--        atol, rtol = 1.5e-1, 1.5e-1
--
-     ref_output = ref_paged_attn(query=query,
-                                 key_cache=key_cache,
-                                 value_cache=value_cache,
-@@ -186,7 +155,7 @@ def test_flash_attn_with_paged_kv(
-                                 scale=scale,
-                                 soft_cap=soft_cap,
-                                 sliding_window=sliding_window)
--    torch.testing.assert_close(output, ref_output, atol=atol, rtol=rtol), \
-+    torch.testing.assert_close(output, ref_output, atol=2e-2, rtol=1e-2), \
-         f"{torch.max(torch.abs(output - ref_output))}"
- 
- 
-@@ -202,7 +171,6 @@ def test_flash_attn_with_paged_kv(
- @pytest.mark.parametrize("soft_cap", [None, 10.0, 50.0])
- @pytest.mark.parametrize("num_blocks", NUM_BLOCKS)
- @pytest.mark.parametrize("fa_version", [2, 3])
--@pytest.mark.parametrize("q_dtype", QDTYPES)
- @torch.inference_mode()
- def test_varlen_with_paged_kv(
-     use_out: bool,
-@@ -215,15 +183,11 @@ def test_varlen_with_paged_kv(
-     soft_cap: Optional[float],
-     num_blocks: int,
-     fa_version: int,
--    q_dtype: Optional[torch.dtype],
- ) -> None:
-     torch.set_default_device("cuda")
-     if not is_fa_version_supported(fa_version):
-         pytest.skip(f"Flash attention version {fa_version} not supported due "
-                     f"to: \"{fa_version_unsupported_reason(fa_version)}\"")
--    if q_dtype is not None and (dtype != torch.bfloat16 or fa_version == 2):
--        pytest.skip("Flash attention with quantized inputs is only "
--                    "supported on version 3 with bfloat16 base type")
-     current_platform.seed_everything(0)
-     num_seqs = len(seq_lens)
-     query_lens = [x[0] for x in seq_lens]
-@@ -259,28 +223,10 @@ def test_varlen_with_paged_kv(
-                                  dtype=torch.int32)
- 
-     out = torch.empty_like(query) if use_out else None
--
--    maybe_quantized_query = query
--    maybe_quantized_key_cache = key_cache
--    maybe_quantized_value_cache = value_cache
--    q_descale = None
--    k_descale = None
--    v_descale = None
--    if q_dtype is not None:
--        # QKV are drawn from N(0, 1): no need for a fp8 scaling factor
--        maybe_quantized_query = query.to(q_dtype)
--        maybe_quantized_key_cache = key_cache.to(q_dtype)
--        maybe_quantized_value_cache = value_cache.to(q_dtype)
--
--        scale_shape = (num_seqs, num_kv_heads)
--        q_descale = torch.ones(scale_shape, dtype=torch.float32)
--        k_descale = torch.ones(scale_shape, dtype=torch.float32)
--        v_descale = torch.ones(scale_shape, dtype=torch.float32)
--
-     output = flash_attn_varlen_func(
--        q=maybe_quantized_query,
--        k=maybe_quantized_key_cache,
--        v=maybe_quantized_value_cache,
-+        q=query,
-+        k=key_cache,
-+        v=value_cache,
-         out=out,
-         cu_seqlens_q=cu_query_lens,
-         seqused_k=kv_lens,
-@@ -292,9 +238,6 @@ def test_varlen_with_paged_kv(
-         block_table=block_tables,
-         softcap=soft_cap if soft_cap is not None else 0,
-         fa_version=fa_version,
--        q_descale=q_descale,
--        k_descale=k_descale,
--        v_descale=v_descale,
-     )
-     output = output if not use_out else out
- 
-@@ -309,8 +252,5 @@ def test_varlen_with_paged_kv(
-         sliding_window=sliding_window,
-         soft_cap=soft_cap,
-     )
--    atol, rtol = 1.5e-2, 1e-2
--    if q_dtype is not None:
--        atol, rtol = 1.5e-1, 1.5e-1
--    torch.testing.assert_close(output, ref_output, atol=atol, rtol=rtol), \
-+    torch.testing.assert_close(output, ref_output, atol=2e-2, rtol=1e-2), \
-         f"{torch.max(torch.abs(output - ref_output))}"
-diff --git a/tests/lora/test_add_lora.py b/tests/lora/test_add_lora.py
-index c8b7a5cb..644a075b 100644
---- a/tests/lora/test_add_lora.py
-+++ b/tests/lora/test_add_lora.py
-@@ -1,8 +1,10 @@
- # SPDX-License-Identifier: Apache-2.0
- import asyncio
- import time
-+from pathlib import Path
- 
- import pytest
-+from huggingface_hub import snapshot_download
- 
- import vllm.envs as env
- from vllm.engine.arg_utils import AsyncEngineArgs
-@@ -11,9 +13,35 @@ from vllm.lora.request import LoRARequest
- from vllm.sampling_params import SamplingParams
- from vllm.utils import merge_async_iterators
- 
--MODEL_PATH = "THUDM/chatglm3-6b"
--LORA_RANK = 64
--DEFAULT_MAX_LORAS = 4 * 3
-+MODEL_PATH = "meta-llama/Llama-2-7b-hf"
-+LORA_MODULE_DOWNLOAD_PATH = None  # Populated by download_and_prepare_lora_module() #noqa
-+LORA_RANK = 8
-+DEFAULT_MAX_LORAS = 16 * 3
-+
-+
-+def download_and_prepare_lora_module():
-+    """
-+    Request submission is expensive when the LoRA adapters have their own
-+    tokenizers. This is because, for each request with a new LoRA adapter ID,
-+    the front-end loads the tokenizer from disk.
-+
-+    In this test, as we are comparing request processing times, we want to
-+    minimize any extra activity. To this effect, we download the LoRA
-+    adapter and remove all the tokenizer files, so the engine will default
-+    to the base model tokenizer.
-+    """
-+    global LORA_MODULE_DOWNLOAD_PATH
-+
-+    LORA_MODULE_HF_PATH = "yard1/llama-2-7b-sql-lora-test"
-+    LORA_MODULE_DOWNLOAD_PATH = snapshot_download(repo_id=LORA_MODULE_HF_PATH)
-+
-+    tokenizer_files = [
-+        'added_tokens.json', 'tokenizer_config.json', 'tokenizer.json',
-+        'tokenizer.model'
-+    ]
-+    for tokenizer_file in tokenizer_files:
-+        del_path = Path(LORA_MODULE_DOWNLOAD_PATH) / tokenizer_file
-+        del_path.unlink(missing_ok=True)
- 
- 
- @pytest.fixture(autouse=True)
-@@ -24,9 +52,11 @@ def v1(run_with_both_engines_lora):
-     pass
- 
- 
--def get_lora_requests(lora_path) -> list[LoRARequest]:
-+def get_lora_requests() -> list[LoRARequest]:
-     lora_requests: list[LoRARequest] = [
--        LoRARequest(lora_name=f"{i}", lora_int_id=i, lora_path=lora_path)
-+        LoRARequest(lora_name=f"{i}",
-+                    lora_int_id=i,
-+                    lora_path=LORA_MODULE_DOWNLOAD_PATH)
-         for i in range(1, DEFAULT_MAX_LORAS + 1)
-     ]
-     return lora_requests
-@@ -63,7 +93,7 @@ async def requests_processing_time(llm,
- 
- 
- @pytest.mark.asyncio
--async def test_add_lora(chatglm3_lora_files):
-+async def test_add_lora():
-     """ 
-     The add_lora function is used to pre-load some LoRA adapters into the
-     engine in anticipation of future requests using these adapters. To test
-@@ -73,7 +103,10 @@ async def test_add_lora(chatglm3_lora_files):
-     We measure the request processing time in both cases and expect the time 
-     to be lesser in the case with add_lora() calls.
-     """
--    lora_requests: list[LoRARequest] = get_lora_requests(chatglm3_lora_files)
-+
-+    download_and_prepare_lora_module()
-+
-+    lora_requests: list[LoRARequest] = get_lora_requests()
- 
-     max_loras = len(set([lr.lora_int_id for lr in lora_requests]))
-     # Create engine in eager-mode. Due to high max_loras, the CI can
-@@ -85,7 +118,6 @@ async def test_add_lora(chatglm3_lora_files):
-         max_lora_rank=LORA_RANK,
-         max_model_len=128,
-         gpu_memory_utilization=0.8,  #avoid OOM
--        trust_remote_code=True,
-         enforce_eager=True)
- 
-     # The run_with_both_engines_lora fixture sets up the `VLLM_USE_V1`
-diff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py
-index 59da575e..85a53a17 100644
---- a/tests/model_executor/test_guided_processors.py
-+++ b/tests/model_executor/test_guided_processors.py
-@@ -16,9 +16,7 @@ from vllm.model_executor.guided_decoding.outlines_logits_processors import (
- from vllm.sampling_params import GuidedDecodingParams
- 
- MODEL_NAME = 'HuggingFaceH4/zephyr-7b-beta'
--GUIDED_DECODING_BACKENDS = [
--    "outlines", "lm-format-enforcer", "xgrammar", "guidance"
--]
-+GUIDED_DECODING_BACKENDS = ["outlines", "lm-format-enforcer", "xgrammar"]
- GUIDED_DECODING_BACKENDS_WITH_REASONING_SUPPORT = ["outlines", "xgrammar"]
- REASONING_MODEL_NAME = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
- 
-diff --git a/tests/models/decoder_only/vision_language/test_models.py b/tests/models/decoder_only/vision_language/test_models.py
-index 02351401..5690249e 100644
---- a/tests/models/decoder_only/vision_language/test_models.py
-+++ b/tests/models/decoder_only/vision_language/test_models.py
-@@ -508,19 +508,6 @@ VLM_TEST_SETTINGS = {
-             limit_mm_per_prompt={"image": 4},
-         )],
-     ),
--    # regression test for https://github.com/vllm-project/vllm/issues/15122
--    "qwen2_5_vl-windows-attention": VLMTestInfo(
--        models=["Qwen/Qwen2.5-VL-3B-Instruct"],
--        test_type=VLMTestType.CUSTOM_INPUTS,
--        max_model_len=4096,
--        max_num_seqs=2,
--        auto_cls=AutoModelForVision2Seq,
--        vllm_output_post_proc=model_utils.qwen2_vllm_to_hf_output,
--        custom_test_opts=[CustomTestOptions(
--            inputs=custom_inputs.windows_attention_image_qwen2_5_vl(),
--            limit_mm_per_prompt={"image": 1},
--        )],
--    ),
- }
- # yapf: enable
- 
-diff --git a/tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py b/tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
-index 235618ae..2f03a114 100644
---- a/tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
-+++ b/tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
-@@ -1,11 +1,7 @@
- # SPDX-License-Identifier: Apache-2.0
- """Custom input builders for edge-cases in different models."""
--from io import BytesIO
- from typing import Callable
- 
--import requests
--from PIL import Image
--
- from vllm.multimodal.image import rescale_image_size
- from vllm.multimodal.video import (rescale_video_size, resize_video,
-                                    sample_frames_from_video)
-@@ -106,17 +102,3 @@ def different_patch_input_cases_internvl():
-         build_single_image_inputs(images, formatted_sprompts, wrapped_sf),
-         build_multi_image_inputs([images], formatted_mprompts, wrapped_sf),
-     ]
--
--
--def windows_attention_image_qwen2_5_vl():
--    # image from regression issue: https://github.com/vllm-project/vllm/issues/15122
--    image_url = "https://aomediacodec.github.io/av1-avif/testFiles/Link-U/hato.jpg"
--    image = Image.open(BytesIO(requests.get(image_url).content))
--
--    question = "Describe the image."
--    img_prompt = "<|vision_start|><|image_pad|><|vision_end|>"
--    prompt = (f"<|im_start|>User\n{img_prompt}{question}<|im_end|>\n"
--              "<|im_start|>assistant\n")
--
--    wrapped_sf = ImageSizeWrapper(type=SizeType.SIZE_FACTOR, data=[0.5])
--    return build_single_image_inputs([image], [prompt], wrapped_sf)
-diff --git a/tests/models/encoder_decoder/vision_language/test_mllama.py b/tests/models/encoder_decoder/vision_language/test_mllama.py
-index ae7a7b02..b6ea31cc 100644
---- a/tests/models/encoder_decoder/vision_language/test_mllama.py
-+++ b/tests/models/encoder_decoder/vision_language/test_mllama.py
-@@ -215,6 +215,7 @@ def _run_test(
-                      max_num_seqs=2,
-                      tensor_parallel_size=tensor_parallel_size,
-                      distributed_executor_backend=distributed_executor_backend,
-+                     enforce_eager=True,
-                      limit_mm_per_prompt={"image": _LIMIT_IMAGE_PER_PROMPT
-                                           }) as vllm_model:
-         vllm_outputs_per_image = [
-@@ -424,6 +425,7 @@ def test_bnb_regression(
-         dtype=dtype,
-         max_model_len=4096,
-         max_num_seqs=2,
-+        enforce_eager=True,
-         quantization="bitsandbytes",
-         load_format="bitsandbytes",
-     )
-@@ -479,6 +481,7 @@ def test_explicit_implicit_prompt(
-         max_model_len=4096,
-         max_num_seqs=2,
-         tensor_parallel_size=1,
-+        enforce_eager=True,
-     )
-     sampling_params = SamplingParams(
-         temperature=0,
-@@ -510,6 +513,7 @@ def test_regression(vllm_runner, image_assets, model, dtype, max_tokens,
-             max_model_len=4096,
-             max_num_seqs=2,
-             tensor_parallel_size=1,
-+            enforce_eager=True,
-             limit_mm_per_prompt={"image":
-                                  _LIMIT_IMAGE_PER_PROMPT}) as vllm_model:
- 
-diff --git a/tests/plugins_tests/test_scheduler_plugins.py b/tests/plugins_tests/test_scheduler_plugins.py
-index 4c95a52a..7abf5066 100644
---- a/tests/plugins_tests/test_scheduler_plugins.py
-+++ b/tests/plugins_tests/test_scheduler_plugins.py
-@@ -6,7 +6,7 @@ from vllm.core.scheduler import Scheduler
- from vllm.engine.arg_utils import EngineArgs
- from vllm.engine.llm_engine import LLMEngine
- from vllm.sampling_params import SamplingParams
--from vllm.v1.core.sched.scheduler import Scheduler as V1Scheduler
-+from vllm.v1.core.scheduler import Scheduler as V1Scheduler
- from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine
- 
- 
-diff --git a/tests/v1/core/test_scheduler.py b/tests/v1/core/test_scheduler.py
-index 8916aa58..94133733 100644
---- a/tests/v1/core/test_scheduler.py
-+++ b/tests/v1/core/test_scheduler.py
-@@ -6,8 +6,7 @@ import pytest
- from vllm.config import CacheConfig, ModelConfig, SchedulerConfig, VllmConfig
- from vllm.multimodal.inputs import MultiModalKwargs, PlaceholderRange
- from vllm.sampling_params import SamplingParams
--from vllm.v1.core.sched.output import SchedulerOutput
--from vllm.v1.core.sched.scheduler import Scheduler
-+from vllm.v1.core.scheduler import Scheduler, SchedulerOutput
- from vllm.v1.outputs import ModelRunnerOutput
- from vllm.v1.request import Request, RequestStatus
- from vllm.v1.structured_output import StructuredOutputManager
-diff --git a/tests/v1/engine/test_engine_core.py b/tests/v1/engine/test_engine_core.py
-index ca5ff8fa..afbe15b9 100644
---- a/tests/v1/engine/test_engine_core.py
-+++ b/tests/v1/engine/test_engine_core.py
-@@ -158,22 +158,6 @@ def test_engine_core(monkeypatch: pytest.MonkeyPatch):
-         assert len(engine_core.scheduler.waiting) == 0
-         assert len(engine_core.scheduler.running) == 0
- 
--        # Sending duplicate requests with same request_id
--        req0 = make_request()
--        req1 = make_request()
--        req0.request_id = req1.request_id = "test"
--        engine_core.add_request(req0)
--
--        while len(engine_core.step().outputs) > 0:
--            pass
--
--        engine_core.add_request(req1)
--        while len(engine_core.step().outputs) > 0:
--            pass
--
--        assert len(engine_core.scheduler.waiting) == 0
--        assert len(engine_core.scheduler.running) == 0
--
- 
- @create_new_process_for_each_test()
- def test_engine_core_advanced_sampling(monkeypatch: pytest.MonkeyPatch):
-diff --git a/tests/v1/tpu/test_sampler.py b/tests/v1/tpu/test_sampler.py
-deleted file mode 100644
-index 4e5a57be..00000000
---- a/tests/v1/tpu/test_sampler.py
-+++ /dev/null
-@@ -1,95 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--import tempfile
--from time import time
--
--import pytest
--
--from vllm import LLM, envs
--from vllm.platforms import current_platform
--from vllm.sampling_params import SamplingParams
--
--if not envs.VLLM_USE_V1:
--    pytest.skip(
--        "Skipping V1 tests. Rerun with `VLLM_USE_V1=1` to test.",
--        allow_module_level=True,
--    )
--
--
--@pytest.mark.parametrize("model_name", ["D4nt3/Qwen2.5-two-layers"])
--@pytest.mark.skipif(not current_platform.is_tpu(),
--                    reason="This test needs a TPU")
--def test_sampler_compilation(model_name: str, monkeypatch):
--    """
--    Check that no recompilation happens despite changing sampling parameters.
--    We can't read XLA metrics from the engine process, hence we measure time.  
--    """
--    with tempfile.TemporaryDirectory() as temp_dir:
--        monkeypatch.setenv("VLLM_XLA_CACHE_PATH", temp_dir)
--        # Compiling model init may still take some time, enforce_eager to skip.
--        llm = LLM(model_name,
--                  enforce_eager=True,
--                  max_num_seqs=16,
--                  max_model_len=1024,
--                  gpu_memory_utilization=0.5)
--        prompts = [
--            "A robot may not injure a human being",
--            "It is only with the heart that one can see rightly;",
--        ]
--        # First inference should be slow
--        sampling_params = SamplingParams(
--            temperature=0.7,
--            # top_p=0.6, # TODO too slow!
--            top_k=10,
--            min_p=0.2,
--            max_tokens=16)
--        s = time()
--        _ = llm.generate(prompts, sampling_params)
--        run1 = time() - s
--
--        # Second request with different params, but for which we
--        # compiled for in previous eager iteration.
--        sampling_params = SamplingParams(temperature=0.1,
--                                         top_k=12,
--                                         min_p=0.8,
--                                         max_tokens=24)
--        s = time()
--        _ = llm.generate(prompts, sampling_params)
--        run2 = time() - s
--        # Much faster after compiling
--        assert run1 * 0.1 > run2
--        print("TIMES", run1, run2)
--
--        # Third request with min_p set to "None". It will not trigger
--        # recompilation as a default 0 value will be used.
--        sampling_params = SamplingParams(max_tokens=24, temperature=0.0)
--        s = time()
--        _ = llm.generate(prompts, sampling_params)
--        run3 = time() - s
--        assert run1 * 0.1 > run3
--        print("TIMES", run1, run3)
--
--
--@pytest.mark.parametrize("model_name", ["Qwen/Qwen2.5-1.5B-Instruct"])
--@pytest.mark.skipif(not current_platform.is_tpu(),
--                    reason="This test needs a TPU")
--def test_sampler_different(model_name: str):
--    """
--    Test significantly different sampling params to assert the model produces 
--    different results.
--    """
--    llm = LLM(
--        model_name,
--        enforce_eager=True,
--        max_num_seqs=1,
--        max_model_len=64,
--        # TODO: setting to 0.5 or it will go OOM
--        gpu_memory_utilization=0.5)
--    prompts = [
--        "Write a short story about a robot that dreams for the first time."
--    ]
--    sampling_params = SamplingParams(temperature=0.9, min_p=0.2, max_tokens=64)
--    output = llm.generate(prompts, sampling_params)
--
--    sampling_params = SamplingParams(temperature=0.1, min_p=0.8, max_tokens=64)
--    output2 = llm.generate(prompts, sampling_params)
--    assert output[0].outputs[0].text != output2[0].outputs[0].text
-diff --git a/tests/v1/worker/test_gpu_model_runner.py b/tests/v1/worker/test_gpu_model_runner.py
-index dd95a7f5..345519a0 100644
---- a/tests/v1/worker/test_gpu_model_runner.py
-+++ b/tests/v1/worker/test_gpu_model_runner.py
-@@ -3,8 +3,8 @@ import pytest
- 
- from vllm.config import CacheConfig, ModelConfig, SchedulerConfig, VllmConfig
- from vllm.sampling_params import SamplingParams
--from vllm.v1.core.sched.output import (CachedRequestData, NewRequestData,
--                                       SchedulerOutput)
-+from vllm.v1.core.scheduler_output import (CachedRequestData, NewRequestData,
-+                                           SchedulerOutput)
- from vllm.v1.sample.metadata import SamplingMetadata
- from vllm.v1.worker.gpu_model_runner import GPUModelRunner
- 
-diff --git a/vllm/attention/__init__.py b/vllm/attention/__init__.py
-index 85c5715f..89229e7b 100644
---- a/vllm/attention/__init__.py
-+++ b/vllm/attention/__init__.py
-@@ -4,16 +4,12 @@ from vllm.attention.backends.abstract import (AttentionBackend,
-                                               AttentionMetadata,
-                                               AttentionMetadataBuilder,
-                                               AttentionState, AttentionType)
-+from vllm.attention.backends.utils import get_flash_attn_version
- from vllm.attention.layer import Attention
- from vllm.attention.selector import get_attn_backend
- 
- __all__ = [
--    "Attention",
--    "AttentionBackend",
--    "AttentionMetadata",
--    "AttentionType",
--    "AttentionMetadataBuilder",
--    "Attention",
--    "AttentionState",
--    "get_attn_backend",
-+    "Attention", "AttentionBackend", "AttentionMetadata", "AttentionType",
-+    "AttentionMetadataBuilder", "Attention", "AttentionState",
-+    "get_attn_backend", "get_flash_attn_version"
- ]
-diff --git a/vllm/attention/backends/abstract.py b/vllm/attention/backends/abstract.py
-index 82d60f9d..0cd95e07 100644
---- a/vllm/attention/backends/abstract.py
-+++ b/vllm/attention/backends/abstract.py
-@@ -232,7 +232,6 @@ class AttentionMetadataBuilder(ABC, Generic[T]):
- 
- class AttentionLayer(Protocol):
- 
--    _q_scale: torch.Tensor
-     _k_scale: torch.Tensor
-     _v_scale: torch.Tensor
-     _k_scale_float: float
-diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py
-index 4cb0b916..0e331efa 100755
---- a/vllm/attention/backends/flash_attn.py
-+++ b/vllm/attention/backends/flash_attn.py
-@@ -19,10 +19,10 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
- # yapf: enable
- from vllm.attention.backends.utils import (
-     PAD_SLOT_ID, CommonAttentionState, compute_slot_mapping,
--    compute_slot_mapping_start_idx, get_num_prefill_decode_query_kv_tokens,
--    get_seq_len_block_table_args, is_all_cross_attn_metadata_set,
--    is_all_encoder_attn_metadata_set, is_block_tables_empty)
--from vllm.fa_utils import get_flash_attn_version
-+    compute_slot_mapping_start_idx, get_flash_attn_version,
-+    get_num_prefill_decode_query_kv_tokens, get_seq_len_block_table_args,
-+    is_all_cross_attn_metadata_set, is_all_encoder_attn_metadata_set,
-+    is_block_tables_empty)
- from vllm.logger import init_logger
- from vllm.multimodal import MultiModalPlaceholderMap
- from vllm.utils import async_tensor_h2d, make_tensor_with_pad
-@@ -630,12 +630,9 @@ class FlashAttentionImpl(AttentionImpl):
-         self.sliding_window = ((sliding_window - 1,
-                                 0) if sliding_window is not None else (-1, -1))
-         self.kv_cache_dtype = kv_cache_dtype
--        self.vllm_flash_attn_version = get_flash_attn_version(
--            requires_alibi=self.alibi_slopes is not None)
--        if (is_quantized_kv_cache(self.kv_cache_dtype)
--                and self.vllm_flash_attn_version != 3):
-+        if is_quantized_kv_cache(self.kv_cache_dtype):
-             raise NotImplementedError(
--                "Only FlashAttention3 supports FP8 KV cache")
-+                "FlashAttention with FP8 KV cache not yet supported")
-         if logits_soft_cap is None:
-             # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
-             logits_soft_cap = 0
-@@ -650,6 +647,7 @@ class FlashAttentionImpl(AttentionImpl):
-                 f"Head size {head_size} is not supported by FlashAttention. "
-                 f"Supported head sizes are: {support_head_sizes}.")
-         self.attn_type = attn_type
-+        self.vllm_flash_attn_version = get_flash_attn_version()
- 
-     def forward(
-         self,
-@@ -673,18 +671,12 @@ class FlashAttentionImpl(AttentionImpl):
-                 for profiling run.
-             attn_metadata: Metadata for attention.
-         NOTE: It in-place updates the output tensor.
--        NOTE: FP8 quantization, flash-attn expect the size of
--              {q,k,v}_descale to be (num_sequences, num_kv_heads).
--              We use torch's .expand() to avoid duplicating values
-         """
--        assert output is not None, "Output tensor must be provided."
-+        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.
-+        assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0, (
-+            "key/v_scale is not supported in FlashAttention.")
- 
--        # NOTE(woosuk): FlashAttention2 does not support FP8 KV cache.
--        if self.vllm_flash_attn_version < 3 or output.dtype != torch.bfloat16:
--            assert (
--                layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0), (
--                    "key/v_scale is only supported in FlashAttention 3 with "
--                    "base dtype bfloat16")
-+        assert output is not None, "Output tensor must be provided."
- 
-         attn_type = self.attn_type
-         if (attn_type == AttentionType.ENCODER
-@@ -702,7 +694,6 @@ class FlashAttentionImpl(AttentionImpl):
-         window_size = self.sliding_window
-         alibi_slopes: Optional[torch.Tensor] = self.alibi_slopes
-         logits_soft_cap: Optional[float] = self.logits_soft_cap
--        fp8_attention = kv_cache_dtype.startswith("fp8")
- 
-         if kv_cache.numel() > 0:
-             key_cache = kv_cache[0]
-@@ -738,19 +729,6 @@ class FlashAttentionImpl(AttentionImpl):
-                     layer._v_scale,
-                 )
- 
--                if fp8_attention:
--                    kv_cache = kv_cache.view(torch.float8_e4m3fn)
--                    key_cache = key_cache.view(torch.float8_e4m3fn)
--                    value_cache = value_cache.view(torch.float8_e4m3fn)
--
--        if fp8_attention:
--            num_tokens, num_heads, head_size = query.shape
--            query, _ = ops.scaled_fp8_quant(
--                query.reshape(
--                    (num_tokens, num_heads * head_size)).contiguous(),
--                layer._q_scale)
--            query = query.reshape((num_tokens, num_heads, head_size))
--
-         (num_prefill_query_tokens, num_prefill_kv_tokens,
-         num_decode_query_tokens) = \
-             get_num_prefill_decode_query_kv_tokens(attn_metadata, attn_type)
-@@ -775,23 +753,6 @@ class FlashAttentionImpl(AttentionImpl):
-                 key = key[:num_prefill_kv_tokens]
-                 value = value[:num_prefill_kv_tokens]
- 
--                if fp8_attention:
--                    num_kv_tokens, num_kv_heads, head_size = key.shape
--
--                    key, _ = ops.scaled_fp8_quant(
--                        key.reshape((num_kv_tokens,
--                                     num_kv_heads * head_size)).contiguous(),
--                        layer._k_scale)
--                    key = key.reshape((num_kv_tokens, num_kv_heads, head_size))
--
--                    value, _ = ops.scaled_fp8_quant(
--                        value.reshape((num_kv_tokens,
--                                       num_kv_heads * head_size)).contiguous(),
--                        layer._v_scale)
--                    value = value.reshape(
--                        (num_kv_tokens, num_kv_heads, head_size))
--
--                descale_shape = (q_seq_start_loc.shape[0] - 1, key.shape[1])
-                 flash_attn_varlen_func(
-                     q=query,
-                     k=key,
-@@ -807,19 +768,13 @@ class FlashAttentionImpl(AttentionImpl):
-                     softcap=logits_soft_cap,
-                     out=prefill_output,
-                     fa_version=self.vllm_flash_attn_version,
--                    q_descale=layer._q_scale.expand(descale_shape),
--                    k_descale=layer._k_scale.expand(descale_shape),
--                    v_descale=layer._v_scale.expand(descale_shape),
-                 )
-             else:
-                 # prefix-enabled attention
-                 assert attn_type == AttentionType.DECODER, (
-                     "Only decoder-only models support prefix caching")
-                 assert prefill_meta.seq_lens is not None
--                assert prefill_meta.query_start_loc is not None
-                 max_seq_len = max(prefill_meta.seq_lens)
--                descale_shape = (prefill_meta.query_start_loc.shape[0] - 1,
--                                 key.shape[1])
-                 flash_attn_varlen_func(  # noqa
-                     q=query,
-                     k=key_cache,
-@@ -836,9 +791,6 @@ class FlashAttentionImpl(AttentionImpl):
-                     softcap=logits_soft_cap,
-                     out=prefill_output,
-                     fa_version=self.vllm_flash_attn_version,
--                    q_descale=layer._q_scale.expand(descale_shape),
--                    k_descale=layer._k_scale.expand(descale_shape),
--                    v_descale=layer._v_scale.expand(descale_shape),
-                 )
- 
-         if decode_meta := attn_metadata.decode_metadata:
-@@ -852,9 +804,6 @@ class FlashAttentionImpl(AttentionImpl):
-                 assert attn_type == AttentionType.DECODER, (
-                     "Only decoder-only models support max_decode_query_len > 1"
-                 )
--                assert decode_meta.query_start_loc is not None
--                descale_shape = (decode_meta.query_start_loc.shape[0] - 1,
--                                 key.shape[1])
-                 flash_attn_varlen_func(
-                     q=decode_query,
-                     k=key_cache,
-@@ -871,9 +820,6 @@ class FlashAttentionImpl(AttentionImpl):
-                     block_table=decode_meta.block_tables,
-                     out=decode_output,
-                     fa_version=self.vllm_flash_attn_version,
--                    q_descale=layer._q_scale.expand(descale_shape),
--                    k_descale=layer._k_scale.expand(descale_shape),
--                    v_descale=layer._v_scale.expand(descale_shape),
-                 )
-             else:
-                 # Use flash_attn_with_kvcache for normal decoding.
-@@ -882,7 +828,6 @@ class FlashAttentionImpl(AttentionImpl):
-                     _,
-                     block_tables_arg,
-                 ) = get_seq_len_block_table_args(decode_meta, False, attn_type)
--                descale_shape = (seq_lens_arg.shape[0], key_cache.shape[-2])
-                 flash_attn_with_kvcache(
-                     q=decode_query.unsqueeze(1),
-                     k_cache=key_cache,
-@@ -896,9 +841,6 @@ class FlashAttentionImpl(AttentionImpl):
-                     softcap=logits_soft_cap,
-                     out=decode_output.unsqueeze(1),
-                     fa_version=self.vllm_flash_attn_version,
--                    q_descale=layer._q_scale.expand(descale_shape),
--                    k_descale=layer._k_scale.expand(descale_shape),
--                    v_descale=layer._v_scale.expand(descale_shape),
-                 )
-         return output
- 
-diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py
-index 258090d3..ff411f75 100644
---- a/vllm/attention/backends/mla/common.py
-+++ b/vllm/attention/backends/mla/common.py
-@@ -203,9 +203,9 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionLayer,
-                                               AttentionState, MLAAttentionImpl)
- from vllm.attention.backends.utils import (PAD_SLOT_ID, compute_slot_mapping,
-                                            compute_slot_mapping_start_idx,
-+                                           get_flash_attn_version,
-                                            is_block_tables_empty)
- from vllm.attention.ops.triton_merge_attn_states import merge_attn_states
--from vllm.fa_utils import get_flash_attn_version
- from vllm.model_executor.layers.linear import (ColumnParallelLinear,
-                                                LinearBase, RowParallelLinear,
-                                                UnquantizedLinearMethod)
-diff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py
-index b4413c36..4374b542 100644
---- a/vllm/attention/backends/utils.py
-+++ b/vllm/attention/backends/utils.py
-@@ -8,11 +8,13 @@ from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Type, TypeVar, Union
- import numpy as np
- import torch
- 
-+from vllm import envs
- from vllm.attention import (AttentionMetadata, AttentionMetadataBuilder,
-                             AttentionState)
- from vllm.attention.backends.abstract import AttentionType
- from vllm.logger import init_logger
- from vllm.multimodal import MultiModalPlaceholderMap
-+from vllm.platforms import current_platform
- from vllm.utils import async_tensor_h2d, make_tensor_with_pad
- 
- logger = init_logger(__name__)
-@@ -583,3 +585,35 @@ def get_num_prefill_decode_query_kv_tokens(
- 
-     return (num_prefill_query_tokens, num_prefill_kv_tokens,
-             num_decode_query_tokens)
-+
-+
-+def get_flash_attn_version():
-+    try:
-+        from vllm.vllm_flash_attn.flash_attn_interface import (
-+            fa_version_unsupported_reason, is_fa_version_supported)
-+
-+        # if hopper default to FA3, otherwise stick to FA2 for now
-+        # TODO(lucas): profile FA3 on ampere to see if it makes sense to
-+        #  use FA3 as default for both
-+        if current_platform.get_device_capability()[0] == 9:
-+            fa_version = 3 if is_fa_version_supported(3) else 2
-+        else:
-+            fa_version = 2
-+
-+        if envs.VLLM_FLASH_ATTN_VERSION is not None:
-+            assert envs.VLLM_FLASH_ATTN_VERSION in [2, 3]
-+            fa_version = envs.VLLM_FLASH_ATTN_VERSION
-+            if (current_platform.get_device_capability()[0] == 10
-+                    and envs.VLLM_FLASH_ATTN_VERSION == 3):
-+                logger.warning("Cannot use FA version 3 on Blackwell platform",
-+                               "defaulting to FA version 2.")
-+                fa_version = 2
-+
-+        if not is_fa_version_supported(fa_version):
-+            logger.error("Cannot use FA version %d is not supported due to %s",
-+                         fa_version, fa_version_unsupported_reason(fa_version))
-+
-+        assert is_fa_version_supported(fa_version)
-+        return fa_version
-+    except (ImportError, AssertionError):
-+        return None
-diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py
-index 946c07d5..3cbd38db 100644
---- a/vllm/attention/layer.py
-+++ b/vllm/attention/layer.py
-@@ -84,9 +84,6 @@ class Attention(nn.Module):
-         self.calculate_kv_scales = calculate_kv_scales
-         self._k_scale = torch.tensor(1.0, dtype=torch.float32)
-         self._v_scale = torch.tensor(1.0, dtype=torch.float32)
--        # FlashAttn doesn't support quantizing the kv-cache only
--        # but requires q to be quantized as well.
--        self._q_scale = torch.tensor(1.0, dtype=torch.float32)
- 
-         # We also keep the float32 versions of k/v_scale for attention
-         # backends that don't support tensors (Flashinfer)
-@@ -156,7 +153,6 @@ class Attention(nn.Module):
-             ).parallel_config.pipeline_parallel_size)
-         ]
- 
--        self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)
-         self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)
-         self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)
- 
-@@ -182,7 +178,7 @@ class Attention(nn.Module):
-         if self.calculate_kv_scales:
-             attn_metadata = get_forward_context().attn_metadata
-             if attn_metadata.enable_kv_scales_calculation:
--                self.calc_kv_scales(query, key, value)
-+                self.calc_kv_scales(key, value)
-         if self.use_output:
-             output_shape = (output_shape
-                             if output_shape is not None else query.shape)
-@@ -229,8 +225,7 @@ class Attention(nn.Module):
-                 return torch.ops.vllm.unified_attention(
-                     query, key, value, self.layer_name)
- 
--    def calc_kv_scales(self, query, key, value):
--        self._q_scale.copy_(torch.abs(query).max() / self.q_range)
-+    def calc_kv_scales(self, key, value):
-         self._k_scale.copy_(torch.abs(key).max() / self.k_range)
-         self._v_scale.copy_(torch.abs(value).max() / self.v_range)
-         self._k_scale_float = self._k_scale.item()
-diff --git a/vllm/config.py b/vllm/config.py
-index 1f7147f7..c248122d 100644
---- a/vllm/config.py
-+++ b/vllm/config.py
-@@ -246,7 +246,6 @@ class ModelConfig:
-         max_seq_len_to_capture: Optional[int] = None,
-         max_logprobs: int = 20,
-         disable_sliding_window: bool = False,
--        disable_cascade_attn: bool = False,
-         skip_tokenizer_init: bool = False,
-         served_model_name: Optional[Union[str, list[str]]] = None,
-         limit_mm_per_prompt: Optional[Mapping[str, int]] = None,
-@@ -323,7 +322,6 @@ class ModelConfig:
-         self.max_seq_len_to_capture = max_seq_len_to_capture
-         self.max_logprobs = max_logprobs
-         self.disable_sliding_window = disable_sliding_window
--        self.disable_cascade_attn = disable_cascade_attn
-         self.skip_tokenizer_init = skip_tokenizer_init
-         self.enable_sleep_mode = enable_sleep_mode
- 
-@@ -672,6 +670,14 @@ class ModelConfig:
-         self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,
-                                           self.max_model_len)
- 
-+        MODEL_NOT_SUPPORT_CUDA_GRAPH = ['mllama']
-+        if (self.hf_config.model_type in MODEL_NOT_SUPPORT_CUDA_GRAPH
-+                and not self.enforce_eager):
-+            logger.warning(
-+                "CUDA graph is not supported for %s yet, fallback to the eager "
-+                "mode.", self.hf_config.model_type)
-+            self.enforce_eager = True
-+
-     def _verify_bnb_config(self) -> None:
-         """
-         The current version of bitsandbytes (0.44.0) with 8-bit models does not
-@@ -1467,7 +1473,7 @@ class ParallelConfig:
-             os.environ["VLLM_ENABLE_V1_MULTIPROCESSING"] = "0"
-             logger.info("Disabling V1 multiprocessing for external launcher.")
- 
--        ray_only_devices: list[str] = []
-+        ray_only_devices = ["tpu"]
-         from vllm.platforms import current_platform
-         if (current_platform.device_type in ray_only_devices
-                 and self.world_size > 1):
-@@ -2779,14 +2785,12 @@ class DecodingConfig:
-         return hash_str
- 
-     def __post_init__(self):
--        valid_guided_backends = [
--            'outlines', 'lm-format-enforcer', 'xgrammar', 'guidance'
--        ]
-+        valid_guided_backends = ['outlines', 'lm-format-enforcer', 'xgrammar']
- 
-         backend = GuidedDecodingParams(
-             backend=self.guided_decoding_backend).backend_name
-         if backend not in valid_guided_backends:
--            raise ValueError(f"Invalid guided_decoding_backend '{backend}',"
-+            raise ValueError(f"Invalid guided_decoding_backend '{backend},"
-                              f" must be one of {valid_guided_backends}")
- 
- 
-diff --git a/vllm/core/block/cpu_gpu_block_allocator.py b/vllm/core/block/cpu_gpu_block_allocator.py
-index d64142e7..359b5b26 100644
---- a/vllm/core/block/cpu_gpu_block_allocator.py
-+++ b/vllm/core/block/cpu_gpu_block_allocator.py
-@@ -341,10 +341,8 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):
-         assert device in self._allocators
-         return self._allocators[device].get_prefix_cache_hit_rate()
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
--        """Reset prefix cache for specified or all devices."""
--        if device:
--            return self._allocators[device].reset_prefix_cache()
-+    def reset_prefix_cache(self) -> bool:
-+        """Reset prefix cache for all devices."""
-         success = True
-         for allocator in self._allocators.values():
-             success = success and allocator.reset_prefix_cache()
-diff --git a/vllm/core/block/interfaces.py b/vllm/core/block/interfaces.py
-index 30165699..0b0197de 100644
---- a/vllm/core/block/interfaces.py
-+++ b/vllm/core/block/interfaces.py
-@@ -305,7 +305,7 @@ class DeviceAwareBlockAllocator(ABC):
-         pass
- 
-     @abstractmethod
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
-+    def reset_prefix_cache(self) -> bool:
-         """Reset prefix cache."""
-         pass
- 
-diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py
-index c6bf6d16..c5b3b04f 100644
---- a/vllm/core/block_manager.py
-+++ b/vllm/core/block_manager.py
-@@ -456,8 +456,8 @@ class SelfAttnBlockSpaceManager(BlockSpaceManager):
-     def get_prefix_cache_hit_rate(self, device: Device) -> float:
-         return self.block_allocator.get_prefix_cache_hit_rate(device)
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
--        return self.block_allocator.reset_prefix_cache(device)
-+    def reset_prefix_cache(self) -> bool:
-+        return self.block_allocator.reset_prefix_cache()
- 
-     def _can_swap(self,
-                   seq_group: SequenceGroup,
-diff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py
-index 4c1182de..b48ba87e 100644
---- a/vllm/core/interfaces.py
-+++ b/vllm/core/interfaces.py
-@@ -2,7 +2,7 @@
- 
- import enum
- from abc import ABC, abstractmethod
--from typing import List, Optional
-+from typing import List
- from typing import Sequence as GenericSequence
- from typing import Tuple
- 
-@@ -125,8 +125,8 @@ class BlockSpaceManager(ABC):
-         pass
- 
-     @abstractmethod
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
--        """Reset prefix cache for specified or all devices."""
-+    def reset_prefix_cache(self) -> bool:
-+        """Reset prefix cache for all devices."""
-         pass
- 
-     @abstractmethod
-diff --git a/vllm/core/placeholder_block_space_manager.py b/vllm/core/placeholder_block_space_manager.py
-index 0f5d8ca6..70c22afa 100644
---- a/vllm/core/placeholder_block_space_manager.py
-+++ b/vllm/core/placeholder_block_space_manager.py
-@@ -1,6 +1,6 @@
- # SPDX-License-Identifier: Apache-2.0
- 
--from typing import List, Optional, Tuple
-+from typing import List, Tuple
- 
- from vllm.core.interfaces import AllocStatus, BlockSpaceManager
- from vllm.sequence import Sequence, SequenceGroup
-@@ -92,7 +92,7 @@ class PlaceholderBlockSpaceManager(BlockSpaceManager):
-     def get_prefix_cache_hit_rate(self, device: Device) -> float:
-         return -1
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
-+    def reset_prefix_cache(self) -> bool:
-         return True
- 
-     def get_num_cached_tokens(self, seq: Sequence) -> int:
-diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py
-index cf85a213..e93143c8 100644
---- a/vllm/core/scheduler.py
-+++ b/vllm/core/scheduler.py
-@@ -634,8 +634,8 @@ class Scheduler:
-     def get_prefix_cache_hit_rate(self, device: Device) -> float:
-         return self.block_manager.get_prefix_cache_hit_rate(device)
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
--        return self.block_manager.reset_prefix_cache(device)
-+    def reset_prefix_cache(self) -> bool:
-+        return self.block_manager.reset_prefix_cache()
- 
-     def get_num_unfinished_seq_groups(self) -> int:
-         return len(self.waiting) + len(self.running) + len(self.swapped)
-diff --git a/vllm/distributed/device_communicators/tpu_communicator.py b/vllm/distributed/device_communicators/tpu_communicator.py
-index 05cb1e0f..524e655b 100644
---- a/vllm/distributed/device_communicators/tpu_communicator.py
-+++ b/vllm/distributed/device_communicators/tpu_communicator.py
-@@ -6,25 +6,16 @@ from typing import Optional
- import torch
- from torch.distributed import ProcessGroup
- 
--from vllm.config import get_current_vllm_config
--from vllm.logger import init_logger
- from vllm.platforms import current_platform
- 
- from .base_device_communicator import DeviceCommunicatorBase
- 
--USE_RAY = parallel_config = get_current_vllm_config(
--).parallel_config.distributed_executor_backend == "ray"
--
--logger = init_logger(__name__)
--
- if current_platform.is_tpu():
--    import torch_xla
-     import torch_xla.core.xla_model as xm
-     import torch_xla.runtime as xr
-     from torch_xla._internal import pjrt
- 
--    if USE_RAY:
--        from vllm.executor import ray_utils
-+    from vllm.executor import ray_utils
- 
- 
- class TpuCommunicator(DeviceCommunicatorBase):
-@@ -42,32 +33,19 @@ class TpuCommunicator(DeviceCommunicatorBase):
-         global_rank = self.global_rank
-         global_world_size = self.global_world_size
- 
--        if USE_RAY:
--            logger.info("TpuCommunicator initialized with RAY")
--            # Calculate how many TPU nodes are in the current deployment. This
--            # is the Ray placement group if it is deployed with Ray. Default
--            # to the number of TPU nodes in the Ray cluster. The number of TPU
--            # nodes is computed by the total number of TPUs divided by the
--            # number of TPU accelerators per node, to account for clusters
--            # with both CPUs and TPUs.
--            num_nodes = ray_utils.get_num_tpu_nodes()
--            num_nodes_in_pg = ray_utils.get_num_nodes_in_placement_group()
--            if num_nodes_in_pg > 0:
--                num_nodes = num_nodes_in_pg
--
--            local_world_size = global_world_size // num_nodes
--            local_rank = global_rank % local_world_size
--        else:
--            logger.info("TpuCommunicator initialized with MP")
--            # Sanity: Verify we run on a single host
--            num_hosts = torch_xla.tpu.num_tpu_workers()
--            assert num_hosts == 1
--
--            # Get the current number of TPUs (we have locally)
--            local_world_size = torch_xla.tpu.num_available_chips()
--
--            # Get current rank
--            local_rank = global_rank % local_world_size
-+        # Calculate how many TPU nodes are in the current deployment. This
-+        # is the Ray placement group if it is deployed with Ray. Default
-+        # to the number of TPU nodes in the Ray cluster. The number of TPU
-+        # nodes is computed by the total number of TPUs divided by the
-+        # number of TPU accelerators per node, to account for clusters
-+        # with both CPUs and TPUs.
-+        num_nodes = ray_utils.get_num_tpu_nodes()
-+        num_nodes_in_pg = ray_utils.get_num_nodes_in_placement_group()
-+        if num_nodes_in_pg > 0:
-+            num_nodes = num_nodes_in_pg
-+
-+        local_world_size = global_world_size // num_nodes
-+        local_rank = global_rank % local_world_size
- 
-         # Ensure environment variables are set for multihost deployments.
-         # On GKE, this is needed for libtpu and TPU driver to know which TPU
-diff --git a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-index 49b97d7b..7315a6f4 100644
---- a/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-+++ b/vllm/distributed/kv_transfer/kv_connector/simple_connector.py
-@@ -12,7 +12,6 @@ from typing import TYPE_CHECKING, List, Optional, Tuple, Union
- 
- import torch
- 
--import vllm.envs as envs
- from vllm import _custom_ops as ops
- from vllm.config import VllmConfig
- from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
-@@ -38,8 +37,6 @@ class SimpleConnector(KVConnectorBase):
- 
-         self.config = config.kv_transfer_config
-         self.tp_size = config.parallel_config.tensor_parallel_size
--        self.is_deepseek_mla = config.model_config.is_deepseek_mla
--        self.use_mla_opt = not envs.VLLM_MLA_DISABLE
- 
-         if self.config.kv_connector == "PyNcclConnector":
-             from vllm.distributed.kv_transfer.kv_pipe.pynccl_pipe import (
-@@ -170,26 +167,8 @@ class SimpleConnector(KVConnectorBase):
-         num_heads = int(model_config.num_key_value_heads / self.tp_size)
-         hidden_size = model_config.hidden_size
-         num_attention_heads = model_config.num_attention_heads
--
--        # Deepseek's MLA (Multi-head Latent Attention) uses two different
--        # kv_cache shapes based on whether VLLM_MLA_DISABLE is set to 0.
--        # When VLLM_MLA_DISABLE=0 (default), forward absorb is applied,
--        # resulting in a kv_cache shape of [num_blks, blk_size, 1,
--        # kv_lora_rank + qk_rope_head_dim].
--        # When VLLM_MLA_DISABLE=1, standard FA is used instead, leading
--        # to a kv_cache shape of [2, num_blks, blk_size,
--        # num_key_value_heads / tp, qk_nope_head_dim + qk_rope_head_dim].
--        # For more details, see vllm/attention/backends/mla/common.py.
--        if self.is_deepseek_mla and self.use_mla_opt:
--            head_size = model_config.kv_lora_rank + \
--                model_config.qk_rope_head_dim
--            num_heads = 1
--        elif self.is_deepseek_mla and not self.use_mla_opt:
--            head_size = model_config.qk_nope_head_dim + \
--                model_config.qk_rope_head_dim
--        else:
--            head_size = getattr(model_config, "head_dim",
--                                int(hidden_size // num_attention_heads))
-+        head_size = getattr(model_config, "head_dim",
-+                            int(hidden_size // num_attention_heads))
- 
-         # query_lens contains new KV caches that are added to vLLM.
-         # so we will send them to decode instance
-@@ -213,12 +192,8 @@ class SimpleConnector(KVConnectorBase):
-             for layer_id in range(start_layer, end_layer):
-                 kv_cache = kv_caches[layer_id - start_layer]
- 
--                if self.is_deepseek_mla and self.use_mla_opt:
--                    key_cache = kv_cache.reshape(-1, num_heads, head_size)
--                    value_cache = kv_cache.reshape(-1, num_heads, head_size)
--                else:
--                    key_cache = kv_cache[0].reshape(-1, num_heads, head_size)
--                    value_cache = kv_cache[1].reshape(-1, num_heads, head_size)
-+                key_cache = kv_cache[0].reshape(-1, num_heads, head_size)
-+                value_cache = kv_cache[1].reshape(-1, num_heads, head_size)
- 
-                 current_slot_mapping = slot_mapping_flat[start_pos:end_pos]
- 
-@@ -248,8 +223,6 @@ class SimpleConnector(KVConnectorBase):
-         # and hidden states.
-         bypass_model_exec = True
- 
--        model_config = model_executable.model.config
--
-         input_tokens_tensor = model_input.input_tokens
-         seq_lens = model_input.attn_metadata.seq_lens
-         num_prefill_tokens = model_input.attn_metadata.num_prefill_tokens
-@@ -318,35 +291,19 @@ class SimpleConnector(KVConnectorBase):
-                 kv_cache = kv_caches[i - model_executable.model.start_layer]
-                 layer = model_executable.model.layers[i]
- 
--                if self.is_deepseek_mla and self.use_mla_opt:
--                    layer.self_attn.attn = layer.self_attn.mla_attn
--                    k_c_normed_k_pe = keys[
--                        i - model_executable.model.start_layer].to(
--                            kv_cache.device).squeeze(1)
--                    k_c_normed = k_c_normed_k_pe[:, :model_config.kv_lora_rank]
--                    k_pe = k_c_normed_k_pe[:, model_config.kv_lora_rank:]
--                    ops.concat_and_cache_mla(
--                        k_c_normed,
--                        k_pe,
--                        kv_cache,
--                        slot_mapping[start_pos:end_pos],
--                        layer.self_attn.attn.kv_cache_dtype,
--                        layer.self_attn.attn._k_scale,
--                    )
--                else:
--                    key_cache, value_cache = kv_cache[0], kv_cache[1]
--                    ops.reshape_and_cache_flash(
--                        keys[i - model_executable.model.start_layer].to(
--                            key_cache.device),
--                        values[i - model_executable.model.start_layer].to(
--                            value_cache.device),
--                        key_cache,
--                        value_cache,
--                        slot_mapping[start_pos:end_pos],
--                        layer.self_attn.attn.kv_cache_dtype,
--                        layer.self_attn.attn._k_scale,
--                        layer.self_attn.attn._v_scale,
--                    )
-+                key_cache, value_cache = kv_cache[0], kv_cache[1]
-+                ops.reshape_and_cache_flash(
-+                    keys[i - model_executable.model.start_layer].to(
-+                        key_cache.device),
-+                    values[i - model_executable.model.start_layer].to(
-+                        value_cache.device),
-+                    key_cache,
-+                    value_cache,
-+                    slot_mapping[start_pos:end_pos],
-+                    layer.self_attn.attn.kv_cache_dtype,
-+                    layer.self_attn.attn._k_scale,
-+                    layer.self_attn.attn._v_scale,
-+                )
- 
-             hidden_or_intermediate_states_for_one_req.append(hidden)
- 
-diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
-index edfa748b..43bf2fe8 100644
---- a/vllm/engine/arg_utils.py
-+++ b/vllm/engine/arg_utils.py
-@@ -26,7 +26,7 @@ from vllm.plugins import load_general_plugins
- from vllm.test_utils import MODEL_WEIGHTS_S3_BUCKET, MODELS_ON_S3
- from vllm.transformers_utils.utils import check_gguf_file
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import FlexibleArgumentParser, StoreBoolean, is_in_ray_actor
-+from vllm.utils import FlexibleArgumentParser, StoreBoolean
- 
- if TYPE_CHECKING:
-     from vllm.transformers_utils.tokenizer_group import BaseTokenizerGroup
-@@ -120,7 +120,6 @@ class EngineArgs:
-     block_size: Optional[int] = None
-     enable_prefix_caching: Optional[bool] = None
-     disable_sliding_window: bool = False
--    disable_cascade_attn: bool = False
-     use_v2_block_manager: bool = True
-     swap_space: float = 4  # GiB
-     cpu_offload_gb: float = 0  # GiB
-@@ -1097,16 +1096,6 @@ class EngineArgs:
-             "using. This is used to parse the reasoning content into OpenAI "
-             "API format. Required for ``--enable-reasoning``.")
- 
--        parser.add_argument(
--            "--disable-cascade-attn",
--            action="store_true",
--            default=False,
--            help="Disable cascade attention for V1. While cascade attention "
--            "does not change the mathematical correctness, disabling it "
--            "could be useful for preventing potential numerical issues. "
--            "Note that even if this is set to False, cascade attention will be "
--            "only used when the heuristic tells that it's beneficial.")
--
-         return parser
- 
-     @classmethod
-@@ -1152,7 +1141,6 @@ class EngineArgs:
-             max_seq_len_to_capture=self.max_seq_len_to_capture,
-             max_logprobs=self.max_logprobs,
-             disable_sliding_window=self.disable_sliding_window,
--            disable_cascade_attn=self.disable_cascade_attn,
-             skip_tokenizer_init=self.skip_tokenizer_init,
-             served_model_name=self.served_model_name,
-             limit_mm_per_prompt=self.limit_mm_per_prompt,
-@@ -1170,15 +1158,22 @@ class EngineArgs:
-         )
- 
-     def create_load_config(self) -> LoadConfig:
-+        # bitsandbytes quantization needs a specific model loader
-+        # so we make sure the quant method and the load format are consistent
-+        if (self.quantization == "bitsandbytes" or
-+           self.qlora_adapter_name_or_path is not None) and \
-+           self.load_format != "bitsandbytes":
-+            raise ValueError(
-+                "BitsAndBytes quantization and QLoRA adapter only support "
-+                f"'bitsandbytes' load format, but got {self.load_format}")
- 
--        if(self.qlora_adapter_name_or_path is not None) and \
-+        if (self.load_format == "bitsandbytes" or
-+            self.qlora_adapter_name_or_path is not None) and \
-             self.quantization != "bitsandbytes":
-             raise ValueError(
--                "QLoRA adapter only support "
-+                "BitsAndBytes load format and QLoRA adapter only support "
-                 f"'bitsandbytes' quantization, but got {self.quantization}")
- 
--        if self.quantization == "bitsandbytes":
--            self.load_format = "bitsandbytes"
-         return LoadConfig(
-             load_format=self.load_format,
-             download_dir=self.download_dir,
-@@ -1245,18 +1240,6 @@ class EngineArgs:
-             cpu_offload_gb=self.cpu_offload_gb,
-             calculate_kv_scales=self.calculate_kv_scales,
-         )
--
--        # Get the current placement group if Ray is initialized and
--        # we are in a Ray actor. If so, then the placement group will be
--        # passed to spawned processes.
--        placement_group = None
--        if is_in_ray_actor():
--            import ray
--
--            # This call initializes Ray automatically if it is not initialized,
--            # but we should not do this here.
--            placement_group = ray.util.get_current_placement_group()
--
-         parallel_config = ParallelConfig(
-             pipeline_parallel_size=self.pipeline_parallel_size,
-             tensor_parallel_size=self.tensor_parallel_size,
-@@ -1269,7 +1252,6 @@ class EngineArgs:
-                 self.tokenizer_pool_extra_config,
-             ),
-             ray_workers_use_nsight=self.ray_workers_use_nsight,
--            placement_group=placement_group,
-             distributed_executor_backend=self.distributed_executor_backend,
-             worker_cls=self.worker_cls,
-             worker_extension_cls=self.worker_extension_cls,
-@@ -1588,7 +1570,7 @@ class EngineArgs:
-         # No FlashInfer or XFormers so far.
-         V1_BACKENDS = [
-             "FLASH_ATTN_VLLM_V1", "FLASH_ATTN", "PALLAS", "PALLAS_VLLM_V1",
--            "TRITON_ATTN_VLLM_V1", "TRITON_MLA", "FLASHMLA"
-+            "TRITON_MLA", "FLASHMLA"
-         ]
-         if (envs.is_set("VLLM_ATTENTION_BACKEND")
-                 and envs.VLLM_ATTENTION_BACKEND not in V1_BACKENDS):
-@@ -1701,7 +1683,7 @@ class EngineArgs:
-         # V1 should use the new scheduler by default.
-         # Swap it only if this arg is set to the original V0 default
-         if self.scheduler_cls == EngineArgs.scheduler_cls:
--            self.scheduler_cls = "vllm.v1.core.sched.scheduler.Scheduler"
-+            self.scheduler_cls = "vllm.v1.core.scheduler.Scheduler"
- 
-         # When no user override, set the default values based on the usage
-         # context.
-diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py
-index c6fafbee..63787590 100644
---- a/vllm/engine/async_llm_engine.py
-+++ b/vllm/engine/async_llm_engine.py
-@@ -35,7 +35,7 @@ from vllm.sampling_params import SamplingParams
- from vllm.sequence import ExecuteModelRequest
- from vllm.transformers_utils.tokenizer import AnyTokenizer
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import Device, deprecate_kwargs, weak_bind
-+from vllm.utils import deprecate_kwargs, weak_bind
- 
- logger = init_logger(__name__)
- ENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S
-@@ -1216,9 +1216,8 @@ class AsyncLLMEngine(EngineClient):
-     async def stop_profile(self) -> None:
-         self.engine.stop_profile()
- 
--    async def reset_prefix_cache(self,
--                                 device: Optional[Device] = None) -> None:
--        self.engine.reset_prefix_cache(device)
-+    async def reset_prefix_cache(self) -> None:
-+        self.engine.reset_prefix_cache()
- 
-     async def sleep(self, level: int = 1) -> None:
-         self.engine.sleep(level)
-diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
-index 51a82c41..ca50f08a 100644
---- a/vllm/engine/llm_engine.py
-+++ b/vllm/engine/llm_engine.py
-@@ -955,12 +955,12 @@ class LLMEngine:
-         """
-         return self.scheduler[virtual_engine].has_unfinished_seqs()
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
-+    def reset_prefix_cache(self) -> bool:
-         """Reset prefix cache for all devices."""
- 
-         success = True
-         for scheduler in self.scheduler:
--            success = success and scheduler.reset_prefix_cache(device)
-+            success = success and scheduler.reset_prefix_cache()
-         return success
- 
-     @staticmethod
-diff --git a/vllm/engine/multiprocessing/__init__.py b/vllm/engine/multiprocessing/__init__.py
-index fdad5358..144dd822 100644
---- a/vllm/engine/multiprocessing/__init__.py
-+++ b/vllm/engine/multiprocessing/__init__.py
-@@ -13,7 +13,7 @@ from vllm.lora.request import LoRARequest
- from vllm.outputs import RequestOutput
- from vllm.prompt_adapter.request import PromptAdapterRequest
- from vllm.sampling_params import SamplingParams
--from vllm.utils import Device, deprecate_kwargs
-+from vllm.utils import deprecate_kwargs
- 
- VLLM_RPC_SUCCESS_STR = "SUCCESS"
- 
-@@ -123,9 +123,8 @@ class RPCUProfileRequest(Enum):
-     STOP_PROFILE = 2
- 
- 
--@dataclass
--class RPCResetPrefixCacheRequest:
--    device: Device
-+class RPCResetPrefixCacheRequest(Enum):
-+    RESET_PREFIX_CACHE = 1
- 
- 
- class RPCSleepRequest(Enum):
-diff --git a/vllm/engine/multiprocessing/client.py b/vllm/engine/multiprocessing/client.py
-index db91c5d3..e2ae9486 100644
---- a/vllm/engine/multiprocessing/client.py
-+++ b/vllm/engine/multiprocessing/client.py
-@@ -47,7 +47,7 @@ from vllm.outputs import PoolingRequestOutput, RequestOutput
- from vllm.prompt_adapter.request import PromptAdapterRequest
- from vllm.sampling_params import SamplingParams
- from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs
--from vllm.utils import Device, deprecate_kwargs
-+from vllm.utils import deprecate_kwargs
- 
- logger = init_logger(__name__)
- 
-@@ -684,12 +684,11 @@ class MQLLMEngineClient(EngineClient):
-         await self._send_one_way_rpc_request(
-             request=RPCUProfileRequest.STOP_PROFILE, socket=self.input_socket)
- 
--    async def reset_prefix_cache(self,
--                                 device: Optional[Device] = None) -> None:
-+    async def reset_prefix_cache(self) -> None:
-         """Reset the prefix cache"""
- 
-         await self._send_one_way_rpc_request(
--            request=RPCResetPrefixCacheRequest(device),
-+            request=RPCResetPrefixCacheRequest.RESET_PREFIX_CACHE,
-             socket=self.input_socket)
- 
-     async def sleep(self, level: int = 1) -> None:
-diff --git a/vllm/engine/protocol.py b/vllm/engine/protocol.py
-index be9f3af0..f314075b 100644
---- a/vllm/engine/protocol.py
-+++ b/vllm/engine/protocol.py
-@@ -18,7 +18,7 @@ from vllm.pooling_params import PoolingParams
- from vllm.prompt_adapter.request import PromptAdapterRequest
- from vllm.sampling_params import BeamSearchParams, SamplingParams
- from vllm.transformers_utils.tokenizer import AnyTokenizer
--from vllm.utils import Device, collect_from_async_generator, random_uuid
-+from vllm.utils import collect_from_async_generator, random_uuid
- 
- logger = init_logger(__name__)
- 
-@@ -274,8 +274,7 @@ class EngineClient(ABC):
-         ...
- 
-     @abstractmethod
--    async def reset_prefix_cache(self,
--                                 device: Optional[Device] = None) -> None:
-+    async def reset_prefix_cache(self) -> None:
-         """Reset the prefix cache"""
-         ...
- 
-diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
-index 84b0093c..a0e2fa29 100644
---- a/vllm/entrypoints/llm.py
-+++ b/vllm/entrypoints/llm.py
-@@ -42,8 +42,7 @@ from vllm.transformers_utils.tokenizer import (AnyTokenizer, MistralTokenizer,
-                                                get_cached_tokenizer)
- from vllm.transformers_utils.tokenizer_group import TokenizerGroup
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import (Counter, Device, deprecate_args, deprecate_kwargs,
--                        is_list_of)
-+from vllm.utils import Counter, deprecate_args, deprecate_kwargs, is_list_of
- 
- logger = init_logger(__name__)
- 
-@@ -1188,8 +1187,8 @@ class LLM:
-     def stop_profile(self) -> None:
-         self.llm_engine.stop_profile()
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:
--        return self.llm_engine.reset_prefix_cache(device)
-+    def reset_prefix_cache(self) -> bool:
-+        return self.llm_engine.reset_prefix_cache()
- 
-     def sleep(self, level: int = 1):
-         """
-diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py
-index f9b1d69a..ef557193 100644
---- a/vllm/entrypoints/openai/api_server.py
-+++ b/vllm/entrypoints/openai/api_server.py
-@@ -85,7 +85,7 @@ from vllm.logger import init_logger
- from vllm.transformers_utils.config import (
-     maybe_register_config_serialize_by_value)
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import (Device, FlexibleArgumentParser, get_open_zmq_ipc_path,
-+from vllm.utils import (FlexibleArgumentParser, get_open_zmq_ipc_path,
-                         is_valid_ipv6_address, set_ulimit)
- from vllm.version import __version__ as VLLM_VERSION
- 
-@@ -677,12 +677,8 @@ if envs.VLLM_SERVER_DEV_MODE:
-         Reset the prefix cache. Note that we currently do not check if the
-         prefix cache is successfully reset in the API server.
-         """
--        device = None
--        device_str = raw_request.query_params.get("device")
--        if device_str is not None:
--            device = Device[device_str.upper()]
--        logger.info("Resetting prefix cache with specific %s...", str(device))
--        await engine_client(raw_request).reset_prefix_cache(device)
-+        logger.info("Resetting prefix cache...")
-+        await engine_client(raw_request).reset_prefix_cache()
-         return Response(status_code=200)
- 
-     @router.post("/sleep")
-@@ -1036,9 +1032,6 @@ async def run_server(args, **uvicorn_kwargs) -> None:
-             host=args.host,
-             port=args.port,
-             log_level=args.uvicorn_log_level,
--            # NOTE: When the 'disable_uvicorn_access_log' value is True,
--            # no access log will be output.
--            access_log=not args.disable_uvicorn_access_log,
-             timeout_keep_alive=TIMEOUT_KEEP_ALIVE,
-             ssl_keyfile=args.ssl_keyfile,
-             ssl_certfile=args.ssl_certfile,
-diff --git a/vllm/entrypoints/openai/cli_args.py b/vllm/entrypoints/openai/cli_args.py
-index 01c67b8a..bd66416d 100644
---- a/vllm/entrypoints/openai/cli_args.py
-+++ b/vllm/entrypoints/openai/cli_args.py
-@@ -89,9 +89,6 @@ def make_arg_parser(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:
-         default="info",
-         choices=['debug', 'info', 'warning', 'error', 'critical', 'trace'],
-         help="Log level for uvicorn.")
--    parser.add_argument("--disable-uvicorn-access-log",
--                        action="store_true",
--                        help="Disable uvicorn access log.")
-     parser.add_argument("--allow-credentials",
-                         action="store_true",
-                         help="Allow credentials.")
-diff --git a/vllm/envs.py b/vllm/envs.py
-index d54de9da..b2937462 100644
---- a/vllm/envs.py
-+++ b/vllm/envs.py
-@@ -45,7 +45,6 @@ if TYPE_CHECKING:
-     VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None
-     VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False
-     VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, "xla_cache")
--    VLLM_XLA_CHECK_RECOMPILATION: bool = False
-     VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024
-     VLLM_USE_RAY_SPMD_WORKER: bool = False
-     VLLM_USE_RAY_COMPILED_DAG: bool = False
-@@ -79,7 +78,6 @@ if TYPE_CHECKING:
-     VLLM_ENABLE_V1_MULTIPROCESSING: bool = True
-     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1
-     VLLM_DISABLE_COMPILE_CACHE: bool = False
--    Q_SCALE_CONSTANT: int = 200
-     K_SCALE_CONSTANT: int = 200
-     V_SCALE_CONSTANT: int = 100
-     VLLM_SERVER_DEV_MODE: bool = False
-@@ -96,7 +94,6 @@ if TYPE_CHECKING:
-     VLLM_DP_MASTER_PORT: int = 0
-     VLLM_MARLIN_USE_ATOMIC_ADD: bool = False
-     VLLM_V0_USE_OUTLINES_CACHE: bool = False
--    VLLM_TPU_DISABLE_TOPK_TOPP_OPTIMIZATION: bool = False
- 
- 
- def get_default_cache_root():
-@@ -447,10 +444,6 @@ environment_variables: dict[str, Callable[[], Any]] = {
-             "VLLM_XLA_CACHE_PATH",
-             os.path.join(get_default_cache_root(), "vllm", "xla_cache"),
-         )),
--
--    # If set, assert on XLA recompilation after each execution step.
--    "VLLM_XLA_CHECK_RECOMPILATION":
--    lambda: bool(int(os.getenv("VLLM_XLA_CHECK_RECOMPILATION", "0"))),
-     "VLLM_FUSED_MOE_CHUNK_SIZE":
-     lambda: int(os.getenv("VLLM_FUSED_MOE_CHUNK_SIZE", "32768")),
- 
-@@ -531,17 +524,13 @@ environment_variables: dict[str, Callable[[], Any]] = {
-     # Pad the fp8 weights to 256 bytes for ROCm
-     "VLLM_ROCM_FP8_PADDING":
-     lambda: bool(int(os.getenv("VLLM_ROCM_FP8_PADDING", "1"))),
--
--    # Divisor for dynamic query scale factor calculation for FP8 KV Cache
--    "Q_SCALE_CONSTANT":
--    lambda: int(os.getenv("Q_SCALE_CONSTANT", "200")),
-     # Divisor for dynamic key scale factor calculation for FP8 KV Cache
-     "K_SCALE_CONSTANT":
-     lambda: int(os.getenv("K_SCALE_CONSTANT", "200")),
-+
-     # Divisor for dynamic value scale factor calculation for FP8 KV Cache
-     "V_SCALE_CONSTANT":
-     lambda: int(os.getenv("V_SCALE_CONSTANT", "100")),
--
-     # If set, enable multiprocessing in LLM for the V1 code path.
-     "VLLM_ENABLE_V1_MULTIPROCESSING":
-     lambda: bool(int(os.getenv("VLLM_ENABLE_V1_MULTIPROCESSING", "1"))),
-@@ -629,11 +618,6 @@ environment_variables: dict[str, Callable[[], Any]] = {
-     # an environment with potentially malicious users.
-     "VLLM_V0_USE_OUTLINES_CACHE":
-     lambda: os.environ.get("VLLM_V0_USE_OUTLINES_CACHE", "0") == "1",
--
--    # If set, disables TPU-specific optimization for top-k & top-p sampling
--    "VLLM_TPU_DISABLE_TOPK_TOPP_OPTIMIZATION":
--    lambda: bool(int(os.environ["VLLM_TPU_DISABLE_TOPK_TOPP_OPTIMIZATION"]))
--    if "VLLM_TPU_DISABLE_TOPK_TOPP_OPTIMIZATION" in os.environ else None,
- }
- 
- # end-env-vars-definition
-diff --git a/vllm/executor/multiproc_worker_utils.py b/vllm/executor/multiproc_worker_utils.py
-index 380b672c..a4a5b3f9 100644
---- a/vllm/executor/multiproc_worker_utils.py
-+++ b/vllm/executor/multiproc_worker_utils.py
-@@ -16,7 +16,7 @@ import torch
- 
- from vllm.config import VllmConfig
- from vllm.logger import init_logger
--from vllm.utils import _maybe_force_spawn, get_mp_context, run_method
-+from vllm.utils import _check_multiproc_method, get_mp_context, run_method
- 
- logger = init_logger(__name__)
- 
-@@ -291,7 +291,7 @@ def set_multiprocessing_worker_envs(parallel_config):
-     in a multiprocessing environment. This should be called by the parent 
-     process before worker processes are created"""
- 
--    _maybe_force_spawn()
-+    _check_multiproc_method()
- 
-     # Configure thread parallelism if OMP_NUM_THREADS isn't set
-     #
-diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
-index d769d235..18ff3215 100644
---- a/vllm/executor/ray_distributed_executor.py
-+++ b/vllm/executor/ray_distributed_executor.py
-@@ -340,8 +340,6 @@ class RayDistributedExecutor(DistributedExecutorBase):
-             and v not in self.non_carry_over_env_vars
-         ]
- 
--        env_vars_to_copy.extend(current_platform.additional_env_vars)
--
-         # Copy existing env vars to each worker's args
-         for args in all_args_to_update_environment_variables:
-             # TODO: refactor platform-specific env vars
-diff --git a/vllm/executor/ray_utils.py b/vllm/executor/ray_utils.py
-index b7222f26..c1bf2fb3 100644
---- a/vllm/executor/ray_utils.py
-+++ b/vllm/executor/ray_utils.py
-@@ -17,7 +17,7 @@ from vllm.utils import get_ip
- from vllm.worker.worker_base import WorkerWrapperBase
- 
- if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler import SchedulerOutput
-     from vllm.v1.outputs import ModelRunnerOutput
- 
- logger = init_logger(__name__)
-@@ -284,9 +284,8 @@ def initialize_ray_cluster(
-     assert_ray_available()
-     from vllm.platforms import current_platform
- 
--    if ray.is_initialized():
--        logger.info("Ray is already initialized. Skipping Ray initialization.")
--    elif current_platform.is_rocm() or current_platform.is_xpu():
-+    # Connect to a ray cluster.
-+    if current_platform.is_rocm() or current_platform.is_xpu():
-         # Try to connect existing ray instance and create a new one if not found
-         try:
-             ray.init("auto", ignore_reinit_error=True)
-@@ -300,21 +299,19 @@ def initialize_ray_cluster(
-     else:
-         ray.init(address=ray_address, ignore_reinit_error=True)
- 
-+    if parallel_config.placement_group:
-+        # Placement group is already set.
-+        return
-+
-     device_str = current_platform.ray_device_key
-     if not device_str:
-         raise ValueError(
-             f"current platform {current_platform.device_name} does not "
-             "support ray.")
- 
--    # Create or get the placement group for worker processes
--    if parallel_config.placement_group:
--        current_placement_group = parallel_config.placement_group
--    else:
--        current_placement_group = ray.util.get_current_placement_group()
--
-+    # Create placement group for worker processes
-+    current_placement_group = ray.util.get_current_placement_group()
-     if current_placement_group:
--        logger.info("Using the existing placement group")
--
-         # We are in a placement group
-         bundles = current_placement_group.bundle_specs
-         # Verify that we can use the placement group.
-@@ -334,8 +331,6 @@ def initialize_ray_cluster(
-                 f"Required number of devices: {parallel_config.world_size}. "
-                 f"Total number of devices: {device_bundles}.")
-     else:
--        logger.info("No current placement group found. "
--                    "Creating a new placement group.")
-         num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)
-         # Log a warning message and delay resource allocation failure response.
-         # Avoid immediate rejection to allow user-initiated placement group
-diff --git a/vllm/fa_utils.py b/vllm/fa_utils.py
-deleted file mode 100644
-index 41765349..00000000
---- a/vllm/fa_utils.py
-+++ /dev/null
-@@ -1,48 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--from typing import Optional
--
--from vllm import envs
--from vllm.logger import init_logger
--
--logger = init_logger(__name__)
--
--
--def get_flash_attn_version(requires_alibi: bool = False) -> Optional[int]:
--    # import here to avoid circular dependencies
--    from vllm.platforms import current_platform
--    try:
--        from vllm.vllm_flash_attn.flash_attn_interface import (
--            fa_version_unsupported_reason, is_fa_version_supported)
--        device_capability = current_platform.get_device_capability()
--
--        assert device_capability is not None
--
--        # 1. default version depending on platform
--        fa_version = 3 if (device_capability.major == 9
--                           and is_fa_version_supported(3)) else 2
--
--        # 2. override if passed by environment
--        if envs.VLLM_FLASH_ATTN_VERSION is not None:
--            assert envs.VLLM_FLASH_ATTN_VERSION in [2, 3]
--            fa_version = envs.VLLM_FLASH_ATTN_VERSION
--
--        # 3. fallback for unsupported combinations
--        if device_capability.major == 10 and fa_version == 3:
--            logger.warning_once(
--                "Cannot use FA version 3 on Blackwell platform "
--                "defaulting to FA version 2.")
--            fa_version = 2
--
--        if requires_alibi and fa_version == 3:
--            logger.warning_once("Cannot use FA version 3 with ALiBi, "
--                                "defaulting to FA version 2.")
--            fa_version = 2
--
--        if not is_fa_version_supported(fa_version):
--            logger.error("Cannot use FA version %d is not supported due to %s",
--                         fa_version, fa_version_unsupported_reason(fa_version))
--
--        assert is_fa_version_supported(fa_version)
--        return fa_version
--    except (ImportError, AssertionError):
--        return None
-diff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py
-index 0c26a605..c21df044 100644
---- a/vllm/model_executor/guided_decoding/__init__.py
-+++ b/vllm/model_executor/guided_decoding/__init__.py
-@@ -79,12 +79,6 @@ def maybe_backend_fallback(
-                     "xgrammar does not support Lark grammars and the "
-                     "grammar failed to convert to GBNF.", "outlines")
- 
--        elif guided_params.json_object:
--            # https://github.com/mlc-ai/xgrammar/issues/256
--            fallback_or_error(guided_params,
--                              "xgrammar does not support json_object.",
--                              "guidance")
--
-         # If the xgrammar module cannot be imported successfully,
-         # we should still allow users to use guided decoding with a fallback.
-         elif not xgr_installed:
-@@ -94,9 +88,9 @@ def maybe_backend_fallback(
- 
-     if (guided_params.backend_name == "outlines"
-             and guided_params.json_object is not None):
--        # outlines doesn't support json_object, fallback to guidance
-+        # outlines doesn't support json_object, fallback to xgrammar
-         fallback_or_error(guided_params,
--                          "outlines does not support json_object.", "guidance")
-+                          "outlines does not support json_object.", "xgrammar")
- 
-     return guided_params
- 
-@@ -128,15 +122,10 @@ async def get_guided_decoding_logits_processor(
-             get_local_xgrammar_guided_decoding_logits_processor)
-         return get_local_xgrammar_guided_decoding_logits_processor(
-             guided_params, tokenizer, model_config, reasoner)
--    if guided_params.backend_name == 'guidance':
--        from vllm.model_executor.guided_decoding.guidance_decoding import (
--            get_local_guidance_guided_decoding_logits_processor)
--        return get_local_guidance_guided_decoding_logits_processor(
--            guided_params, tokenizer)
-+
-     raise ValueError(
-         f"Unknown guided decoding backend '{guided_params.backend}'. "
--        "Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar', 'guidance'"
--    )
-+        "Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'")
- 
- 
- def get_local_guided_decoding_logits_processor(
-@@ -166,13 +155,7 @@ def get_local_guided_decoding_logits_processor(
-             get_local_xgrammar_guided_decoding_logits_processor)
-         return get_local_xgrammar_guided_decoding_logits_processor(
-             guided_params, tokenizer, model_config, reasoner)
--    if guided_params.backend_name == 'guidance':
--        from vllm.model_executor.guided_decoding.guidance_decoding import (
--            get_local_guidance_guided_decoding_logits_processor)
--        return get_local_guidance_guided_decoding_logits_processor(
--            guided_params, tokenizer)
- 
-     raise ValueError(
-         f"Unknown guided decoding backend '{guided_params.backend}'. "
--        "Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar', 'guidance'"
--    )
-+        "Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'")
-diff --git a/vllm/model_executor/guided_decoding/guidance_decoding.py b/vllm/model_executor/guided_decoding/guidance_decoding.py
-deleted file mode 100644
-index d8675a14..00000000
---- a/vllm/model_executor/guided_decoding/guidance_decoding.py
-+++ /dev/null
-@@ -1,44 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--from re import escape as regex_escape
--
--import llguidance
--from transformers import PreTrainedTokenizerBase
--
--from vllm.model_executor.guided_decoding.guidance_logits_processors import (
--    GuidanceLogitsProcessor)
--from vllm.sampling_params import GuidedDecodingParams
--
--
--def get_local_guidance_guided_decoding_logits_processor(
--        guided_params: GuidedDecodingParams,
--        tokenizer: PreTrainedTokenizerBase) -> GuidanceLogitsProcessor:
--    """
--    Given an OpenAI-compatible request, check for guided decoding parameters
--    and get the necessary logits processor for the given guide.
--    """
--
--    grm = ""
--    if guided_params.json:
--        grm = llguidance.LLMatcher.grammar_from_json_schema(
--            guided_params.json,
--            overrides={"whitespace_pattern": guided_params.whitespace_pattern})
--    elif guided_params.json_object:
--        grm = llguidance.LLMatcher.grammar_from_json_schema(
--            '{"type": "object"}',
--            overrides={"whitespace_pattern": guided_params.whitespace_pattern})
--    elif guided_params.regex:
--        grm = llguidance.grammar_from("regex", guided_params.regex)
--    elif guided_params.choice:
--        # choice just uses regex
--        choices = (regex_escape(str(choice))
--                   for choice in guided_params.choice)
--        choices_regex = "(" + "|".join(choices) + ")"
--        grm = llguidance.grammar_from("regex", choices_regex)
--    elif guided_params.grammar:
--        # this supports Lark and GBNF
--        grm = llguidance.grammar_from("grammar", guided_params.grammar)
--
--    if grm:
--        return GuidanceLogitsProcessor(grm, tokenizer)
--
--    raise ValueError("Unknown guided decoding mode")
-diff --git a/vllm/model_executor/guided_decoding/guidance_logits_processors.py b/vllm/model_executor/guided_decoding/guidance_logits_processors.py
-deleted file mode 100644
-index 26fcafe3..00000000
---- a/vllm/model_executor/guided_decoding/guidance_logits_processors.py
-+++ /dev/null
-@@ -1,85 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--import os
--from typing import Any, List
--
--import llguidance
--import llguidance.hf
--import llguidance.torch
--import torch
--from transformers import PreTrainedTokenizerBase
--
--from vllm.logger import init_logger
--
--logger = init_logger(__name__)
--
--
--class GuidanceLogitsProcessor:
--    """Base Guidance Logits Processor"""
--
--    cached_tokenizers: dict[str, Any] = {}
--
--    def __init__(
--        self,
--        grammar: str,
--        tokenizer: PreTrainedTokenizerBase,
--    ) -> None:
--        """Base Guidance Logits Processor
--
--        Args:
--            grammar (str)
--                grammar to guide the generation
--            tokenizer (PreTrainedTokenizerBase)
--                model's tokenizer
--        """
--        self.grammar = grammar
--        self.tokenizer = tokenizer
--        self.tokenizer_name = tokenizer.name_or_path
--        self.new_sampling = False
--        self.initialized = False
--
--    def _initialize(self):
--        if self.initialized:
--            return
--
--        ll_tokenizer = self.cached_tokenizers.get(self.tokenizer.name_or_path,
--                                                  None)
--        if ll_tokenizer is None:
--            ll_tokenizer = llguidance.hf.from_tokenizer(self.tokenizer, None)
--            self.cached_tokenizers[self.tokenizer.name_or_path] = ll_tokenizer
--
--        self.ll_tokenizer = ll_tokenizer
--        self.ll_matcher = llguidance.LLMatcher(
--            self.ll_tokenizer,
--            self.grammar,
--            log_level=int(os.environ.get("LLGUIDANCE_LOG_LEVEL", "1")),
--        )
--
--        # create reusable bitmask
--        self.bitmask = llguidance.torch.allocate_token_bitmask(
--            1, self.ll_tokenizer.vocab_size)
--
--        self.initialized = True
--
--    def __call__(
--        self,
--        input_ids: List[int],
--        scores: torch.Tensor,
--    ) -> torch.Tensor:
--        # we initialize the guidance model here
--        # to avoid pickling ll_tokenizer and ll_interpreter
--        self._initialize()
--
--        if self.new_sampling and len(input_ids) > 0:
--            self.ll_matcher.consume_token(input_ids[-1])
--            err = self.ll_matcher.get_error()
--            if err:
--                logger.warning("Error in LLMatcher: %s", err)
--
--        llguidance.torch.fill_next_token_bitmask(self.ll_matcher, self.bitmask,
--                                                 0)
--        llguidance.torch.apply_token_bitmask_inplace(
--            scores, self.bitmask.to(scores.device))
--
--        self.new_sampling = True
--
--        return scores
-diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
-index d7a45bc5..53d68b60 100644
---- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
-+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
-@@ -251,9 +251,6 @@ class MambaMixer2(CustomOp):
-                 "then num_groups must equal 1."
-             )
- 
--        assert self.tp_size == 1 or quant_config is None, \
--            "Tensor parallel currently not supported for quantized models."
--
-         self.ssm_state_size = ssm_state_size
-         self.activation = activation
- 
-@@ -334,24 +331,22 @@ class MambaMixer2(CustomOp):
-                 ], self.tp_size, tp_rank)
-             })
- 
--        if quant_config is None:
--            # - quant layers do not have a weight loader
--            delattr(self.in_proj.weight, "weight_loader")
--            set_weight_attrs(
--                self.in_proj.weight,
--                {
--                    "weight_loader":
--                    mamba_v2_sharded_weight_loader(
--                        [
--                            intermediate_settings,  # for gate
--                            intermediate_settings,
--                            group_shard_settings,
--                            group_shard_settings,
--                            head_setings,  # for dt
--                        ],
--                        self.tp_size,
--                        tp_rank)
--                })
-+        delattr(self.in_proj.weight, "weight_loader")
-+        set_weight_attrs(
-+            self.in_proj.weight,
-+            {
-+                "weight_loader":
-+                mamba_v2_sharded_weight_loader(
-+                    [
-+                        intermediate_settings,  # for gate
-+                        intermediate_settings,
-+                        group_shard_settings,
-+                        group_shard_settings,
-+                        head_setings,  # for dt
-+                    ],
-+                    self.tp_size,
-+                    tp_rank)
-+            })
- 
-         # - these are TPed by heads to reduce the size of the
-         #   temporal shape
-@@ -470,11 +465,10 @@ class MambaMixer2(CustomOp):
-         if has_prefill:
- 
-             initial_states = None
--            if has_initial_states is not None and torch.any(
--                    has_initial_states):
--                zero_init_indices = mamba_cache_params.state_indices_tensor[
--                    ~has_initial_states]
--                mamba_cache_params.ssm_state[zero_init_indices] = 0
-+            if has_initial_states is not None and any(has_initial_states):
-+                for idx in mamba_cache_params.state_indices_tensor[
-+                        ~has_initial_states]:
-+                    mamba_cache_params.ssm_state[idx].zero_()
-                 initial_states = mamba_cache_params.ssm_state[
-                     mamba_cache_params.state_indices_tensor]
- 
-@@ -500,8 +494,8 @@ class MambaMixer2(CustomOp):
- 
-             # update ssm states
-             # - varlen state is a (batch, nheads, headdim, dstate) tensor
--            mamba_cache_params.ssm_state[
--                mamba_cache_params.state_indices_tensor] = varlen_state
-+            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):
-+                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])
- 
-             # - reshape
-             hidden_states = scan_output.view(seq_len, -1)
-diff --git a/vllm/model_executor/layers/quantization/kv_cache.py b/vllm/model_executor/layers/quantization/kv_cache.py
-index 5d766c2c..92990487 100644
---- a/vllm/model_executor/layers/quantization/kv_cache.py
-+++ b/vllm/model_executor/layers/quantization/kv_cache.py
-@@ -26,14 +26,11 @@ class BaseKVCacheMethod(QuantizeMethodBase):
- 
-     def create_weights(self, layer: torch.nn.Module):
-         """
--        Create "weight" (aka q_scale, k_scale and v_scale)
--        for an attention layer.
-+        Create "weight" (aka k_scale and v_scale) for an attention layer.
-         """
--        # Initialize the Q and KV cache scales to -1.0, an invalid value.
--        # If the q and k/v_scales appear in the checkpoint, it will be
-+        # Initialize the KV cache scales to -1.0, which is an invalid value.
-+        # If the k/v_scale appears in the checkpoint, it will be
-         # overwritten when loading weights.
--        layer.q_scale = torch.nn.Parameter(torch.tensor(-1.0),
--                                           requires_grad=False)
-         layer.k_scale = torch.nn.Parameter(torch.tensor(-1.0),
-                                            requires_grad=False)
-         layer.v_scale = torch.nn.Parameter(torch.tensor(-1.0),
-@@ -78,13 +75,6 @@ class BaseKVCacheMethod(QuantizeMethodBase):
-                 raise ValueError("Only support per-tensor scaling factor "
-                                  "for fp8 KV cache")
- 
--            if layer.q_scale < 0.0:
--                logger.warning_once(
--                    "Checkpoint does not provide a q scaling factor. "
--                    "Setting it to k_scale. This only matters for "
--                    "the flash-attn backend.")
--                layer._q_scale.copy_(k_scale)
--
-             # These are used in the final Attention.forward()
-             layer._k_scale.copy_(k_scale)
-             layer._v_scale.copy_(v_scale)
-diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py
-index fcab533e..d66f61a8 100644
---- a/vllm/model_executor/models/deepseek_v2.py
-+++ b/vllm/model_executor/models/deepseek_v2.py
-@@ -589,7 +589,6 @@ class DeepseekV2Model(nn.Module):
-         model_config = vllm_config.model_config
-         cache_config = vllm_config.cache_config
-         quant_config = vllm_config.quant_config
--        self.config = config
- 
-         self.vocab_size = config.vocab_size
- 
-diff --git a/vllm/model_executor/models/gemma3_mm.py b/vllm/model_executor/models/gemma3_mm.py
-index 8db2bfb9..62e55d64 100644
---- a/vllm/model_executor/models/gemma3_mm.py
-+++ b/vllm/model_executor/models/gemma3_mm.py
-@@ -183,7 +183,7 @@ class Gemma3ProcessingInfo(BaseProcessingInfo):
-         image_width: int,
-         image_height: int,
-         processor: Optional[Gemma3Processor],
--    ) -> PromptUpdateDetails[str]:
-+    ) -> PromptUpdateDetails:
-         if processor is None:
-             processor = self.get_hf_processor()
- 
-diff --git a/vllm/model_executor/models/h2ovl.py b/vllm/model_executor/models/h2ovl.py
-index 3b2ad695..e23765cc 100644
---- a/vllm/model_executor/models/h2ovl.py
-+++ b/vllm/model_executor/models/h2ovl.py
-@@ -249,15 +249,20 @@ class H2OVLProcessor(BaseInternVLProcessor):
-     def image_token_id(self) -> int:
-         return self.tokenizer.get_vocab()[IMG_CONTEXT]
- 
--    def get_image_repl(
-+    def get_image_repl_features(
-         self,
-         feature_size: int,
-         num_patches: Optional[int],
--    ) -> PromptUpdateDetails[str]:
--        repl_features = IMG_CONTEXT * feature_size
--        repl_full = IMG_START + repl_features + IMG_END
-+    ) -> str:
-+        return IMG_CONTEXT * feature_size
- 
--        return PromptUpdateDetails(full=repl_full, features=repl_features)
-+    def get_image_repl_full(
-+        self,
-+        feature_size: int,
-+        num_patches: Optional[int],
-+    ) -> str:
-+        features = self.get_image_repl_features(feature_size, num_patches)
-+        return IMG_START + features + IMG_END
- 
-     def resolve_min_max_num(
-         self,
-@@ -496,7 +501,12 @@ class H2OVLMultiModalProcessor(InternVLMultiModalProcessor[H2OVLProcessingInfo]
-             if num_patches is not None:
-                 assert isinstance(num_patches, int)
- 
--            return hf_processor.get_image_repl(feature_size, num_patches)
-+            return PromptUpdateDetails(
-+                full=hf_processor.get_image_repl_full(feature_size,
-+                                                      num_patches),
-+                features=hf_processor.get_image_repl_features(
-+                    feature_size, num_patches),
-+            )
- 
-         return [
-             PromptReplacement(
-diff --git a/vllm/model_executor/models/internvl.py b/vllm/model_executor/models/internvl.py
-index e8ec9173..d31b623b 100644
---- a/vllm/model_executor/models/internvl.py
-+++ b/vllm/model_executor/models/internvl.py
-@@ -9,13 +9,14 @@
- from abc import ABC, abstractmethod
- from collections.abc import Iterable, Mapping, Sequence
- from functools import cached_property
--from typing import Literal, Optional, Set, Tuple, TypedDict, TypeVar, Union
-+from typing import (List, Literal, Optional, Set, Tuple, TypedDict, TypeVar,
-+                    Union)
- 
- import torch
- import torch.nn as nn
- import torchvision.transforms as T
- from PIL import Image
--from transformers import BatchEncoding, PretrainedConfig, TensorType
-+from transformers import BatchFeature, PretrainedConfig, TensorType
- 
- from vllm.config import VllmConfig
- from vllm.model_executor.layers.quantization import QuantizationConfig
-@@ -35,12 +36,10 @@ from vllm.multimodal.processing import (BaseMultiModalProcessor,
- from vllm.multimodal.profiling import BaseDummyInputsBuilder, ProcessorInputs
- from vllm.sequence import IntermediateTensors
- from vllm.transformers_utils.tokenizer import AnyTokenizer
--from vllm.utils import flatten_2d_lists
- 
- from .interfaces import MultiModalEmbeddings, SupportsMultiModal, SupportsPP
- from .utils import (AutoWeightsLoader, flatten_bn, init_vllm_registered_model,
-                     maybe_prefix, merge_multimodal_embeddings)
--from .vision import scatter_patch_features, select_patch_features
- 
- IMG_START = '<img>'
- IMG_END = '</img>'
-@@ -52,26 +51,16 @@ IMAGENET_STD = (0.229, 0.224, 0.225)
- 
- class InternVLImagePixelInputs(TypedDict):
-     type: Literal["pixel_values"]
--    pixel_values_flat: torch.Tensor
-+    data: torch.Tensor
-     """
-     Shape:
-     `(batch_size * num_images * (1 + num_patches), num_channels, height, width)`
-     """
--
--    num_patches: torch.Tensor
--    """Shape: `(batch_size * num_images)`"""
--
--    embed_is_patch: Union[torch.Tensor, list[torch.Tensor]]
-+    patches_per_image: List[int]
-     """
--    A boolean mask indicating which image embeddings correspond
--    to patch tokens.
--
--    Shape: `(batch_size, num_images, num_embeds)`
-+    List of number of total patches for each image in the batch.
-     """
- 
--    num_embeds: Union[torch.Tensor, list[torch.Tensor]]
--    """Shape: `(batch_size, num_images)`"""
--
- 
- class InternVLImageEmbeddingInputs(TypedDict):
-     type: Literal["image_embeds"]
-@@ -297,11 +286,19 @@ class BaseInternVLProcessor(ABC):
-         raise NotImplementedError
- 
-     @abstractmethod
--    def get_image_repl(
-+    def get_image_repl_features(
-         self,
-         feature_size: int,
-         num_patches: Optional[int],
--    ) -> PromptUpdateDetails[str]:
-+    ) -> str:
-+        raise NotImplementedError
-+
-+    @abstractmethod
-+    def get_image_repl_full(
-+        self,
-+        feature_size: int,
-+        num_patches: Optional[int],
-+    ) -> str:
-         raise NotImplementedError
- 
-     def resolve_min_max_num(
-@@ -397,7 +394,7 @@ class BaseInternVLProcessor(ABC):
-         max_dynamic_patch: Optional[int] = None,
-         dynamic_image_size: Optional[bool] = None,
-         return_tensors: Optional[Union[str, TensorType]] = None,
--    ) -> Mapping[str, NestedTensors]:
-+    ) -> BatchFeature:
-         if text is None:
-             text = []
-         if not isinstance(text, list):
-@@ -416,41 +413,28 @@ class BaseInternVLProcessor(ABC):
-                 max_dynamic_patch=max_dynamic_patch,
-                 dynamic_image_size=dynamic_image_size,
-             )
--            image_inputs: dict[str, NestedTensors] = {
--                "pixel_values_flat":
--                torch.cat(pixel_values_lst),
--                "image_num_patches":
--                torch.tensor([len(item) for item in pixel_values_lst]),
-+            image_inputs = {
-+                "pixel_values_flat": torch.cat(pixel_values_lst),
-+                "image_num_patches": list(map(len, pixel_values_lst)),
-             }
- 
--            tokenizer = self.tokenizer
--            image_token_id = self.image_token_id
--
--            num_embeds = list[int]()
--            embed_is_patch = list[torch.Tensor]()
--
-             for pixel_values in pixel_values_lst:
-                 num_patches = pixel_values.shape[0]
-                 feature_size = num_patches * self.num_image_token
- 
--                image_repl = self.get_image_repl(feature_size, num_patches)
--                feature_tokens = tokenizer.encode(image_repl.features,
--                                                  add_special_tokens=False)
--
--                text = [t.replace('<image>', image_repl.full, 1) for t in text]
--                num_embeds.append(len(feature_tokens))
--                embed_is_patch.append(
--                    torch.tensor(feature_tokens) == image_token_id)
--
--            image_inputs["num_embeds"] = torch.tensor(num_embeds)
--            image_inputs["embed_is_patch"] = embed_is_patch
-+                image_repl = self.get_image_repl_full(feature_size,
-+                                                      num_patches)
-+                text = [t.replace('<image>', image_repl, 1) for t in text]
- 
-         text_inputs = self.tokenizer(text)
- 
--        return {
--            **BatchEncoding(text_inputs, tensor_type=return_tensors),
--            **image_inputs,
--        }
-+        return BatchFeature(
-+            {
-+                **text_inputs,
-+                **image_inputs,
-+            },
-+            tensor_type=return_tensors,
-+        )
- 
- 
- class InternVLProcessor(BaseInternVLProcessor):
-@@ -459,15 +443,20 @@ class InternVLProcessor(BaseInternVLProcessor):
-     def image_token_id(self) -> int:
-         return self.tokenizer.get_vocab()[IMG_CONTEXT]
- 
--    def get_image_repl(
-+    def get_image_repl_features(
-         self,
-         feature_size: int,
-         num_patches: Optional[int],
--    ) -> PromptUpdateDetails[str]:
--        repl_features = IMG_CONTEXT * feature_size
--        repl_full = IMG_START + repl_features + IMG_END
-+    ) -> str:
-+        return IMG_CONTEXT * feature_size
- 
--        return PromptUpdateDetails(full=repl_full, features=repl_features)
-+    def get_image_repl_full(
-+        self,
-+        feature_size: int,
-+        num_patches: Optional[int],
-+    ) -> str:
-+        features = self.get_image_repl_features(feature_size, num_patches)
-+        return IMG_START + features + IMG_END
- 
- 
- class BaseInternVLProcessingInfo(BaseProcessingInfo):
-@@ -577,15 +566,16 @@ class InternVLMultiModalProcessor(BaseMultiModalProcessor[_I]):
-         prompt: str,
-         mm_data: Mapping[str, object],
-         mm_kwargs: Mapping[str, object],
--    ) -> Mapping[str, NestedTensors]:
-+    ) -> BatchFeature:
-         processed_outputs = super()._call_hf_processor(
-             prompt=prompt,
-             mm_data=mm_data,
-             mm_kwargs=mm_kwargs,
-         )
- 
--        hf_processor = self.info.get_hf_processor(**mm_kwargs)
--        image_token_id = hf_processor.image_token_id
-+        image_token_id = self.info.get_hf_processor(**mm_kwargs).image_token_id
-+        image_data = mm_data.get("images", [])
-+        assert isinstance(image_data, list)
- 
-         # Since there may be extra tokens in the feature placeholders,
-         # we need to pass the image token ID to the model to select the
-@@ -596,7 +586,7 @@ class InternVLMultiModalProcessor(BaseMultiModalProcessor[_I]):
- 
-     def _get_mm_fields_config(
-         self,
--        hf_inputs: Mapping[str, NestedTensors],
-+        hf_inputs: BatchFeature,
-         hf_processor_mm_kwargs: Mapping[str, object],
-     ) -> Mapping[str, MultiModalFieldConfig]:
-         image_num_patches = hf_inputs.get("image_num_patches", torch.empty(0))
-@@ -606,8 +596,6 @@ class InternVLMultiModalProcessor(BaseMultiModalProcessor[_I]):
-             pixel_values_flat=MultiModalFieldConfig.flat_from_sizes(
-                 "image", image_num_patches),
-             image_num_patches=MultiModalFieldConfig.batched("image"),
--            embed_is_patch=MultiModalFieldConfig.batched("image"),
--            num_embeds=MultiModalFieldConfig.batched("image"),
-             image_embeds=MultiModalFieldConfig.batched("image"),
-             image_token_id=MultiModalFieldConfig.shared("image", num_images),
-         )
-@@ -649,7 +637,12 @@ class InternVLMultiModalProcessor(BaseMultiModalProcessor[_I]):
-             if num_patches is not None:
-                 assert isinstance(num_patches, int)
- 
--            return hf_processor.get_image_repl(feature_size, num_patches)
-+            return PromptUpdateDetails(
-+                full=hf_processor.get_image_repl_full(feature_size,
-+                                                      num_patches),
-+                features=hf_processor.get_image_repl_features(
-+                    feature_size, num_patches),
-+            )
- 
-         return [
-             PromptReplacement(
-@@ -839,8 +832,6 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
-             self, **kwargs: object) -> Optional[InternVLImageInputs]:
-         pixel_values_flat = kwargs.pop("pixel_values_flat", None)
-         image_num_patches = kwargs.pop("image_num_patches", None)
--        embed_is_patch = kwargs.pop("embed_is_patch", None)
--        num_embeds = kwargs.pop("num_embeds", None)
-         image_embeds = kwargs.pop("image_embeds", None)
- 
-         if pixel_values_flat is None and image_embeds is None:
-@@ -867,47 +858,35 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
- 
-             if not isinstance(image_num_patches, (torch.Tensor, list)):
-                 raise ValueError("Incorrect type of image_num_patches. "
--                                 f"Got type: {type(image_num_patches)}")
--
--            if not isinstance(embed_is_patch, (torch.Tensor, list)):
--                raise ValueError("Incorrect type of embed_is_patch. "
--                                 f"Got type: {type(embed_is_patch)}")
--
--            if not isinstance(num_embeds, (torch.Tensor, list)):
--                raise ValueError("Incorrect type of num_embeds. "
--                                 f"Got type: {type(num_embeds)}")
--
--            pixel_values_flat = flatten_bn(pixel_values_flat, concat=True)
--            image_num_patches = flatten_bn(image_num_patches, concat=True)
-+                                 f"Got type: {type(pixel_values_flat)}")
- 
-             return InternVLImagePixelInputs(
-                 type="pixel_values",
--                pixel_values_flat=self._validate_pixel_values(
--                    pixel_values_flat),
--                num_patches=image_num_patches,
--                embed_is_patch=embed_is_patch,
--                num_embeds=num_embeds,
--            )
-+                data=self._validate_pixel_values(
-+                    flatten_bn(pixel_values_flat, concat=True)),
-+                patches_per_image=flatten_bn(image_num_patches,
-+                                             concat=True).tolist())
- 
-         raise AssertionError("This line should be unreachable.")
- 
-     def _process_image_input(
-         self,
-         image_input: InternVLImageInputs,
--    ) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]:
-+    ) -> tuple[torch.Tensor, ...]:
-         if image_input["type"] == "image_embeds":
-             return image_input["data"]
- 
-         assert self.vision_model is not None
- 
--        image_embeds = self.extract_feature(image_input["pixel_values_flat"])
-+        image_embeds = self.extract_feature(image_input["data"])
- 
--        num_patches = image_input["num_patches"]
-+        patches_per_image = image_input["patches_per_image"]
- 
-         # Only one image in the current batch
--        if len(num_patches) == 1:
--            return image_embeds.view(
-+        if len(patches_per_image) == 1:
-+            image_embeds = image_embeds.view(
-                 -1, self.config.text_config.hidden_size).unsqueeze(0)
-+            return image_embeds
- 
-         # NOTE: Image embeddings are split into separate tensors for each image
-         # by the size of each embedding.
-@@ -915,9 +894,10 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
-         image_embeds = image_embeds.view(-1,
-                                          self.config.text_config.hidden_size)
-         image_feature_sizes = [
--            num_patches * feature_size for num_patches in num_patches
-+            num_patches * feature_size for num_patches in patches_per_image
-         ]
--        return image_embeds.split(image_feature_sizes)
-+        image_embeds = image_embeds.split(image_feature_sizes)
-+        return image_embeds
- 
-     def _set_visual_token_mask(self, input_ids: torch.Tensor) -> None:
-         if self.is_mono:
-@@ -931,19 +911,8 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
-         image_input = self._parse_and_validate_image_input(**kwargs)
-         if image_input is None:
-             return None
--
--        image_features = self._process_image_input(image_input)
--
--        if (kwargs.get("v0_path", False)
--                or image_input["type"] != "pixel_values"):
--            return image_features
--
--        return flatten_2d_lists(
--            scatter_patch_features(*args) for args in zip(
--                image_features,
--                image_input["num_embeds"],
--                image_input["embed_is_patch"],
--            ))
-+        vision_embeddings = self._process_image_input(image_input)
-+        return vision_embeddings
- 
-     def get_input_embeddings(
-         self,
-@@ -955,11 +924,8 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
-             assert self.img_context_token_id is not None
-             self._set_visual_token_mask(input_ids)
-             inputs_embeds = merge_multimodal_embeddings(
--                input_ids,
--                inputs_embeds,
--                select_patch_features(multimodal_embeddings),
--                self.img_context_token_id,
--            )
-+                input_ids, inputs_embeds, multimodal_embeddings,
-+                self.img_context_token_id)
-         return inputs_embeds
- 
-     def forward(
-@@ -978,7 +944,6 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP):
-         # NOTE: In v1, inputs_embeds is always generated at model runner, this
-         # condition is for v0 compatibility.
-         elif inputs_embeds is None:
--            kwargs.update({"v0_path": True})
-             vision_embeddings = self.get_multimodal_embeddings(**kwargs)
-             inputs_embeds = self.get_input_embeddings(input_ids,
-                                                       vision_embeddings)
-diff --git a/vllm/model_executor/models/llava_onevision.py b/vllm/model_executor/models/llava_onevision.py
-index fbc298b8..6a2328f9 100644
---- a/vllm/model_executor/models/llava_onevision.py
-+++ b/vllm/model_executor/models/llava_onevision.py
-@@ -25,6 +25,7 @@ from vllm.multimodal.parse import (ImageSize, MultiModalDataItems,
- from vllm.multimodal.processing import PromptReplacement, PromptUpdate
- from vllm.multimodal.profiling import ProcessorInputs
- from vllm.sequence import IntermediateTensors
-+from vllm.utils import is_list_of
- 
- from .clip import CLIPVisionModel
- from .interfaces import MultiModalEmbeddings, SupportsMultiModal, SupportsPP
-@@ -43,7 +44,7 @@ class LlavaOnevisionVideoPixelInputs(TypedDict):
-     type: Literal["pixel_values_videos"]
-     pixel_values_videos: Union[torch.Tensor, list[torch.Tensor]]
-     """
--    Shape: `(batch_size * num_videos, num_frames, num_channels, height, width)`
-+    Shape: `(batch_size, num_videos, num_frames, num_channels, height, width)`
- 
-     Note that `num_videos` may be different for each batch, and 'num_frames'
-     may be different for each video, in which case the data is passed as a
-@@ -579,7 +580,7 @@ class LlavaOnevisionForConditionalGeneration(nn.Module, SupportsMultiModal,
- 
-         return LlavaOnevisionVideoPixelInputs(
-             type="pixel_values_videos",
--            pixel_values_videos=flatten_bn(pixel_values_videos),
-+            pixel_values_videos=pixel_values_videos,
-         )
- 
-     def _parse_and_validate_multimodal_inputs(self, **kwargs: object) -> dict:
-@@ -767,6 +768,22 @@ class LlavaOnevisionForConditionalGeneration(nn.Module, SupportsMultiModal,
-             for i, patch_features_batch in enumerate(patch_embeddings)
-         ]
- 
-+    def _add_image_newline(
-+        self,
-+        video_features: torch.Tensor,
-+        videos: int = 1,
-+        frames: int = 1,
-+        strategy: str = "one_token",
-+    ) -> torch.Tensor:
-+        if strategy == "one_token":
-+            video_features = video_features.reshape(
-+                videos, frames * video_features.shape[1], -1)
-+            image_newline = self.image_newline[None, None, :].repeat(
-+                videos, 1, 1).to(video_features.device)
-+            video_features = torch.cat((video_features, image_newline), dim=1)
-+            return video_features
-+        raise ValueError(f"Unexpected video newline strategy: {strategy}")
-+
-     def _video_pixels_to_features(
-         self,
-         vision_tower: Union[CLIPVisionModel, SiglipVisionModel],
-@@ -790,43 +807,33 @@ class LlavaOnevisionForConditionalGeneration(nn.Module, SupportsMultiModal,
-         video_pixels = inputs["pixel_values_videos"]
- 
-         if isinstance(video_pixels, torch.Tensor):
--            total_videos, frames, c, h, w = video_pixels.shape
--            video_pixels_flat = video_pixels.view(total_videos * frames, c, h,
--                                                  w)
--
--            embeddings_flat = self._video_pixels_to_features(
--                self.vision_tower, video_pixels_flat)
--
--            embeddings_flat = embeddings_flat.reshape(
--                total_videos, frames * embeddings_flat.shape[1], -1)
--
--            image_newline = self.image_newline[None, None, :].expand(
--                total_videos, -1, -1)
--            return torch.cat((embeddings_flat, image_newline), dim=1)
--
--        frames_per_video = [len(video) for video in video_pixels]
--        video_pixels_flat = torch.cat(video_pixels)
--
--        embeddings_flat = self._video_pixels_to_features(
--            self.vision_tower, video_pixels_flat)
--
--        image_newline = self.image_newline[None, None, :]
--
--        return [
--            torch.cat(
--                (
--                    embeds.reshape(1, num_frame * embeddings_flat.shape[1],
--                                   -1),
--                    image_newline,
--                ),
--                dim=1,
--            ) for num_frame, embeds in zip(
--                frames_per_video,
--                torch.split(embeddings_flat, frames_per_video),
--            )
--        ]
-+            b, num_videos, frames, c, h, w = video_pixels.shape
-+            pixel_values = video_pixels.view(b * num_videos * frames, c, h, w)
-+            stacked_embeddings = self._video_pixels_to_features(
-+                self.vision_tower, pixel_values)
-+            stacked_embeddings = self._add_image_newline(stacked_embeddings,
-+                                                         videos=b * num_videos,
-+                                                         frames=frames,
-+                                                         strategy="one_token")
-+            return stacked_embeddings
-+        elif is_list_of(video_pixels, torch.Tensor):
-+            stacked_embeddings = []
-+            for video_pixel in video_pixels:
-+                num_videos, frames, c, h, w = video_pixel.shape
-+                pixel_values = video_pixel.view(num_videos * frames, c, h, w)
-+                embeddings = self._video_pixels_to_features(
-+                    self.vision_tower, pixel_values)
-+                embeddings = self._add_image_newline(embeddings,
-+                                                     videos=num_videos,
-+                                                     frames=frames,
-+                                                     strategy="one_token")
-+                stacked_embeddings.append(embeddings)
-+            return stacked_embeddings
-+        else:
-+            raise ValueError(
-+                f"Unsupported type of video input {type(video_pixels)}")
- 
--    def apply_pooling(self, image_features: torch.Tensor, stride: int = 2):
-+    def apply_pooling(self, image_features, stride=2):
-         vision_config = self.config.vision_config
-         height = width = vision_config.image_size // vision_config.patch_size
-         batch_frames, _, dim = image_features.shape
-diff --git a/vllm/model_executor/models/mllama.py b/vllm/model_executor/models/mllama.py
-index 9ed49597..8e98eb27 100644
---- a/vllm/model_executor/models/mllama.py
-+++ b/vllm/model_executor/models/mllama.py
-@@ -1368,7 +1368,7 @@ class MllamaForConditionalGeneration(nn.Module, SupportsMultiModal,
-             full_text_row_masked_out_mask = (
-                 attn_metadata.encoder_seq_lens_tensor
-                 != 0).reshape(-1, 1).to(input_ids.device)
--            skip_cross_attention = attn_metadata.max_encoder_seq_len == 0
-+            skip_cross_attention = max(attn_metadata.encoder_seq_lens) == 0
- 
-         # For image-present prefill.
-         else:
-diff --git a/vllm/model_executor/models/nvlm_d.py b/vllm/model_executor/models/nvlm_d.py
-index 9d04f30c..0f5cbf08 100644
---- a/vllm/model_executor/models/nvlm_d.py
-+++ b/vllm/model_executor/models/nvlm_d.py
-@@ -36,11 +36,11 @@ class NVLMProcessor(BaseInternVLProcessor):
-     def image_token_id(self) -> int:
-         return self.tokenizer.get_vocab()[IMG_PAD]
- 
--    def get_image_repl(
-+    def get_image_repl_features(
-         self,
-         feature_size: int,
-         num_patches: Optional[int],
--    ) -> PromptUpdateDetails[str]:
-+    ) -> str:
-         if num_patches is None:
-             raise NotImplementedError("Embedding inputs are not supported")
- 
-@@ -55,9 +55,14 @@ class NVLMProcessor(BaseInternVLProcessor):
-         # We include the start and end as well because "<Image><tile" is
-         # tokenized as ["<Image", "><", "tile"], resulting in assertion error
-         # when trying to find "<tile" as a subsequence of "<Image><tile"
--        repl = "<Image>" + features + "</Image>"
-+        return "<Image>" + features + "</Image>"
- 
--        return PromptUpdateDetails(full=repl, features=repl)
-+    def get_image_repl_full(
-+        self,
-+        feature_size: int,
-+        num_patches: Optional[int],
-+    ) -> str:
-+        return self.get_image_repl_features(feature_size, num_patches)
- 
- 
- class NVLMProcessingInfo(BaseInternVLProcessingInfo):
-@@ -175,11 +180,11 @@ class NVLMMultiModalProcessor(InternVLMultiModalProcessor[NVLMProcessingInfo]):
-             if num_patches is not None:
-                 assert isinstance(num_patches, int)
- 
--            repl = hf_processor.get_image_repl(feature_size, num_patches)
--
-             return PromptUpdateDetails(
--                full=repl.full + "\n",
--                features=repl.features + "\n",
-+                full=hf_processor.get_image_repl_full(feature_size,
-+                                                      num_patches) + "\n",
-+                features=hf_processor.get_image_repl_features(
-+                    feature_size, num_patches) + "\n",
-             )
- 
-         # See note in dummy data regarding why we have the extra newline
-diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
-index adca97c7..8a570d13 100644
---- a/vllm/model_executor/models/qwen2_5_vl.py
-+++ b/vllm/model_executor/models/qwen2_5_vl.py
-@@ -647,17 +647,15 @@ class Qwen2_5_VisionTransformer(nn.Module):
- 
-         max_seqlen = None
-         seqlens = None
-+        if self.attn_backend == _Backend.FLASH_ATTN:
-+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
-+        elif self.attn_backend == _Backend.XFORMERS:
-+            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
-         for layer_num, blk in enumerate(self.blocks):
-             if layer_num in self.fullatt_block_indexes:
-                 cu_seqlens_now = cu_seqlens
-             else:
-                 cu_seqlens_now = cu_window_seqlens
--            # pre-compute cu_seqlens for window attn
--            if self.attn_backend == _Backend.FLASH_ATTN:
--                max_seqlen = (cu_seqlens_now[1:] -
--                              cu_seqlens_now[:-1]).max().item()
--            elif self.attn_backend == _Backend.XFORMERS:
--                seqlens = (cu_seqlens_now[1:] - cu_seqlens_now[:-1]).tolist()
-             hidden_states = blk(
-                 hidden_states,
-                 cu_seqlens=cu_seqlens_now,
-diff --git a/vllm/multimodal/processing.py b/vllm/multimodal/processing.py
-index fec77acc..db995957 100644
---- a/vllm/multimodal/processing.py
-+++ b/vllm/multimodal/processing.py
-@@ -103,13 +103,13 @@ The token sequence or text to update.
- 
- 
- @dataclass
--class PromptUpdateDetails(Generic[_S]):
-+class PromptUpdateDetails:
-     """Details about the token sequence or text that are part of the update."""
- 
--    full: _S
-+    full: PromptSeq
-     """The full content."""
- 
--    features: _S
-+    features: PromptSeq
-     """
-     The part of the content that corresponds to feature placeholders;
-     this will be replaced by the output of the vision encoder during model
-@@ -117,7 +117,7 @@ class PromptUpdateDetails(Generic[_S]):
-     """
- 
-     @staticmethod
--    def from_seq(seq: _S) -> "PromptUpdateDetails[_S]":
-+    def from_seq(seq: PromptSeq) -> "PromptUpdateDetails":
-         return PromptUpdateDetails(full=seq, features=seq)
- 
- 
-diff --git a/vllm/platforms/cuda.py b/vllm/platforms/cuda.py
-index 38d8fffd..8a53337e 100644
---- a/vllm/platforms/cuda.py
-+++ b/vllm/platforms/cuda.py
-@@ -14,7 +14,6 @@ from typing_extensions import ParamSpec
- # import custom ops, trigger op registration
- import vllm._C  # noqa
- import vllm.envs as envs
--from vllm.fa_utils import get_flash_attn_version
- from vllm.logger import init_logger
- from vllm.utils import import_pynvml
- 
-@@ -213,14 +212,9 @@ class CudaPlatformBase(Platform):
-                         return ("vllm.attention.backends."
-                                 "flashmla.FlashMLABackend")
-         if use_v1:
--            if selected_backend == _Backend.TRITON_ATTN_VLLM_V1:
--                logger.info_once("Using Triton backend on V1 engine.")
--                return ("vllm.v1.attention.backends."
--                        "triton_attn.TritonAttentionBackend")
--            if cls.has_device_capability(80):
--                logger.info_once("Using Flash Attention backend on V1 engine.")
--                return ("vllm.v1.attention.backends."
--                        "flash_attn.FlashAttentionBackend")
-+            logger.info_once("Using Flash Attention backend on V1 engine.")
-+            return ("vllm.v1.attention.backends.flash_attn."
-+                    "FlashAttentionBackend")
-         if selected_backend == _Backend.FLASHINFER:
-             logger.info("Using FlashInfer backend.")
-             return "vllm.attention.backends.flashinfer.FlashInferBackend"
-@@ -246,6 +240,15 @@ class CudaPlatformBase(Platform):
-                 "Cannot use FlashAttention-2 backend for dtype other than "
-                 "torch.float16 or torch.bfloat16.")
-             target_backend = _Backend.XFORMERS
-+        elif kv_cache_dtype is not None and \
-+            kv_cache_dtype.startswith("fp8"):
-+            logger.info(
-+                "Cannot use FlashAttention-2 backend for FP8 KV cache.")
-+            logger.warning(
-+                "Please use FlashInfer backend with FP8 KV Cache for "
-+                "better performance by setting environment variable "
-+                "VLLM_ATTENTION_BACKEND=FLASHINFER")
-+            target_backend = _Backend.XFORMERS
-         elif block_size % 16 != 0:
-             logger.info(
-                 "Cannot use FlashAttention-2 backend for block size not "
-@@ -267,17 +270,6 @@ class CudaPlatformBase(Platform):
-                         "Cannot use FlashAttention-2 backend for head size %d.",
-                         head_size)
-                     target_backend = _Backend.XFORMERS
--                fp8_kv_cache = (kv_cache_dtype is not None
--                                and kv_cache_dtype.startswith("fp8"))
--                if (fp8_kv_cache and get_flash_attn_version() != 3):
--                    logger.info(
--                        "Cannot use FlashAttention-2 backend for FP8 KV cache."
--                    )
--                    logger.warning(
--                        "Please use FlashInfer backend with FP8 KV Cache for "
--                        "better performance by setting environment variable "
--                        "VLLM_ATTENTION_BACKEND=FLASHINFER")
--                    target_backend = _Backend.XFORMERS
-             except ImportError:
-                 logger.info(
-                     "Cannot use FlashAttention-2 backend because the "
-diff --git a/vllm/platforms/interface.py b/vllm/platforms/interface.py
-index d3bffaf4..7415b5d5 100644
---- a/vllm/platforms/interface.py
-+++ b/vllm/platforms/interface.py
-@@ -29,7 +29,6 @@ def in_wsl() -> bool:
- class _Backend(enum.Enum):
-     FLASH_ATTN = enum.auto()
-     FLASH_ATTN_VLLM_V1 = enum.auto()
--    TRITON_ATTN_VLLM_V1 = enum.auto()
-     XFORMERS = enum.auto()
-     ROCM_FLASH = enum.auto()
-     TORCH_SDPA = enum.auto()
-@@ -113,8 +112,6 @@ class Platform:
- 
-     supported_quantization: list[str] = []
- 
--    additional_env_vars: list[str] = []
--
-     def is_cuda(self) -> bool:
-         return self._enum == PlatformEnum.CUDA
- 
-diff --git a/vllm/platforms/rocm.py b/vllm/platforms/rocm.py
-index ee708f59..75f287b5 100644
---- a/vllm/platforms/rocm.py
-+++ b/vllm/platforms/rocm.py
-@@ -120,9 +120,8 @@ class RocmPlatform(Platform):
-         selected_backend = (_Backend.ROCM_FLASH if selected_backend
-                             == _Backend.FLASH_ATTN else selected_backend)
-         if envs.VLLM_USE_V1:
--            logger.info("Using Triton Attention backend on V1 engine.")
--            return ("vllm.v1.attention.backends."
--                    "triton_attn.TritonAttentionBackend")
-+            logger.info("Using ROCm Attention backend on V1 engine.")
-+            return "vllm.v1.attention.backends.rocm_attn.ROCmAttentionBackend"
-         if selected_backend == _Backend.ROCM_FLASH:
-             if not cls.has_device_capability(90):
-                 # not Instinct series GPUs.
-diff --git a/vllm/platforms/tpu.py b/vllm/platforms/tpu.py
-index 073d46c2..8e2c28d9 100644
---- a/vllm/platforms/tpu.py
-+++ b/vllm/platforms/tpu.py
-@@ -29,10 +29,6 @@ class TpuPlatform(Platform):
-         "tpu_int8", "compressed-tensors", "compressed_tensors"
-     ]
- 
--    additional_env_vars: list[str] = [
--        "TPU_CHIPS_PER_HOST_BOUNDS", "TPU_HOST_BOUNDS"
--    ]
--
-     @classmethod
-     def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,
-                              dtype: torch.dtype, kv_cache_dtype: Optional[str],
-diff --git a/vllm/transformers_utils/tokenizer_group/__init__.py b/vllm/transformers_utils/tokenizer_group/__init__.py
-index 9d220957..c223768b 100644
---- a/vllm/transformers_utils/tokenizer_group/__init__.py
-+++ b/vllm/transformers_utils/tokenizer_group/__init__.py
-@@ -18,7 +18,7 @@ else:
- def init_tokenizer_from_configs(model_config: ModelConfig,
-                                 scheduler_config: SchedulerConfig,
-                                 parallel_config: ParallelConfig,
--                                lora_config: Optional[LoRAConfig]):
-+                                lora_config: LoRAConfig):
-     init_kwargs = dict(tokenizer_id=model_config.tokenizer,
-                        enable_lora=bool(lora_config),
-                        max_num_seqs=scheduler_config.max_num_seqs,
-diff --git a/vllm/utils.py b/vllm/utils.py
-index cb375f8f..79787303 100644
---- a/vllm/utils.py
-+++ b/vllm/utils.py
-@@ -411,11 +411,6 @@ async def merge_async_iterators(
-     When it yields, it yields a tuple (i, item) where i is the index of the
-     iterator that yields the item.
-     """
--    if len(iterators) == 1:
--        # Fast-path single iterator case.
--        async for item in iterators[0]:
--            yield 0, item
--        return
- 
-     loop = asyncio.get_running_loop()
- 
-@@ -2147,48 +2142,20 @@ def zmq_socket_ctx(path: str, socket_type: Any) -> Iterator[zmq.Socket]:
-         ctx.destroy(linger=0)
- 
- 
--def is_in_ray_actor():
--    """Check if we are in a Ray actor."""
--
--    try:
--        import ray
--        return (ray.is_initialized()
--                and ray.get_runtime_context().get_actor_id() is not None)
--    except ImportError:
--        return False
--
--
--def _maybe_force_spawn():
--    """Check if we need to force the use of the `spawn` multiprocessing start
--    method.
--    """
--    if os.environ.get("VLLM_WORKER_MULTIPROC_METHOD") == "spawn":
--        return
--
--    reason = None
--    if cuda_is_initialized():
--        reason = "CUDA is initialized"
--    elif is_in_ray_actor():
--        reason = "In a Ray actor and can only be spawned"
--
--    if reason is not None:
--        logger.warning(
--            "We must use the `spawn` multiprocessing start method. "
--            "Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. "
--            "See https://docs.vllm.ai/en/latest/getting_started/"
--            "troubleshooting.html#python-multiprocessing "
--            "for more information. Reason: %s", reason)
-+def _check_multiproc_method():
-+    if (cuda_is_initialized()
-+            and os.environ.get("VLLM_WORKER_MULTIPROC_METHOD") != "spawn"):
-+        logger.warning("CUDA was previously initialized. We must use "
-+                       "the `spawn` multiprocessing start method. Setting "
-+                       "VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. "
-+                       "See https://docs.vllm.ai/en/latest/getting_started/"
-+                       "troubleshooting.html#python-multiprocessing "
-+                       "for more information.")
-         os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"
- 
- 
- def get_mp_context():
--    """Get a multiprocessing context with a particular method (spawn or fork).
--    By default we follow the value of the VLLM_WORKER_MULTIPROC_METHOD to
--    determine the multiprocessing method (default is fork). However, under
--    certain conditions, we may enforce spawn and override the value of
--    VLLM_WORKER_MULTIPROC_METHOD.
--    """
--    _maybe_force_spawn()
-+    _check_multiproc_method()
-     mp_method = envs.VLLM_WORKER_MULTIPROC_METHOD
-     return multiprocessing.get_context(mp_method)
- 
-diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
-index 27b3aabb..ad44f256 100755
---- a/vllm/v1/attention/backends/flash_attn.py
-+++ b/vllm/v1/attention/backends/flash_attn.py
-@@ -6,18 +6,17 @@ from typing import TYPE_CHECKING, Any, Optional
- import numpy as np
- import torch
- 
--from vllm import _custom_ops as ops
- from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
-                                               AttentionMetadata, AttentionType,
-                                               is_quantized_kv_cache)
-+from vllm.attention.backends.utils import get_flash_attn_version
- from vllm.attention.ops.triton_merge_attn_states import merge_attn_states
--from vllm.fa_utils import get_flash_attn_version
- from vllm.logger import init_logger
- from vllm.platforms import current_platform
- from vllm.utils import cdiv
- 
- if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler_output import SchedulerOutput
-     from vllm.v1.worker.gpu_input_batch import InputBatch
-     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
- 
-@@ -227,9 +226,6 @@ class FlashAttentionImpl(AttentionImpl):
-             attn_metadata: Metadata for attention.
-         Returns:
-             shape = [num_tokens, num_heads * head_size]
--        NOTE: FP8 quantization, flash-attn expect the size of
--              {q,k,v}_descale to be (num_sequences, num_kv_heads).
--              We use torch's .expand() to avoid duplicating values
-         """
-         assert output is not None, "Output tensor must be provided."
- 
-@@ -263,17 +259,6 @@ class FlashAttentionImpl(AttentionImpl):
-             layer._k_scale,
-             layer._v_scale,
-         )
--        descale_shape = (attn_metadata.query_start_loc.shape[0] - 1,
--                         key.shape[1])
--        if self.kv_cache_dtype.startswith("fp8"):
--            key_cache = key_cache.view(torch.float8_e4m3fn)
--            value_cache = value_cache.view(torch.float8_e4m3fn)
--            num_tokens, num_heads, head_size = query.shape
--            query, _ = ops.scaled_fp8_quant(
--                query.reshape(
--                    (num_tokens, num_heads * head_size)).contiguous(),
--                layer._q_scale)
--            query = query.reshape((num_tokens, num_heads, head_size))
- 
-         # Compute attention and update output up to `num_actual_tokens`.
-         if not attn_metadata.use_cascade:
-@@ -294,9 +279,6 @@ class FlashAttentionImpl(AttentionImpl):
-                 block_table=attn_metadata.block_table,
-                 softcap=self.logits_soft_cap,
-                 fa_version=self.vllm_flash_attn_version,
--                q_descale=layer._q_scale.expand(descale_shape),
--                k_descale=layer._k_scale.expand(descale_shape),
--                v_descale=layer._v_scale.expand(descale_shape),
-             )
-             return output
- 
-@@ -319,9 +301,6 @@ class FlashAttentionImpl(AttentionImpl):
-             block_table=attn_metadata.block_table,
-             common_prefix_len=attn_metadata.common_prefix_len,
-             fa_version=self.vllm_flash_attn_version,
--            q_descale=layer._q_scale,
--            k_descale=layer._k_scale,
--            v_descale=layer._v_scale,
-         )
-         return output
- 
-@@ -412,9 +391,6 @@ def cascade_attention(
-     block_table: torch.Tensor,
-     common_prefix_len: int,
-     fa_version: int,
--    q_descale: Optional[torch.Tensor] = None,
--    k_descale: Optional[torch.Tensor] = None,
--    v_descale: Optional[torch.Tensor] = None,
- ) -> torch.Tensor:
-     assert alibi_slopes is None, ("Cascade attention does not support ALiBi.")
-     # TODO: Support sliding window.
-@@ -426,7 +402,6 @@ def cascade_attention(
-     assert common_prefix_len % block_size == 0
-     num_common_kv_blocks = common_prefix_len // block_size
-     assert num_common_kv_blocks > 0
--    descale_shape = (cu_prefix_query_lens.shape[0] - 1, key_cache.shape[-2])
- 
-     # Process shared prefix.
-     prefix_output, prefix_lse = flash_attn_varlen_func(
-@@ -444,16 +419,8 @@ def cascade_attention(
-         softcap=logits_soft_cap,
-         return_softmax_lse=True,
-         fa_version=fa_version,
--        q_descale=q_descale.expand(descale_shape)
--        if q_descale is not None else None,
--        k_descale=k_descale.expand(descale_shape)
--        if k_descale is not None else None,
--        v_descale=v_descale.expand(descale_shape)
--        if v_descale is not None else None,
-     )
- 
--    descale_shape = (cu_query_lens.shape[0] - 1, key_cache.shape[-2])
--
-     # Process suffix per query.
-     suffix_output, suffix_lse = flash_attn_varlen_func(
-         q=query,
-@@ -470,12 +437,6 @@ def cascade_attention(
-         softcap=logits_soft_cap,
-         return_softmax_lse=True,
-         fa_version=fa_version,
--        q_descale=q_descale.expand(descale_shape)
--        if q_descale is not None else None,
--        k_descale=k_descale.expand(descale_shape)
--        if k_descale is not None else None,
--        v_descale=v_descale.expand(descale_shape)
--        if v_descale is not None else None,
-     )
- 
-     # Merge prefix and suffix outputs, and store the result in output.
-diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
-index 188a425b..f801745a 100644
---- a/vllm/v1/attention/backends/mla/common.py
-+++ b/vllm/v1/attention/backends/mla/common.py
-@@ -212,7 +212,7 @@ except ImportError:
-     from flash_attn import flash_attn_varlen_func
- 
- if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler_output import SchedulerOutput
-     from vllm.v1.worker.gpu_input_batch import InputBatch
-     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
- 
-diff --git a/vllm/v1/attention/backends/pallas.py b/vllm/v1/attention/backends/pallas.py
-index 14d3664d..bbbdf50a 100644
---- a/vllm/v1/attention/backends/pallas.py
-+++ b/vllm/v1/attention/backends/pallas.py
-@@ -41,7 +41,7 @@ class PallasAttentionBackend(AttentionBackend):
-         num_kv_heads: int,
-         head_size: int,
-     ) -> tuple[int, ...]:
--        return (num_blocks, block_size, num_kv_heads * head_size)
-+        return (num_blocks, block_size, num_kv_heads, head_size)
- 
-     @staticmethod
-     def swap_blocks(
-@@ -142,8 +142,8 @@ class PallasAttentionBackendImpl(AttentionImpl):
-             query: shape = [num_tokens, num_heads * head_size]
-             key: shape = [num_tokens, num_kv_heads * head_size]
-             value: shape = [num_tokens, num_kv_heads * head_size]
--            kv_cache = ([num_blocks, block_size, num_kv_heads * head_size], 
--                        [num_blocks, block_size, num_kv_heads * head_size])
-+            kv_cache = ([num_blocks, block_size, num_kv_heads, head_size], 
-+                        [num_blocks, block_size, num_kv_heads, head_size])
-             attn_metadata: Metadata for attention.
-         Returns:
-             shape = [num_tokens, num_heads * head_size]
-@@ -157,6 +157,8 @@ class PallasAttentionBackendImpl(AttentionImpl):
-         assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0
-         num_tokens, hidden_size = query.shape
-         query = query.view(num_tokens, self.num_heads, self.head_size)
-+        key = key.view(num_tokens, self.num_kv_heads, self.head_size)
-+        value = value.view(num_tokens, self.num_kv_heads, self.head_size)
- 
-         key_cache, value_cache = kv_cache
-         if kv_cache[0].numel() > 0:
-@@ -190,10 +192,10 @@ def write_to_kv_cache(
-     """ Write the key and values to the KV cache.
- 
-     Args:
--        key: shape = [num_tokens, num_kv_heads * head_size]
--        value: shape = [num_tokens, num_kv_heads * head_size]
--        k_cache = [num_blocks, block_size, num_kv_heads * head_size]
--        v_cache = [num_blocks, block_size, num_kv_heads * head_size]
-+        key: shape = [num_tokens, num_kv_heads, head_size]
-+        value: shape = [num_tokens, num_kv_heads, head_size]
-+        k_cache = [num_blocks, block_size, num_kv_heads, head_size]
-+        v_cache = [num_blocks, block_size, num_kv_heads, head_size]
- 
-     """
-     torch.ops.xla.dynamo_set_buffer_donor_(key_cache, True)
-@@ -201,5 +203,6 @@ def write_to_kv_cache(
- 
-     key_cache = key_cache.flatten(0, 1)
-     value_cache = value_cache.flatten(0, 1)
-+    slot_mapping = slot_mapping.flatten()
-     key_cache.index_copy_(0, slot_mapping, key)
-     value_cache.index_copy_(0, slot_mapping, value)
-diff --git a/vllm/v1/attention/backends/triton_attn.py b/vllm/v1/attention/backends/rocm_attn.py
-similarity index 91%
-rename from vllm/v1/attention/backends/triton_attn.py
-rename to vllm/v1/attention/backends/rocm_attn.py
-index f11f2b62..640c3b3d 100644
---- a/vllm/v1/attention/backends/triton_attn.py
-+++ b/vllm/v1/attention/backends/rocm_attn.py
-@@ -1,5 +1,5 @@
- # SPDX-License-Identifier: Apache-2.0
--"""Attention layer with PagedAttention and Triton prefix prefill."""
-+"""Attention layer with PagedAttention on rocm"""
- from typing import Any, Optional
- 
- import torch
-@@ -16,7 +16,7 @@ from vllm.v1.attention.backends.flash_attn import (
- logger = init_logger(__name__)
- 
- 
--class TritonAttentionBackend(AttentionBackend):
-+class ROCmAttentionBackend(AttentionBackend):
- 
-     accept_output_buffer: bool = True
- 
-@@ -26,11 +26,11 @@ class TritonAttentionBackend(AttentionBackend):
- 
-     @staticmethod
-     def get_name() -> str:
--        return "TRITON_ATTN_VLLM_V1"
-+        return "ROCM_ATTN_VLLM_V1"
- 
-     @staticmethod
--    def get_impl_cls() -> type["TritonAttentionImpl"]:
--        return TritonAttentionImpl
-+    def get_impl_cls() -> type["ROCmAttentionImpl"]:
-+        return ROCmAttentionImpl
- 
-     @staticmethod
-     def get_metadata_cls() -> type["AttentionMetadata"]:
-@@ -56,7 +56,7 @@ class TritonAttentionBackend(AttentionBackend):
-         return FlashAttentionMetadataBuilder
- 
- 
--class TritonAttentionImpl(AttentionImpl):
-+class ROCmAttentionImpl(AttentionImpl):
- 
-     def __init__(
-         self,
-@@ -73,7 +73,7 @@ class TritonAttentionImpl(AttentionImpl):
-     ) -> None:
-         if blocksparse_params is not None:
-             raise ValueError(
--                "TritonAttention does not support block-sparse attention.")
-+                "ROCmAttention does not support block-sparse attention.")
-         self.num_heads = num_heads
-         self.head_size = head_size
-         self.scale = float(scale)
-@@ -90,17 +90,17 @@ class TritonAttentionImpl(AttentionImpl):
-         assert self.num_heads % self.num_kv_heads == 0
-         self.num_queries_per_kv = self.num_heads // self.num_kv_heads
- 
--        support_head_sizes = TritonAttentionBackend.get_supported_head_sizes()
-+        support_head_sizes = ROCmAttentionBackend.get_supported_head_sizes()
-         if head_size not in support_head_sizes:
-             raise ValueError(
--                f"Head size {head_size} is not supported by TritonAttention. "
-+                f"Head size {head_size} is not supported by ROCmAttention. "
-                 f"Supported head sizes are: {support_head_sizes}.")
- 
-         if attn_type != AttentionType.DECODER:
-             raise NotImplementedError("Encoder self-attention and "
-                                       "encoder/decoder cross-attention "
-                                       "are not implemented for "
--                                      "TritonAttentionImpl")
-+                                      "ROCmAttentionImpl")
- 
-     def forward(
-         self,
-diff --git a/vllm/v1/core/sched/__init__.py b/vllm/v1/core/sched/__init__.py
-deleted file mode 100644
-index e69de29b..00000000
-diff --git a/vllm/v1/core/sched/interface.py b/vllm/v1/core/sched/interface.py
-deleted file mode 100644
-index bfed44f9..00000000
---- a/vllm/v1/core/sched/interface.py
-+++ /dev/null
-@@ -1,139 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--from abc import ABC, abstractmethod
--from collections.abc import Iterable
--from typing import TYPE_CHECKING, Optional, Union
--
--if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
--    from vllm.v1.engine import EngineCoreOutputs
--    from vllm.v1.metrics.stats import SchedulerStats
--    from vllm.v1.outputs import ModelRunnerOutput
--    from vllm.v1.request import Request, RequestStatus
--
--
--class SchedulerInterface(ABC):
--
--    @abstractmethod
--    def schedule(self) -> "SchedulerOutput":
--        """Schedule the requests to process in this scheduling step.
--
--        The scheduling decision is made at the iteration level. Each scheduling
--        step corresponds to a single forward pass of the model. Therefore, this
--        method is called repeatedly by a busy loop in the engine.
--
--        Essentially, the scheduler produces a dictionary of {req_id: num_tokens}
--        that specifies how many tokens to process for each request in this
--        scheduling step. For example, num_tokens can be as large as the number
--        of prompt tokens for new requests, or it can be 1 for the requests that
--        are auto-regressively generating new tokens one by one. Otherwise, it
--        can be somewhere in between in case of chunked prefills, prefix caching,
--        speculative decoding, etc.
--
--        Additionally, the scheduler also returns useful data about each request
--        or the batch as a whole. The model runner will use this information in
--        preparing inputs to the model.
--
--        Returns:
--            A SchedulerOutput object containing information about the scheduled
--            requests.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def update_from_output(
--        self,
--        scheduler_output: "SchedulerOutput",
--        model_runner_output: "ModelRunnerOutput",
--    ) -> "EngineCoreOutputs":
--        """Update the scheduler state based on the model runner output.
--
--        This method is called after the model runner has processed the scheduled
--        requests. The model runner output includes generated token ids, draft
--        token ids for next step, etc. The scheduler uses this information to
--        update its states, checks the finished requests, and returns the output
--        for each request.
--
--        Returns:
--            A EngineCoreOutputs object containing the outputs for each request.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def add_request(self, request: "Request") -> None:
--        """Add a new request to the scheduler's internal queue.
--        
--        Args:
--            request: The new request being added.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def finish_requests(
--        self,
--        request_ids: Union[str, Iterable[str]],
--        finished_status: "RequestStatus",
--    ) -> None:
--        """Finish the requests in the scheduler's internal queue. If the request
--        is not in the queue, this method will do nothing.
--
--        This method is called in two cases:
--        1. When the request is aborted by the client.
--        2. When the frontend process detects a stop string of the request after
--           de-tokenizing its generated tokens.
--           
--        Args:
--            request_ids: A single or a list of request IDs.
--            finished_status: The finished status of the given requests.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def get_num_unfinished_requests(self) -> int:
--        """Number of unfinished requests in the scheduler's internal queue."""
--        raise NotImplementedError
--
--    def has_unfinished_requests(self) -> bool:
--        """Returns True if there are unfinished requests in the scheduler's
--        internal queue."""
--        return self.get_num_unfinished_requests() > 0
--
--    @abstractmethod
--    def has_finished_requests(self) -> bool:
--        """Returns True if there are finished requests that need to be cleared.
--        NOTE: This is different from `not self.has_unfinished_requests()`.
--
--        The scheduler maintains an internal list of the requests finished in the
--        previous step. This list is returned from the next call to schedule(),
--        to be sent to the model runner in the next step to clear cached states
--        for these finished requests.
--
--        This method checks if this internal list of finished requests is
--        non-empty. This information is useful for DP attention.
--        """
--        raise NotImplementedError
--
--    def has_requests(self) -> bool:
--        """Returns True if there are unfinished requests, or finished requests
--        not yet returned in SchedulerOutputs."""
--        return self.has_unfinished_requests() or self.has_finished_requests()
--
--    @abstractmethod
--    def get_num_unscheduled_requests(self) -> int:
--        """Number of requests that are not being processed by the executor."""
--        raise NotImplementedError
--
--    @abstractmethod
--    def reset_prefix_cache(self) -> bool:
--        """Reset the prefix cache for KV cache.
--
--        This is particularly required when the model weights are live-updated.
--        """
--        raise NotImplementedError
--
--    @abstractmethod
--    def make_stats(self) -> Optional["SchedulerStats"]:
--        """Make a SchedulerStats object for logging.
--
--        The SchedulerStats object is created for every scheduling step.
--        """
--        raise NotImplementedError
-diff --git a/vllm/v1/core/sched/utils.py b/vllm/v1/core/sched/utils.py
-deleted file mode 100644
-index 3a0028a5..00000000
---- a/vllm/v1/core/sched/utils.py
-+++ /dev/null
-@@ -1,22 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--from vllm.v1.request import Request, RequestStatus
--
--
--def check_stop(request: Request, max_model_len: int) -> bool:
--    if (request.num_tokens >= max_model_len
--            or request.num_output_tokens >= request.max_tokens):
--        request.status = RequestStatus.FINISHED_LENGTH_CAPPED
--        return True
--
--    sampling_params = request.sampling_params
--    last_token_id = request.output_token_ids[-1]
--    if (not sampling_params.ignore_eos
--            and last_token_id == request.eos_token_id):
--        request.status = RequestStatus.FINISHED_STOPPED
--        return True
--
--    if last_token_id in (sampling_params.stop_token_ids or ()):
--        request.status = RequestStatus.FINISHED_STOPPED
--        request.stop_reason = last_token_id
--        return True
--    return False
-diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/scheduler.py
-similarity index 96%
-rename from vllm/v1/core/sched/scheduler.py
-rename to vllm/v1/core/scheduler.py
-index d002a19b..056458ef 100644
---- a/vllm/v1/core/sched/scheduler.py
-+++ b/vllm/v1/core/scheduler.py
-@@ -13,10 +13,8 @@ from vllm.logger import init_logger
- from vllm.v1.core.encoder_cache_manager import (EncoderCacheManager,
-                                                 compute_encoder_budget)
- from vllm.v1.core.kv_cache_manager import KVCacheManager
--from vllm.v1.core.sched.interface import SchedulerInterface
--from vllm.v1.core.sched.output import (CachedRequestData, NewRequestData,
--                                       SchedulerOutput)
--from vllm.v1.core.sched.utils import check_stop
-+from vllm.v1.core.scheduler_output import (CachedRequestData, NewRequestData,
-+                                           SchedulerOutput)
- from vllm.v1.engine import (EngineCoreEventType, EngineCoreOutput,
-                             EngineCoreOutputs)
- from vllm.v1.metrics.stats import SchedulerStats
-@@ -27,7 +25,7 @@ from vllm.v1.structured_output import StructuredOutputManager
- logger = init_logger(__name__)
- 
- 
--class Scheduler(SchedulerInterface):
-+class Scheduler:
- 
-     def __init__(
-         self,
-@@ -604,7 +602,7 @@ class Scheduler(SchedulerInterface):
- 
-                     # Check for stop and update request state.
-                     # This must be called before we make the EngineCoreOutput.
--                    stopped = check_stop(request, self.max_model_len)
-+                    stopped = self._check_stop(request)
-                     if stopped:
-                         self._free_request(request)
-                         break
-@@ -650,6 +648,25 @@ class Scheduler(SchedulerInterface):
-             scheduler_stats=self.make_stats(),
-         )
- 
-+    def _check_stop(self, request: Request) -> bool:
-+        if (request.num_tokens >= self.max_model_len
-+                or request.num_output_tokens >= request.max_tokens):
-+            request.status = RequestStatus.FINISHED_LENGTH_CAPPED
-+            return True
-+
-+        sampling_params = request.sampling_params
-+        last_token_id = request.output_token_ids[-1]
-+        if (not sampling_params.ignore_eos
-+                and last_token_id == request.eos_token_id):
-+            request.status = RequestStatus.FINISHED_STOPPED
-+            return True
-+
-+        if last_token_id in (sampling_params.stop_token_ids or ()):
-+            request.status = RequestStatus.FINISHED_STOPPED
-+            request.stop_reason = last_token_id
-+            return True
-+        return False
-+
-     def add_request(self, request: Request) -> None:
-         self.waiting.append(request)
-         self.requests[request.request_id] = request
-@@ -698,9 +715,17 @@ class Scheduler(SchedulerInterface):
-     def get_num_unfinished_requests(self) -> int:
-         return len(self.waiting) + len(self.running)
- 
-+    def has_unfinished_requests(self) -> bool:
-+        return self.get_num_unfinished_requests() > 0
-+
-     def has_finished_requests(self) -> bool:
-         return len(self.finished_req_ids) > 0
- 
-+    def has_requests(self):
-+        """Returns True if there are unfinished requests, or finished requests
-+        not yet returned in SchedulerOutputs."""
-+        return self.has_unfinished_requests() or self.has_finished_requests()
-+
-     def get_num_unscheduled_requests(self) -> int:
-         """Number of requests that are not being processed by the executor."""
-         return self.get_num_unfinished_requests() - len(self.scheduled_req_ids)
-diff --git a/vllm/v1/core/sched/output.py b/vllm/v1/core/scheduler_output.py
-similarity index 100%
-rename from vllm/v1/core/sched/output.py
-rename to vllm/v1/core/scheduler_output.py
-diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
-index 171c1c7d..d4ac9c06 100644
---- a/vllm/v1/engine/async_llm.py
-+++ b/vllm/v1/engine/async_llm.py
-@@ -24,7 +24,7 @@ from vllm.sampling_params import RequestOutputKind, SamplingParams
- from vllm.transformers_utils.tokenizer import AnyTokenizer
- from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import Device, cdiv, kill_process_tree
-+from vllm.utils import cdiv, kill_process_tree
- from vllm.v1.engine.core_client import EngineCoreClient
- from vllm.v1.engine.output_processor import OutputProcessor
- from vllm.v1.engine.parallel_sampling import ParentRequest
-@@ -398,10 +398,7 @@ class AsyncLLM(EngineClient):
-     async def stop_profile(self) -> None:
-         await self.engine_core.profile_async(False)
- 
--    async def reset_prefix_cache(self,
--                                 device: Optional[Device] = None) -> None:
--        if device == Device.CPU:
--            raise ValueError("Not supported on CPU.")
-+    async def reset_prefix_cache(self) -> None:
-         await self.engine_core.reset_prefix_cache_async()
- 
-     async def sleep(self, level: int = 1) -> None:
-diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
-index 1598e6b8..8f93d3c7 100644
---- a/vllm/v1/engine/core.py
-+++ b/vllm/v1/engine/core.py
-@@ -22,8 +22,8 @@ from vllm.transformers_utils.config import (
- from vllm.utils import (get_exception_traceback, resolve_obj_by_qualname,
-                         zmq_socket_ctx)
- from vllm.v1.core.kv_cache_utils import get_kv_cache_configs
--from vllm.v1.core.sched.output import SchedulerOutput
--from vllm.v1.core.sched.scheduler import Scheduler as V1Scheduler
-+from vllm.v1.core.scheduler import Scheduler as V1Scheduler
-+from vllm.v1.core.scheduler import SchedulerOutput
- from vllm.v1.engine import (EngineCoreOutputs, EngineCoreRequest,
-                             EngineCoreRequestType, UtilityOutput)
- from vllm.v1.engine.mm_input_cache import MMInputCacheServer
-@@ -179,6 +179,16 @@ class EngineCore:
-                 scheduler_stats=self.scheduler.make_stats(),
-             )
-         scheduler_output = self.scheduler.schedule()
-+
-+        # This case may occur when the only unfinished requests are
-+        # structured output requests where the grammar has not finished
-+        # compiling yet, so there's nothing to run.
-+        if scheduler_output.total_num_scheduled_tokens == 0:
-+            return EngineCoreOutputs(
-+                outputs=[],
-+                scheduler_stats=self.scheduler.make_stats(),
-+            )
-+
-         output = self.model_executor.execute_model(scheduler_output)
-         engine_core_outputs = self.scheduler.update_from_output(
-             scheduler_output, output)  # type: ignore
-diff --git a/vllm/v1/engine/llm_engine.py b/vllm/v1/engine/llm_engine.py
-index 14338e5c..63b0a8fc 100644
---- a/vllm/v1/engine/llm_engine.py
-+++ b/vllm/v1/engine/llm_engine.py
-@@ -20,7 +20,6 @@ from vllm.sampling_params import SamplingParams
- from vllm.transformers_utils.tokenizer_group import (
-     BaseTokenizerGroup, init_tokenizer_from_configs)
- from vllm.usage.usage_lib import UsageContext
--from vllm.utils import Device
- from vllm.v1.engine.core_client import EngineCoreClient
- from vllm.v1.engine.output_processor import OutputProcessor
- from vllm.v1.engine.parallel_sampling import ParentRequest
-@@ -227,7 +226,7 @@ class LLMEngine:
-     def stop_profile(self):
-         self.engine_core.profile(False)
- 
--    def reset_prefix_cache(self, device: Optional[Device] = None):
-+    def reset_prefix_cache(self):
-         self.engine_core.reset_prefix_cache()
- 
-     def sleep(self, level: int = 1):
-diff --git a/vllm/v1/executor/multiproc_executor.py b/vllm/v1/executor/multiproc_executor.py
-index 21e7d265..b2cbba51 100644
---- a/vllm/v1/executor/multiproc_executor.py
-+++ b/vllm/v1/executor/multiproc_executor.py
-@@ -5,7 +5,6 @@ import pickle
- import signal
- import sys
- import time
--import traceback
- import weakref
- from dataclasses import dataclass
- from enum import Enum, auto
-@@ -371,9 +370,6 @@ class WorkerProc:
-                     func = partial(cloudpickle.loads(method), self.worker)
-                 output = func(*args, **kwargs)
-             except Exception as e:
--                # Notes have been introduced in python 3.11
--                if hasattr(e, "add_note"):
--                    e.add_note(traceback.format_exc())
-                 self.worker_response_mq.enqueue(
-                     (WorkerProc.ResponseStatus.FAILURE, e))
-                 logger.exception("WorkerProc hit an exception: %s", exc_info=e)
-diff --git a/vllm/v1/metrics/stats.py b/vllm/v1/metrics/stats.py
-index 83383ce1..36317fc9 100644
---- a/vllm/v1/metrics/stats.py
-+++ b/vllm/v1/metrics/stats.py
-@@ -6,7 +6,7 @@ from typing import TYPE_CHECKING, Optional
- 
- if TYPE_CHECKING:
-     from vllm.v1.engine import EngineCoreEvent, EngineCoreOutput, FinishReason
--    from vllm.v1.engine.output_processor import RequestState
-+    from vllm.v1.output_processor import RequestState
- 
- 
- @dataclass
-diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py
-index 1dea7118..d461a809 100644
---- a/vllm/v1/sample/ops/topk_topp_sampler.py
-+++ b/vllm/v1/sample/ops/topk_topp_sampler.py
-@@ -65,15 +65,6 @@ class TopKTopPSampler(nn.Module):
-                     "native implementation of top-p & top-k sampling. For the "
-                     "best performance, please install FlashInfer.")
-                 self.forward = self.forward_native
--        elif current_platform.is_tpu():
--            if envs.VLLM_TPU_DISABLE_TOPK_TOPP_OPTIMIZATION:
--                logger.warning(
--                    "TPU-specific optimization for top-k & top-p sampling are "
--                    "disabled, falling back to PyTorch-native implementation "
--                    "which could be very slow.")
--                self.forward = self.forward_native
--            else:
--                self.forward = self.forward_tpu
-         else:
-             self.forward = self.forward_native
- 
-@@ -105,29 +96,6 @@ class TopKTopPSampler(nn.Module):
-             return random_sample(probs, generators)
-         return flashinfer_sample(probs, k, p, generators)
- 
--    def forward_tpu(
--        self,
--        logits: torch.Tensor,
--        generators: dict[int, torch.Generator],
--        k: Optional[torch.Tensor],
--        p: Optional[torch.Tensor],
--    ) -> torch.Tensor:
--        # If only top-k is specified, use pytorch's builtin topk op. This leads
--        # to significant speed up on TPU compared to using apply_top_k_top_p.
--        if k is not None and p is None:
--            topk_values, topk_indices = torch.topk(logits, k, dim=-1)
--
--            mask = torch.ones_like(logits, dtype=torch.bool)
--            mask.scatter_(-1, topk_indices, False)
--            logits.masked_fill_(mask, float('-inf'))
--        else:
--            # TODO Placeholder for TPU optimized topp kernel
--            # logits = apply_top_k_top_p(logits, k, p)
--            pass
--
--        probs = logits.softmax(dim=-1, dtype=torch.float32)
--        return random_sample(probs, generators)
--
- 
- def apply_top_k_top_p(
-     logits: torch.Tensor,
-@@ -144,7 +112,7 @@ def apply_top_k_top_p(
- 
-     if k is not None:
-         # Apply top-k.
--        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B
-+        top_k_mask = logits_sort.size(1) - k.to(torch.long)
-         # Get all the top_k values.
-         top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))
-         top_k_mask = logits_sort < top_k_mask
-diff --git a/vllm/v1/sample/tpu/__init__.py b/vllm/v1/sample/tpu/__init__.py
-deleted file mode 100644
-index e69de29b..00000000
-diff --git a/vllm/v1/sample/tpu/metadata.py b/vllm/v1/sample/tpu/metadata.py
-deleted file mode 100644
-index b4f7c19a..00000000
---- a/vllm/v1/sample/tpu/metadata.py
-+++ /dev/null
-@@ -1,159 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--from dataclasses import dataclass, field
--from typing import Optional
--
--import torch
--import torch_xla.core.xla_model as xm
--
--from vllm.v1.sample.metadata import SamplingMetadata
--
--
--@dataclass
--class TPUSupportedSamplingMetadata:
--    # This class exposes a more xla-friendly interface than SamplingMetadata
--    # on TPU, in particular all arguments should be traceable and no optionals
--    # are allowed, to avoid graph recompilation on Nones.
--    temperature: torch.Tensor
--
--    min_p: torch.Tensor
--    # Still too slow on forward_native!
--    top_k: torch.Tensor = None
--    top_p: torch.Tensor = None
--
--    # XLA-unfriendly control flow in Sampler
--    all_greedy: bool = False
--    all_random: bool = False
--    # Greedy sampling flag for compiling single xla graph.
--    do_argmax: torch.Tensor = None
--
--    # speculation not supported
--    spec_token_ids = None
--
--    # Generator not supported by xla
--    generators: dict[int,
--                     torch.Generator] = field(default_factory=lambda: dict())
--
--    # unsupported, you need to return an extra tensor of static size BxV
--    max_num_logprobs = None
--
--    # TODO No penalties for now
--    no_penalties: bool = True
--    prompt_token_ids = None
--    frequency_penalties = None
--    presence_penalties = None
--    repetition_penalties = None
--    # should use tensor
--    output_token_ids: list[list[int]] = field(default_factory=lambda: list())
--
--    min_tokens = None  # impl is not vectorized
--
--    logit_bias: list[Optional[dict[int, float]]] = field(
--        default_factory=lambda: list())
--
--    allowed_token_ids_mask = None
--    bad_words_token_ids = None
--    indices_do_sample: torch.Tensor = None
--
--    def __post_init__(self):
--        temp = self.temperature
--        if self.indices_do_sample is None:
--            self.indices_do_sample = torch.zeros(temp.shape[0],
--                                                 device=temp.device,
--                                                 dtype=torch.int32)
--        if self.do_argmax is None:
--            self.do_argmax = torch.tensor(0,
--                                          dtype=torch.bool,
--                                          device=temp.device)
--
--    @classmethod
--    def from_sampling_metadata(
--            cls, metadata: SamplingMetadata,
--            padded_do_sample_indices: torch.Tensor, num_do_sample: int,
--            device: torch.device) -> "TPUSupportedSamplingMetadata":
--        """
--        Create an XLA-frienly SamplingMetadata structure. Do so by first 
--        instantiating an object with fixed-sized tensors and then writing the
--        values in input `metadata`. Do that only for non-None values so that 
--        recompilation is not triggered for optional values (None/torch.Tensor).
--        
--        In order to handle different sizes for the params that range from 1 up 
--        to `max_num_seqs`, pad tensors to the closest pre-compiled shape.
--        Same thing for `padded_do_sample_indices`, which contains the indices 
--        to be fed to the Sampler, padded to the closest pre-compiled shape.
--
--        Eg. pad to 4 temperature: [0.7, 0.2]=>[0.7, 0.2, 0.0, 0.0]
--            do_sample_indices: [4, 10]=>padded_do_sample_indices: [4, 10, 0, 0]
--        """
--        metadata = cls._validate_sampling_metadata(metadata)
--        # NOTE we have to initialize default tensor-based params first and
--        # skip None values altogether to produce the same xla graph.
--        num_samples = len(padded_do_sample_indices)
--        do_argmax = torch.tensor(metadata.all_greedy,
--                                 dtype=torch.bool,
--                                 device=device)
--        new_metadata = cls.get_default_sampling_params(num_samples, device,
--                                                    indices_do_sample=\
--                                                    padded_do_sample_indices,
--                                                    do_argmax=do_argmax
--                                                    )
--        supported_params = \
--            TPUSupportedSamplingMetadata._get_default_params_values()
--        # Copy input non-None values into `new_metadata` fixed-sized tensors.
--        for p_name in supported_params:
--            old_val = getattr(metadata, p_name)
--            new_val = getattr(new_metadata, p_name)
--            if isinstance(old_val, torch.Tensor):
--                new_val[:num_do_sample] = old_val
--            setattr(new_metadata, p_name, new_val)
--
--        xm.mark_step()
--        xm.wait_device_ops()
--        return new_metadata
--
--    @classmethod
--    def get_default_sampling_params(
--            cls,
--            num_samples: int,
--            device: torch.device,
--            indices_do_sample=None,
--            do_argmax=None) -> "TPUSupportedSamplingMetadata":
--        # As sampling happens on a single traced graph, options
--        # are "disabled" by having them evaluate to an Identity op.
--        # Note that initialization is dependent on num_samples.
--        sampling_metadata_disable_value = \
--            TPUSupportedSamplingMetadata._get_default_params_values()
--        init_kwargs = dict()
--        for p_name, (default_val,
--                     dtype) in sampling_metadata_disable_value.items():
--            default_tensor = torch.full((num_samples, ),
--                                        default_val,
--                                        dtype=dtype,
--                                        device=device)
--            init_kwargs[p_name] = default_tensor
--
--        return cls(**init_kwargs,
--                   indices_do_sample=indices_do_sample,
--                   do_argmax=do_argmax)
--
--    @staticmethod
--    def _validate_sampling_metadata(
--            sampling_metadata: SamplingMetadata) -> SamplingMetadata:
--        if sampling_metadata.all_greedy:
--            # Set to None since #13587. Make sure default isn't overruled.
--            assert sampling_metadata.temperature is None
--        return sampling_metadata
--
--    @staticmethod
--    def _get_default_params_values():
--        return dict(
--            # Since #13587 greedy sampling requires branching off which leads
--            # to separate graphs. We set temp to noop and handle argmax here.
--            temperature=(1.0, torch.float32),
--            min_p=(0.0, torch.float32),
--            # strictly disabled for now
--            # top_k=(-1, torch.int32),
--            # top_p=(0.0, torch.float32),
--            # frequency_penalties=(0.0, torch.float32),
--            # presence_penalties=(0.0, torch.float32),
--            # repetition_penalties=(0.0, torch.float32),
--        )
-\ No newline at end of file
-diff --git a/vllm/v1/sample/tpu/sampler.py b/vllm/v1/sample/tpu/sampler.py
-deleted file mode 100644
-index 33526c00..00000000
---- a/vllm/v1/sample/tpu/sampler.py
-+++ /dev/null
-@@ -1,154 +0,0 @@
--# SPDX-License-Identifier: Apache-2.0
--"""Sampler layer implementing TPU supported operations."""
--
--import torch
--import torch.nn as nn
--
--from vllm.v1.outputs import LogprobsTensors, SamplerOutput
--from vllm.v1.sample.ops.topk_topp_sampler import TopKTopPSampler
--from vllm.v1.sample.tpu.metadata import TPUSupportedSamplingMetadata
--
--_SAMPLING_EPS = 1e-5
--
--
--class Sampler(nn.Module):
--
--    def __init__(self):
--        super().__init__()
--        self.topk_topp_sampler = TopKTopPSampler()
--
--    def forward(
--        self,
--        logits: torch.Tensor,
--        sampling_metadata: TPUSupportedSamplingMetadata,
--    ) -> SamplerOutput:
--        # NOTE(woosuk): Use the original logits (before any penalties or
--        # temperature scaling) for the top-k logprobs.
--        # This is different from the V0 sampler, which uses the logits that
--        # is used for sampling (after penalties and temperature scaling).
--
--        # Use float32 for the logits.
--        logits = logits.to(torch.float32)
--        # Sample the next token.
--        sampled = self.sample(logits, sampling_metadata)
--
--        # Use int32 to reduce the tensor size.
--        sampled = sampled.to(torch.int32)
--
--        # These are GPU tensors.
--        sampler_output = SamplerOutput(
--            # The sampled tokens are expanded to 2D tensor with shape
--            # [num_requests, 1], where each row represents one generated
--            # token per request.
--            sampled_token_ids=sampled.unsqueeze(-1),
--            logprobs_tensors=None,
--        )
--        return sampler_output
--
--    def apply_temperature(
--        self,
--        logits: torch.Tensor,
--        temp: torch.Tensor,
--    ) -> torch.Tensor:
--        # Use in-place division to avoid creating a new tensor.
--        return logits.div_(temp.unsqueeze(dim=1))
--
--    def greedy_sample(self, logits: torch.Tensor) -> torch.Tensor:
--        return logits.argmax(dim=-1).view(-1)
--
--    def sample(
--        self,
--        logits: torch.Tensor,
--        sampling_metadata: TPUSupportedSamplingMetadata,
--    ) -> torch.Tensor:
--        greedy_sampled = self.greedy_sample(logits)
--
--        assert sampling_metadata.temperature is not None
--
--        # Apply temperature.
--        logits = self.apply_temperature(logits, sampling_metadata.temperature)
--
--        # Apply min_p.
--        if sampling_metadata.min_p is not None:
--            logits = self.apply_min_p(logits, sampling_metadata.min_p)
--
--        # Apply top_k and/or top_p.
--        random_sampled = self.topk_topp_sampler(
--            logits,
--            sampling_metadata.generators,
--            sampling_metadata.top_k,
--            sampling_metadata.top_p,
--        )
--
--        sampled = torch.where(sampling_metadata.temperature < _SAMPLING_EPS,
--                              greedy_sampled, random_sampled)
--        return sampled
--
--    def compute_logprobs(self, logits: torch.Tensor) -> torch.Tensor:
--        return logits.log_softmax(dim=-1, dtype=torch.float32)
--
--    def gather_logprobs(
--        self,
--        logprobs: torch.Tensor,
--        num_logprobs: int,
--        token_ids: torch.Tensor,
--    ) -> LogprobsTensors:
--        """
--        Gather logprobs for topk and sampled/prompt token.
--
--        Args:
--          logits: (num tokens) x (vocab) tensor
--          num_logprobs: minimum number of logprobs to
--                        retain per token
--          token_ids: prompt tokens (if prompt logprobs)
--                     or sampled tokens (if sampled
--                     logprobs); 1D token ID tensor
--                     with (num tokens) elements
--
--        Returns:
--          Top-k int indices tensor, (num tokens) x (num_logprobs + 1)
--          Top-k float logprobs tensor, (num tokens) x (num_logprobs + 1)
--          Sampled token rank tensor, (num tokens)
--        """
--        # Find the topK values.
--        topk_logprobs, topk_indices = torch.topk(logprobs,
--                                                 num_logprobs,
--                                                 dim=-1)
--
--        # Get with the logprob of the prompt or sampled token.
--        token_ids = token_ids.unsqueeze(-1)
--        token_logprobs = logprobs.gather(-1, token_ids)
--
--        # Compute the ranks of the actual token.
--        token_ranks = (logprobs >= token_logprobs).sum(-1)
--
--        # Concatenate together with the topk.
--        indices = torch.cat((token_ids, topk_indices), dim=1)
--        logprobs = torch.cat((token_logprobs, topk_logprobs), dim=1)
--
--        # Use int32 to reduce the tensor size.
--        indices = indices.to(torch.int32)
--
--        return LogprobsTensors(indices, logprobs, token_ranks)
--
--    def apply_min_p(
--        self,
--        logits: torch.Tensor,
--        min_p: torch.Tensor,
--    ) -> torch.Tensor:
--        """
--        Filters logits using adaptive probability thresholding.
--        """
--        # Convert logits to probability distribution
--        probability_values = torch.nn.functional.softmax(logits, dim=-1)
--        # Calculate maximum probabilities per sequence
--        max_probabilities = torch.amax(probability_values,
--                                       dim=-1,
--                                       keepdim=True)
--        # Reshape min_p for broadcasting
--        adjusted_min_p = min_p.unsqueeze(1) * max_probabilities
--        # Identify valid tokens using threshold comparison
--        valid_token_mask = probability_values >= adjusted_min_p
--        # Apply mask using boolean indexing (xla friendly)
--        logits.masked_fill_(~valid_token_mask, -float("inf"))
--        return logits
-diff --git a/vllm/v1/structured_output/__init__.py b/vllm/v1/structured_output/__init__.py
-index 0fdc45c2..58ac00e9 100644
---- a/vllm/v1/structured_output/__init__.py
-+++ b/vllm/v1/structured_output/__init__.py
-@@ -9,6 +9,7 @@ from vllm.config import VllmConfig
- from vllm.logger import init_logger
- from vllm.v1.structured_output.backend_types import (StructuredOutputBackend,
-                                                      StructuredOutputGrammar)
-+from vllm.v1.structured_output.backend_xgrammar import XgrammarBackend
- 
- if TYPE_CHECKING:
-     import numpy as np
-@@ -46,9 +47,6 @@ class StructuredOutputManager:
-         if self.backend is None:
-             backend_name = request.sampling_params.guided_decoding.backend_name
-             if backend_name == "xgrammar":
--                from vllm.v1.structured_output.backend_xgrammar import (
--                    XgrammarBackend)
--
-                 self.backend = XgrammarBackend(self.vllm_config)
-             else:
-                 raise ValueError(
-diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
-index b186300a..657333c6 100644
---- a/vllm/v1/worker/gpu_model_runner.py
-+++ b/vllm/v1/worker/gpu_model_runner.py
-@@ -45,7 +45,7 @@ from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
- if TYPE_CHECKING:
-     import xgrammar as xgr
- 
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler_output import SchedulerOutput
- else:
-     xgr = LazyLoader("xgr", globals(), "xgrammar")
- 
-@@ -127,7 +127,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):
- 
-         self.attn_metadata_builder = self.attn_backend.get_builder_cls()(
-             weakref.proxy(self))
--        self.cascade_attn_enabled = not self.model_config.disable_cascade_attn
- 
-         # Multi-modal data support
-         self.input_registry = INPUT_REGISTRY
-@@ -566,14 +565,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):
-                 self.positions_cpu[:total_num_scheduled_tokens],
-                 non_blocking=True)
- 
--        # Prepare for cascade attention if enabled & beneficial.
--        common_prefix_len = 0
--        if self.cascade_attn_enabled:
--            common_prefix_len = self._compute_cascade_attn_prefix_len(
--                num_scheduled_tokens,
--                scheduler_output.num_common_prefix_blocks,
--            )
--
-+        # Prepare for cascade attention if needed.
-+        common_prefix_len = self._compute_cascade_attn_prefix_len(
-+            num_scheduled_tokens,
-+            scheduler_output.num_common_prefix_blocks,
-+        )
-         attn_metadata = self.attn_metadata_builder.build(
-             num_reqs=num_reqs,
-             num_actual_tokens=total_num_scheduled_tokens,
-@@ -1562,7 +1558,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
-                     block_size=block_size,
-                     num_kv_heads=attn_module.num_kv_heads,
-                     head_size=attn_module.head_size,
--                    dtype=self.kv_cache_dtype,
-+                    dtype=attn_module.dtype,
-                     use_mla=use_mla)
-             elif attn_module.attn_type in (AttentionType.ENCODER,
-                                            AttentionType.ENCODER_ONLY):
-diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
-index a63a2d02..241869e3 100644
---- a/vllm/v1/worker/gpu_worker.py
-+++ b/vllm/v1/worker/gpu_worker.py
-@@ -28,7 +28,7 @@ from vllm.v1.worker.worker_base import WorkerBase
- logger = init_logger(__name__)
- 
- if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler_output import SchedulerOutput
- 
- 
- class Worker(WorkerBase):
-diff --git a/vllm/v1/worker/tpu_model_runner.py b/vllm/v1/worker/tpu_model_runner.py
-index d772a3ee..62d8354f 100644
---- a/vllm/v1/worker/tpu_model_runner.py
-+++ b/vllm/v1/worker/tpu_model_runner.py
-@@ -11,7 +11,6 @@ import torch.nn as nn
- import torch_xla.core.xla_model as xm
- import torch_xla.runtime as xr
- 
--import vllm.envs as envs
- from vllm.attention.backends.abstract import AttentionType
- from vllm.attention.layer import Attention
- from vllm.config import VllmConfig
-@@ -24,21 +23,18 @@ from vllm.multimodal.utils import group_mm_inputs_by_modality
- from vllm.sampling_params import SamplingType
- from vllm.sequence import IntermediateTensors
- from vllm.utils import LayerBlockType, cdiv, is_pin_memory_available
--from vllm.v1.attention.backends.pallas import (NUM_KV_PAGES_PER_BLOCK,
--                                               PallasAttentionBackend,
-+from vllm.v1.attention.backends.pallas import (PallasAttentionBackend,
-                                                PallasMetadata)
- from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
- from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,
-                                         KVCacheSpec)
- from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsTensors,
--                             ModelRunnerOutput, SamplerOutput)
--from vllm.v1.sample.tpu.metadata import TPUSupportedSamplingMetadata
--from vllm.v1.sample.tpu.sampler import Sampler as TPUSampler
-+                             ModelRunnerOutput)
- from vllm.v1.utils import bind_kv_cache
- from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
- 
- if TYPE_CHECKING:
--    from vllm.v1.core.sched.output import SchedulerOutput
-+    from vllm.v1.core.scheduler import SchedulerOutput
- 
- logger = init_logger(__name__)
- 
-@@ -46,8 +42,6 @@ logger = init_logger(__name__)
- # FIXME(woosuk): Find a more reliable way to prevent possible bugs.
- _PAD_SLOT_ID = 1_000_000_000
- INVALID_TOKEN_ID = -1
--# Smallest output size
--MIN_NUM_SEQS = 8
- 
- 
- class TPUModelRunner:
-@@ -74,10 +68,6 @@ class TPUModelRunner:
-         scheduler_config = self.scheduler_config
-         parallel_config = self.parallel_config
-         self.device = device
--        self.check_recompilation = envs.VLLM_XLA_CHECK_RECOMPILATION
--        if self.check_recompilation:
--            self.num_xla_graphs = xr.get_num_cached_compilation_graph()
--        self.enforce_eager = model_config.enforce_eager
-         self.pin_memory = is_pin_memory_available()
-         self.dtype = self.model_config.dtype
- 
-@@ -148,10 +138,8 @@ class TPUModelRunner:
-                                             device="cpu")
-         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
- 
--        padded_max_num_blocks_per_req = _get_padded_number(
--            self.max_num_blocks_per_req, NUM_KV_PAGES_PER_BLOCK)
-         self.block_table_cpu = torch.zeros(
--            (self.max_num_tokens, padded_max_num_blocks_per_req),
-+            (self.max_num_tokens, self.max_num_blocks_per_req),
-             dtype=self.input_batch.block_table.get_cpu_tensor().dtype,
-             device="cpu")
- 
-@@ -279,9 +267,6 @@ class TPUModelRunner:
-                 req_data.num_computed_tokens)
-             self.input_batch.block_table.append_row(req_data.new_block_ids,
-                                                     req_index)
--        # Check if the batch has changed. If not, we can skip copying the
--        # sampling metadata from CPU to GPU.
--        batch_changed = len(removed_req_indices) > 0 or len(req_ids_to_add) > 0
- 
-         # Add the new or resumed requests to the persistent batch.
-         # The smaller empty indices are filled first.
-@@ -299,10 +284,6 @@ class TPUModelRunner:
-         # Condense the batched states if there are empty indices.
-         if removed_req_indices:
-             self.input_batch.condense(removed_req_indices)
--
--        # TODO This slices tensors to copy to device, triggering recompilation.
--        if batch_changed:
--            self.input_batch.refresh_sampling_metadata()
-         return len(unscheduled_req_ids) > 0 or len(req_ids_to_add) > 0
- 
-     def get_model(self) -> nn.Module:
-@@ -466,8 +447,6 @@ class TPUModelRunner:
-         # TODO: Support prompt logprobs.
-         padded_num_reqs = _get_padded_num_reqs_with_upper_limit(
-             num_reqs, self.max_num_reqs)
--        # Indices at which we sample (positions of last token in the sequence).
--        # Padded to avoid recompiling when `num_reqs` varies.
-         logits_indices = self.query_start_loc_cpu[1:padded_num_reqs + 1] - 1
-         logits_indices = logits_indices.to(self.device)
-         return attn_metadata, logits_indices
-@@ -597,14 +576,7 @@ class TPUModelRunner:
-             # then the embedding layer is not included in the CUDA graph.
-             input_ids = self.input_ids
-             inputs_embeds = None
--        sampling_metadata = self.input_batch.sampling_metadata
--        num_reqs = self.input_batch.num_reqs
--        # NOTE (NickLucche) here we sync with TPU: if there's any shape
--        # mismatch in pre-processing, it will trigger a small recompilation
--        # of the code thus far. Forward graph remains untouched.
--        tpu_sampling_metadata = TPUSupportedSamplingMetadata.\
--            from_sampling_metadata(sampling_metadata, logits_indices,
--                                    num_reqs, self.device)
-+
-         # Run the decoder
-         with set_forward_context(attn_metadata, self.vllm_config):
-             hidden_states = self.model(
-@@ -613,13 +585,12 @@ class TPUModelRunner:
-                 kv_caches=self.kv_caches,
-                 inputs_embeds=inputs_embeds,
-             )
--        selected_token_ids = self.model.sample_from_hidden(
--            hidden_states, tpu_sampling_metadata)
--        # Remove padding on cpu and keep dynamic op outside of xla graph.
-+        num_reqs = self.input_batch.num_reqs
-+        selected_token_ids = self.model.compute_logits(hidden_states,
-+                                                       logits_indices, None)
-         selected_token_ids = selected_token_ids.cpu()[:num_reqs]
- 
--        # Update the cache state concurrently. Code above will not block until
--        # we use `selected_token_ids`. Add mark_step if post-processing changes
-+        # Then, let's update the cache state.
-         request_seq_lens: list[tuple[int, CachedRequestState, int]] = []
-         for i, req_id in zip(range(num_reqs), self.input_batch.req_ids):
-             assert req_id is not None
-@@ -636,6 +607,7 @@ class TPUModelRunner:
-                     # This relies on cuda-specific torch-internal impl details
-                     generator.set_offset(generator.get_offset() - 4)
- 
-+        # num_reqs entries should be non-None
-         assert all(
-             req_id is not None for req_id in
-             self.input_batch.req_ids[:num_reqs]), "req_ids contains None"
-@@ -648,7 +620,6 @@ class TPUModelRunner:
-         max_gen_len = selected_token_ids.shape[-1]
-         if max_gen_len == 1:
-             valid_sampled_token_ids = selected_token_ids.tolist()
--
-             for i, req_state, seq_len in request_seq_lens:
-                 token_id = valid_sampled_token_ids[i][0]
-                 self.input_batch.token_ids_cpu[i, seq_len] = token_id
-@@ -676,12 +647,6 @@ class TPUModelRunner:
-             logprobs=None,
-             prompt_logprobs_dict=prompt_logprobs_dict,
-         )
--        # Check there is no new graph compilation, all the graphs should be
--        # captured and compiled during warming up.
--        if self.check_recompilation and not self.enforce_eager:
--            curr_cached_graph = xr.get_num_cached_compilation_graph()
--            assert self.num_xla_graphs == curr_cached_graph, (
--                "Recompilation after warm up is detected.")
-         return model_runner_output
- 
-     def load_model(self) -> None:
-@@ -711,8 +676,11 @@ class TPUModelRunner:
-                                    fullgraph=True,
-                                    dynamic=False)
- 
--    @torch.no_grad()
--    def _dummy_run(self, kv_caches, num_tokens: int) -> None:
-+    def _dummy_run(
-+        self,
-+        kv_caches,
-+        num_tokens: int,
-+    ) -> None:
-         if self.is_multimodal_model:
-             input_ids = None
-             inputs_embeds = torch.zeros((num_tokens, self.hidden_size),
-@@ -761,10 +729,32 @@ class TPUModelRunner:
-         torch._dynamo.mark_dynamic(attn_metadata.slot_mapping, 0)
- 
-         with set_forward_context(attn_metadata, self.vllm_config, 0):
--            self.model(input_ids=input_ids,
--                       positions=position_ids,
--                       kv_caches=kv_caches,
--                       inputs_embeds=inputs_embeds)
-+            assert self.model is not None
-+            hidden_states = self.model(
-+                input_ids=input_ids,
-+                positions=position_ids,
-+                kv_caches=kv_caches,
-+                inputs_embeds=inputs_embeds,
-+            )
-+            num_reqs = _get_padded_num_reqs_with_upper_limit(
-+                64, self.max_num_reqs)
-+            # NOTE(chengjiyao): In total, the compute_logits function utilizes a
-+            # compilation cache size of token_bucket_num multiplied by
-+            # req_bucket_num. This is acceptable, given the graph's relatively
-+            # small size.
-+            while True:
-+                logits_indices = torch.zeros(
-+                    num_reqs,
-+                    dtype=torch.int32,
-+                    device=self.device,
-+                )
-+                torch._dynamo.mark_dynamic(hidden_states, 0)
-+                torch._dynamo.mark_dynamic(logits_indices, 0)
-+                self.model.compute_logits(hidden_states, logits_indices, None)
-+                if num_reqs >= self.max_num_reqs:
-+                    break
-+                num_reqs = _get_padded_num_reqs_with_upper_limit(
-+                    num_reqs + 1, self.max_num_reqs)
- 
-     def capture_model(self) -> None:
-         """Compile the model."""
-@@ -774,62 +764,16 @@ class TPUModelRunner:
-         start = time.perf_counter()
-         num_tokens = 16
-         while True:
--            logger.info("  -- num_tokens: %d", num_tokens)
-             self._dummy_run(self.kv_caches, num_tokens)
-+            logger.info("  -- num_tokens: %d", num_tokens)
-             xm.mark_step()
-+            xm.wait_device_ops()
-             if num_tokens >= self.max_num_tokens:
-                 break
-             num_tokens *= 2
--        xm.wait_device_ops()
-         end = time.perf_counter()
-         logger.info("Compilation finished in in %.2f [secs].", end - start)
- 
--        logger.info("Compiling sampling with different input shapes.")
--        start = time.perf_counter()
--        num_tokens = 16
--        hsize = self.model_config.get_hidden_size()
--        device = self.device
--        # Compile sampling step for different model+sampler outputs in bucketed
--        # n_tokens x max_num_reqs. Graph is really small so this is fine.
--        while True:
--            num_reqs_to_sample = MIN_NUM_SEQS
--            dummy_hidden = torch.randn((num_tokens, hsize),
--                                       device=device,
--                                       dtype=torch.bfloat16)
--            while True:
--                # Default metadata is an all_greedy setup. But since the
--                # `do_argmax` flag is a tensor, we still compile the full graph
--                meta = self.input_batch.sampling_metadata
--                indices = torch.zeros(
--                    num_reqs_to_sample,
--                    dtype=torch.int32,
--                    device=device,
--                )
--                sampling_meta = TPUSupportedSamplingMetadata.\
--                    from_sampling_metadata(meta, indices,
--                                           num_reqs_to_sample, device)
--                logger.info("  -- num_tokens: %d, num_seqs: %d", num_tokens,
--                            num_reqs_to_sample)
--                self.model.sample_from_hidden(dummy_hidden, sampling_meta)
--                xm.mark_step()
--                if num_reqs_to_sample >= self.max_num_reqs:
--                    break
--                num_reqs_to_sample *= 2
--            if num_tokens >= self.max_num_tokens:
--                break
--            num_tokens *= 2
--        xm.wait_device_ops()
--        end = time.perf_counter()
--        logger.info("Compilation finished in in %.2f [secs].", end - start)
--        # Record the number cached XLA graph after warming up, this will be
--        # used for checking there is no additional graph compilation during
--        # runtime execution.
--        if self.check_recompilation:
--            total_cached_graphs = xr.get_num_cached_compilation_graph()
--            num_compiled_graphs = total_cached_graphs - self.num_xla_graphs
--            logger.info("Compiled %d XLA graphs.", num_compiled_graphs)
--            self.num_xla_graphs += num_compiled_graphs
--
-     def initialize_kv_cache(self, kv_cache_config: KVCacheConfig) -> None:
-         """
-         Initialize KV cache based on `kv_cache_config`.
-@@ -874,13 +818,6 @@ class ModelWrapperV1(nn.Module):
-     def __init__(self, model: nn.Module):
-         super().__init__()
-         self.model = model
--        self.sampler = TPUSampler()
--
--    def sample(
--            self, logits: torch.Tensor,
--            sampling_metadata: TPUSupportedSamplingMetadata) -> SamplerOutput:
--        sampler_out = self.sampler(logits, sampling_metadata)
--        return sampler_out
- 
-     def forward(
-         self,
-@@ -889,7 +826,7 @@ class ModelWrapperV1(nn.Module):
-         kv_caches: list[tuple[torch.Tensor, torch.Tensor]],
-         inputs_embeds: Optional[torch.Tensor] = None,
-     ) -> torch.Tensor:
--        """Executes the forward pass of the model.
-+        """Executes the forward pass of the model and samples the next token.
- 
-         Args:
-             input_ids: The input token IDs of shape [num_tokens].
-@@ -900,6 +837,7 @@ class ModelWrapperV1(nn.Module):
-                 hidden_size]. It is used for multimodal models.
-         """
- 
-+        assert self.model is not None
-         hidden_states = self.model(
-             input_ids=input_ids,
-             positions=positions,
-@@ -908,33 +846,17 @@ class ModelWrapperV1(nn.Module):
- 
-         return hidden_states
- 
--    def sample_from_hidden(
-+    @torch.compile(backend="openxla", fullgraph=True, dynamic=False)
-+    def compute_logits(
-         self,
-         hidden_states: torch.Tensor,
--        sampling_metadata: TPUSupportedSamplingMetadata,
--    ) -> torch.Tensor:
--        """
--        Sample with xla-friendly function. This function is to be traced 
--        separately from `forward` for lighter compilation overhead.
--        """
--        # Tensor `sample_hidden_states` is of fixed pre-compiled size.
--        sample_hidden_states = \
--            hidden_states[sampling_metadata.indices_do_sample]
--        logits = self.compute_logits(sample_hidden_states)
--        # Greedy sampling can't be run without branching the graph on Sampler.
--        # Therefore do_argmax/all_greedy is checked here in a xla-friendly way.
--        # NOTE do_argmax is a scalar, this is just an optimized if/else.
--        out_tokens = torch.where(sampling_metadata.do_argmax,
--                        torch.argmax(logits, dim=-1, keepdim=True),
--                        self.sample(logits, sampling_metadata)\
--                                            .sampled_token_ids)
--        return out_tokens
--
--    def compute_logits(self,
--                       hidden_states: torch.Tensor) -> Optional[torch.Tensor]:
--        # SamplingMetadata here for pruning output in LogitsProcessor, disabled
--        logits = self.model.compute_logits(hidden_states, None)
--        return logits
-+        logits_indices: torch.Tensor,
-+        sampling_metadata,
-+    ) -> Optional[torch.Tensor]:
-+        hidden_states = hidden_states[logits_indices]
-+        logits = self.model.compute_logits(hidden_states, sampling_metadata)
-+        selected_token_ids = torch.argmax(logits, dim=-1, keepdim=True)
-+        return selected_token_ids
- 
-     def get_multimodal_embeddings(self, *args, **kwargs):
-         return self.model.get_multimodal_embeddings(*args, **kwargs)
-@@ -954,5 +876,5 @@ def _get_padded_token_len(x: int) -> int:
- 
- 
- def _get_padded_num_reqs_with_upper_limit(x, upper_limit) -> int:
--    res = MIN_NUM_SEQS if x <= MIN_NUM_SEQS else 1 << (x - 1).bit_length()
-+    res = 64 if x <= 64 else 1 << (x - 1).bit_length()
-     return min(res, upper_limit)
-diff --git a/vllm/v1/worker/tpu_worker.py b/vllm/v1/worker/tpu_worker.py
-index dbb23195..9f595611 100644
---- a/vllm/v1/worker/tpu_worker.py
-+++ b/vllm/v1/worker/tpu_worker.py
-@@ -17,7 +17,7 @@ from vllm.distributed import (ensure_model_parallel_initialized,
- from vllm.logger import init_logger
- from vllm.model_executor import set_random_seed
- from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE
--from vllm.v1.core.sched.output import SchedulerOutput
-+from vllm.v1.core.scheduler import SchedulerOutput
- from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,
-                                         KVCacheSpec)
- from vllm.v1.outputs import ModelRunnerOutput
